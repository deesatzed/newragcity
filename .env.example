# ============================================
# newragcity Environment Configuration
# ============================================
#
# Copy this file to .env and configure your settings:
#   cp .env.example .env
#
# Then edit .env with your actual API keys and preferences.

# ============================================
# LLM API Keys (Choose One or More)
# ============================================

# OpenAI API (recommended for production)
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Anthropic Claude API (optional, for Claude models)
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# OpenRouter API (optional, access to multiple models)
# Get your key at: https://openrouter.ai/
OPENROUTER_API_KEY=

# ============================================
# PageIndex Enhancement (Optional)
# ============================================

# PageIndex API Key (optional - defaults to OPENAI_API_KEY)
# If not set, PageIndex will fall back to simple chunking
# This enables LLM-powered document structure extraction
PAGEINDEX_API_KEY=

# ============================================
# Database Configuration
# ============================================

# PostgreSQL password (change this for production!)
POSTGRES_PASSWORD=changeme

# Full database URL (auto-generated, usually no need to change)
# DATABASE_URL=postgresql://newragcity:changeme@postgres:5432/newragcity

# ============================================
# System Configuration
# ============================================

# Logging level: debug, info, warning, error
LOG_LEVEL=info

# Data and index directories (relative to project root)
DATA_DIR=./data
INDEX_DIR=./data/indexes

# ============================================
# Confidence Thresholds
# ============================================

# Minimum confidence to display results (0.0-1.0)
# Default: 0.80 (80% confidence)
CONFIDENCE_THRESHOLD=0.80

# Developer/critical threshold for automated decisions (0.0-1.0)
# Default: 0.95 (95% confidence)
DEVELOPER_THRESHOLD=0.95

# ============================================
# Model Configuration
# ============================================

# Multimodal model for RoT reasoning
# Options: qwen2.5-vl:7b, llava:13b, phi-3-vision:latest
MULTIMODAL_MODEL=qwen2.5-vl:7b

# Model framework for RoT
# Options: ollama (default), mlx (macOS), vllm (production GPU)
MODEL_FRAMEWORK=ollama

# Embedding model for LEANN vector search
# Default: ibm-granite/granite-embedding-english-r2 (384 dimensions)
EMBEDDING_MODEL=ibm-granite/granite-embedding-english-r2

# ============================================
# Advanced Configuration (Optional)
# ============================================

# RoT compression ratio target (default: 3.5Ã—)
COMPRESSION_RATIO_TARGET=3.5

# LEANN backend (default: hnsw)
# Options: hnsw, faiss, lancedb
LEANN_BACKEND=hnsw

# Maximum chunk characters before truncation with pointer
MAX_CHUNK_CHARS=2000

# ============================================
# Usage Notes
# ============================================
#
# Minimum Required:
#   - OPENAI_API_KEY (or use local Ollama)
#   - POSTGRES_PASSWORD (change from default)
#
# Recommended:
#   - Set PAGEINDEX_API_KEY for enhanced document intelligence
#   - Use strong POSTGRES_PASSWORD in production
#   - Adjust CONFIDENCE_THRESHOLD based on use case
#
# Optional Enhancements:
#   - ANTHROPIC_API_KEY for Claude models
#   - OPENROUTER_API_KEY for multiple model access
#   - Adjust MODEL_FRAMEWORK for your platform
#
# Using Local Ollama (No API Keys Required):
#   1. Leave API keys empty
#   2. Ensure Ollama service is running
#   3. Pull required model: docker-compose exec ollama ollama pull qwen2.5-vl:7b
#   4. All inference will use local Ollama instance
#
# ============================================
# Quick Start
# ============================================
#
# 1. Copy this file:
#    cp .env.example .env
#
# 2. Edit .env and add your API keys (or use local Ollama)
#
# 3. Start the system:
#    docker-compose up -d
#
# 4. Initialize Ollama model (if using local):
#    docker-compose exec ollama ollama pull qwen2.5-vl:7b
#
# 5. Access the system:
#    - Web UI: http://localhost:5050
#    - REST API: http://localhost:8000
#
# 6. Upload documents and start querying!
#
