#!/usr/bin/env python3
"""
Automated SOTA Testing and Comparison System for newragcity

Fully automated pipeline for:
1. Running all 15 BEIR datasets with Qwen3 embeddings
2. Comparing results to published SOTA baselines
3. Statistical significance testing
4. Automated reporting and dashboard generation
5. Progress monitoring and error recovery

Usage:
    # Run full automated BEIR benchmark
    python automated_sota_testing.py --mode full

    # Run quick validation (3 small datasets)
    python automated_sota_testing.py --mode quick

    # Resume from checkpoint
    python automated_sota_testing.py --mode resume

    # Generate report from existing results
    python automated_sota_testing.py --mode report-only
"""

import sys
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict
import numpy as np
from scipy import stats

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from beir_all_datasets import BEIRMultiDatasetBenchmark, BEIR_DATASETS


# Published SOTA baselines (January 2026)
SOTA_BASELINES = {
    "nfcorpus": {
        "system": "newragcity (MiniLM)",
        "ndcg@10": 0.5086,
        "year": 2026,
        "notes": "Our previous result with old embeddings"
    },
    "scifact": {
        "system": "Nomic Embed v1.5",
        "ndcg@10": 0.7036,
        "year": 2024,
        "notes": "Current SOTA"
    },
    "arguana": {
        "system": "NV-Embed",
        "ndcg@10": 0.6375,
        "year": 2024
    },
    "trec-covid": {
        "system": "Nomic Embed v1.5",
        "ndcg@10": 0.6910,
        "year": 2024
    },
    "fiqa": {
        "system": "NV-Embed",
        "ndcg@10": 0.3649,
        "year": 2024
    },
    "scidocs": {
        "system": "Nomic Embed v1.5",
        "ndcg@10": 0.1776,
        "year": 2024
    },
    "quora": {
        "system": "NV-Embed",
        "ndcg@10": 0.8882,
        "year": 2024
    },
    "dbpedia-entity": {
        "system": "NV-Embed",
        "ndcg@10": 0.4464,
        "year": 2024
    },
    "robust04": {
        "system": "NV-Embed",
        "ndcg@10": 0.5083,
        "year": 2024
    },
    "msmarco": {
        "system": "NV-Embed",
        "ndcg@10": 0.4406,
        "year": 2024
    },
    "nq": {
        "system": "NV-Embed",
        "ndcg@10": 0.5569,
        "year": 2024
    },
    "hotpotqa": {
        "system": "NV-Embed",
        "ndcg@10": 0.6673,
        "year": 2024
    },
    "fever": {
        "system": "NV-Embed",
        "ndcg@10": 0.8199,
        "year": 2024
    },
    "climate-fever": {
        "system": "NV-Embed",
        "ndcg@10": 0.2774,
        "year": 2024
    },
    "signal1m": {
        "system": "NV-Embed",
        "ndcg@10": 0.3370,
        "year": 2024
    },
}

# BEIR weighted aggregate SOTA
BEIR_AGGREGATE_SOTA = {
    "NV-Embed": 0.5935,
    "Nomic Embed v1.5": 0.5881,
    "E5-mistral-7b": 0.5720,
    "Cohere embed-v3": 0.5580,
    "GTE-large": 0.5525,
}


@dataclass
class DatasetResult:
    """Result for a single dataset"""
    dataset: str
    ndcg_at_10: float
    recall_at_100: float
    queries_tested: int
    elapsed_seconds: float
    sota_baseline: float
    improvement_pct: float
    beats_sota: bool
    statistical_significance: Optional[bool] = None
    p_value: Optional[float] = None
    confidence_interval: Optional[Tuple[float, float]] = None


@dataclass
class AggregateResult:
    """Aggregate results across all datasets"""
    weighted_avg_ndcg: float
    unweighted_avg_ndcg: float
    total_datasets: int
    datasets_beating_sota: int
    beir_rank: int
    beats_aggregate_sota: bool
    improvement_vs_sota_pct: float
    statistical_tests_passed: int
    dataset_results: List[DatasetResult]
    timestamp: str


class AutomatedSOTATesting:
    """
    Fully automated SOTA testing system.

    Features:
    - Automatic dataset execution
    - Real-time progress monitoring
    - Statistical significance testing
    - SOTA comparison and ranking
    - Automated report generation
    - Error recovery and checkpointing
    """

    def __init__(
        self,
        datasets_dir: str = "/Volumes/WS4TB/newragcity/UltraRAG-main/datasets",
        results_dir: str = None
    ):
        self.datasets_dir = Path(datasets_dir)
        self.results_dir = Path(results_dir) if results_dir else Path(__file__).parent / "results" / "automated_sota"
        self.results_dir.mkdir(parents=True, exist_ok=True)

        self.benchmark_system = BEIRMultiDatasetBenchmark(
            datasets_dir=str(self.datasets_dir),
            results_dir=str(self.results_dir)
        )

    def run_full_benchmark(self) -> AggregateResult:
        """
        Run complete automated BEIR benchmark.

        Returns:
            AggregateResult with all metrics and comparisons
        """
        print(f"\n{'='*70}")
        print("AUTOMATED SOTA TESTING SYSTEM")
        print("newragcity with Qwen3-Embedding-0.6B")
        print(f"{'='*70}\n")

        print("Mode: Full BEIR Benchmark (15 datasets)")
        print(f"Expected time: 60-80 hours")
        print(f"Datasets directory: {self.datasets_dir}")
        print(f"Results directory: {self.results_dir}")
        print(f"\n{'='*70}\n")

        # Run all datasets
        start_time = time.time()
        aggregate_raw = self.benchmark_system.run_all(resume=True)
        total_time = time.time() - start_time

        # Process results with SOTA comparison
        aggregate_result = self._process_results(aggregate_raw, total_time)

        # Generate comprehensive report
        self._generate_report(aggregate_result)

        # Save structured results
        self._save_results(aggregate_result)

        return aggregate_result

    def run_quick_validation(self) -> AggregateResult:
        """
        Run quick validation on 3 small datasets.

        Datasets: nfcorpus, scifact, arguana
        Expected time: 2-3 hours
        """
        print(f"\n{'='*70}")
        print("QUICK VALIDATION MODE")
        print("Testing: nfcorpus, scifact, arguana")
        print(f"{'='*70}\n")

        quick_datasets = ["nfcorpus", "scifact", "arguana"]

        start_time = time.time()
        results = {}

        for dataset_name in quick_datasets:
            dataset_info = next((d for d in BEIR_DATASETS if d["name"] == dataset_name), None)
            if dataset_info:
                result = self.benchmark_system.run_single_dataset(dataset_info)
                if result:
                    results[dataset_name] = result

        total_time = time.time() - start_time

        # Create aggregate from quick results
        aggregate_raw = self.benchmark_system._calculate_aggregate(results)
        aggregate_result = self._process_results(aggregate_raw, total_time)

        self._generate_report(aggregate_result, mode="quick")
        self._save_results(aggregate_result, suffix="_quick")

        return aggregate_result

    def _process_results(self, raw_results: Dict, total_time: float) -> AggregateResult:
        """
        Process raw results and add SOTA comparison.

        Args:
            raw_results: Raw results from benchmark system
            total_time: Total execution time in seconds

        Returns:
            AggregateResult with SOTA comparison
        """
        dataset_results = []
        statistical_tests_passed = 0

        for dataset_info in raw_results.get("datasets_info", []):
            dataset_name = dataset_info["name"]
            our_ndcg = dataset_info["nDCG@10"]

            # Get SOTA baseline
            sota_baseline = SOTA_BASELINES.get(dataset_name, {}).get("ndcg@10", 0.0)

            # Calculate improvement
            if sota_baseline > 0:
                improvement_pct = ((our_ndcg - sota_baseline) / sota_baseline) * 100
            else:
                improvement_pct = 0.0

            beats_sota = our_ndcg > sota_baseline

            # Statistical significance testing (if per-query scores available)
            stat_sig, p_value, conf_interval = self._test_statistical_significance(
                dataset_name, our_ndcg, sota_baseline
            )

            if stat_sig:
                statistical_tests_passed += 1

            result = DatasetResult(
                dataset=dataset_name,
                ndcg_at_10=our_ndcg,
                recall_at_100=dataset_info["Recall@100"],
                queries_tested=dataset_info["queries"],
                elapsed_seconds=0,  # Would need to track per-dataset
                sota_baseline=sota_baseline,
                improvement_pct=improvement_pct,
                beats_sota=beats_sota,
                statistical_significance=stat_sig,
                p_value=p_value,
                confidence_interval=conf_interval
            )

            dataset_results.append(result)

        # Calculate aggregate metrics
        weighted_avg_ndcg = raw_results["beir_weighted_average_ndcg@10"]
        unweighted_avg_ndcg = raw_results["beir_unweighted_average_ndcg@10"]

        # Determine BEIR rank
        beir_rank = self._calculate_beir_rank(weighted_avg_ndcg)

        # Check if we beat aggregate SOTA
        current_sota = BEIR_AGGREGATE_SOTA["NV-Embed"]
        beats_aggregate_sota = weighted_avg_ndcg > current_sota
        improvement_vs_sota_pct = ((weighted_avg_ndcg - current_sota) / current_sota) * 100

        return AggregateResult(
            weighted_avg_ndcg=weighted_avg_ndcg,
            unweighted_avg_ndcg=unweighted_avg_ndcg,
            total_datasets=raw_results["datasets_tested"],
            datasets_beating_sota=raw_results["datasets_beating_sota"],
            beir_rank=beir_rank,
            beats_aggregate_sota=beats_aggregate_sota,
            improvement_vs_sota_pct=improvement_vs_sota_pct,
            statistical_tests_passed=statistical_tests_passed,
            dataset_results=dataset_results,
            timestamp=datetime.now().isoformat()
        )

    def _test_statistical_significance(
        self,
        dataset_name: str,
        our_score: float,
        baseline_score: float
    ) -> Tuple[Optional[bool], Optional[float], Optional[Tuple[float, float]]]:
        """
        Test statistical significance using per-query scores.

        Returns:
            (is_significant, p_value, confidence_interval)
        """
        # Load per-query scores if available
        result_file = self.results_dir / f"{dataset_name}_results.json"
        if not result_file.exists():
            return (None, None, None)

        try:
            with open(result_file, 'r') as f:
                results = json.load(f)

            per_query_scores = results.get("per_query_scores", {}).get("ndcg@10", [])

            if not per_query_scores or len(per_query_scores) < 30:
                # Need minimum sample size for statistical testing
                return (None, None, None)

            # One-sample t-test against baseline
            t_statistic, p_value = stats.ttest_1samp(per_query_scores, baseline_score)

            # 95% confidence interval
            mean_score = np.mean(per_query_scores)
            std_error = stats.sem(per_query_scores)
            conf_interval = stats.t.interval(
                0.95,
                len(per_query_scores) - 1,
                loc=mean_score,
                scale=std_error
            )

            # Significant if p < 0.05 AND improvement
            is_significant = (p_value < 0.05) and (our_score > baseline_score)

            return (is_significant, float(p_value), tuple(map(float, conf_interval)))

        except Exception as e:
            print(f"Warning: Statistical testing failed for {dataset_name}: {e}")
            return (None, None, None)

    def _calculate_beir_rank(self, our_score: float) -> int:
        """Calculate our rank on BEIR leaderboard."""

        # Add our score to SOTA baselines
        all_scores = list(BEIR_AGGREGATE_SOTA.values()) + [our_score]
        all_scores_sorted = sorted(all_scores, reverse=True)

        return all_scores_sorted.index(our_score) + 1

    def _generate_report(self, results: AggregateResult, mode: str = "full"):
        """Generate comprehensive report."""

        report_file = self.results_dir / f"AUTOMATED_SOTA_REPORT_{mode.upper()}.md"

        report = f"""# Automated SOTA Testing Report - newragcity with Qwen3

**Date**: {results.timestamp}
**Mode**: {mode.upper()} benchmark
**System**: ThreeApproachRAG (PageIndex + LEANN + deepConf)
**Embedding Model**: Qwen3-Embedding-0.6B (600M params)

---

## Executive Summary

### BEIR Aggregate Performance

**Our Score**: {results.weighted_avg_ndcg:.4f} nDCG@10 (weighted average)
**Current SOTA**: {BEIR_AGGREGATE_SOTA['NV-Embed']:.4f} (NV-Embed)
**Improvement**: {results.improvement_vs_sota_pct:+.1f}%
**BEIR Rank**: #{results.beir_rank} (out of ~{len(BEIR_AGGREGATE_SOTA) + 1} systems)

**Result**: {'‚úÖ **NEW BEIR SOTA!**' if results.beats_aggregate_sota else f'‚ùå Below SOTA (rank #{results.beir_rank})'}

### Dataset Performance

- **Total datasets tested**: {results.total_datasets}/15
- **Datasets beating SOTA**: {results.datasets_beating_sota}/{results.total_datasets} ({results.datasets_beating_sota/results.total_datasets*100:.1f}%)
- **Statistical tests passed**: {results.statistical_tests_passed}/{results.total_datasets}

---

## Per-Dataset Results

### Datasets Beating SOTA ‚úÖ

"""

        # Sort datasets by improvement
        beating_sota = [d for d in results.dataset_results if d.beats_sota]
        beating_sota.sort(key=lambda x: x.improvement_pct, reverse=True)

        if beating_sota:
            report += f"| Dataset | Our nDCG@10 | SOTA | Improvement | Stat. Sig. |\n"
            report += f"|---------|-------------|------|-------------|------------|\n"

            for d in beating_sota:
                stat_sig_marker = "‚úÖ" if d.statistical_significance else "‚ö†Ô∏è" if d.statistical_significance is None else "‚ùå"
                report += f"| {d.dataset} | **{d.ndcg_at_10:.4f}** | {d.sota_baseline:.4f} | **+{d.improvement_pct:.1f}%** | {stat_sig_marker} (p={d.p_value:.3f if d.p_value else 'N/A'}) |\n"
        else:
            report += "*No datasets beating SOTA yet*\n"

        report += f"\n### Datasets Below SOTA ‚ùå\n\n"

        below_sota = [d for d in results.dataset_results if not d.beats_sota]
        below_sota.sort(key=lambda x: x.improvement_pct, reverse=True)

        if below_sota:
            report += f"| Dataset | Our nDCG@10 | SOTA | Gap | Analysis |\n"
            report += f"|---------|-------------|------|-----|----------|\n"

            for d in below_sota:
                gap_pct = abs(d.improvement_pct)
                analysis = self._analyze_performance_gap(d)
                report += f"| {d.dataset} | {d.ndcg_at_10:.4f} | {d.sota_baseline:.4f} | -{gap_pct:.1f}% | {analysis} |\n"
        else:
            report += "*All datasets beat SOTA!* ‚úÖ\n"

        report += f"""
---

## SOTA Leaderboard Comparison

### Current BEIR Leaderboard (Weighted Average nDCG@10)

| Rank | System | Score | Year | Notes |
|------|--------|-------|------|-------|
"""

        # Add all systems including ours
        leaderboard = list(BEIR_AGGREGATE_SOTA.items())
        leaderboard.append(("newragcity (Qwen3)", results.weighted_avg_ndcg))
        leaderboard.sort(key=lambda x: x[1], reverse=True)

        for rank, (system, score) in enumerate(leaderboard, 1):
            marker = "**" if "newragcity" in system else ""
            year = "2026" if "newragcity" in system else "2024"
            report += f"| {rank} | {marker}{system}{marker} | {marker}{score:.4f}{marker} | {year} | {'üëë NEW SOTA!' if rank == 1 and 'newragcity' in system else ''} |\n"

        report += f"""
---

## Statistical Analysis

### Significance Testing

**Tests conducted**: {len([d for d in results.dataset_results if d.statistical_significance is not None])}/{results.total_datasets}
**Tests passed (p < 0.05)**: {results.statistical_tests_passed}
**Success rate**: {results.statistical_tests_passed / max(1, len([d for d in results.dataset_results if d.statistical_significance is not None])) * 100:.1f}%

### Confidence Intervals (95%)

"""

        for d in results.dataset_results:
            if d.confidence_interval:
                report += f"- **{d.dataset}**: [{d.confidence_interval[0]:.4f}, {d.confidence_interval[1]:.4f}] (mean: {d.ndcg_at_10:.4f})\n"

        report += f"""
---

## Next Steps

"""

        if results.beats_aggregate_sota:
            report += f"""
### üéâ Congratulations - New BEIR SOTA!

**Immediate Actions**:
1. ‚úÖ Verify all results are reproducible
2. ‚úÖ Run ablation studies (PageIndex, LEANN, deepConf contributions)
3. ‚úÖ Prepare publication draft
4. ‚úÖ Submit preprint to arXiv
5. ‚úÖ Target conference: SIGIR 2026, ACL 2026, or NeurIPS 2026

**Publication Strategy**:
- Title: "newragcity: Achieving State-of-the-Art on BEIR with Qwen3 Embeddings and Multi-Approach RAG"
- Venue: Top-tier (SIGIR, ACL, NeurIPS)
- Expected impact: High (new SOTA on standard benchmark)
"""
        else:
            report += f"""
### Improvement Recommendations

**Current Rank**: #{results.beir_rank}
**Gap to SOTA**: {abs(results.improvement_vs_sota_pct):.1f}%

**Recommended Actions**:
1. Add cross-encoder re-ranking (expected +3-5 points)
2. Fine-tune Qwen3 on MS MARCO (expected +2-4 points)
3. Implement hard-negative mining (expected +1-2 points)
4. Optimize confidence thresholds per dataset (expected +0.5-1 points)

**Timeline to SOTA**: 1-2 months with above improvements
"""

        report += f"""
---

## System Configuration

- **Embedding Model**: Qwen3-Embedding-0.6B (600M params, 4096D)
- **Retrieval**: LEANN HNSW backend
- **Document Processing**: PageIndex reasoning-based extraction
- **Confidence Scoring**: deepConf multi-factor analysis
- **Confidence Threshold**: 0.80

---

**Report generated automatically by Automated SOTA Testing System**
"""

        # Save report
        with open(report_file, 'w') as f:
            f.write(report)

        print(f"\n‚úÖ Report saved to: {report_file}")

    def _analyze_performance_gap(self, result: DatasetResult) -> str:
        """Analyze why we're below SOTA on this dataset."""

        gap = abs(result.improvement_pct)

        if gap < 5:
            return "Minor gap, within noise"
        elif gap < 10:
            return "Small gap, tuning needed"
        elif gap < 20:
            return "Moderate gap, re-ranking may help"
        else:
            return "Large gap, domain-specific tuning required"

    def _save_results(self, results: AggregateResult, suffix: str = ""):
        """Save structured results to JSON."""

        output_file = self.results_dir / f"automated_sota_results{suffix}.json"

        results_dict = asdict(results)

        with open(output_file, 'w') as f:
            json.dump(results_dict, f, indent=2)

        print(f"‚úÖ Structured results saved to: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description="Automated SOTA Testing System for newragcity"
    )
    parser.add_argument(
        "--mode",
        choices=["full", "quick", "resume", "report-only"],
        default="full",
        help="Execution mode"
    )
    parser.add_argument(
        "--datasets-dir",
        type=str,
        default="/Volumes/WS4TB/newragcity/UltraRAG-main/datasets",
        help="Path to BEIR datasets"
    )
    parser.add_argument(
        "--results-dir",
        type=str,
        help="Path to save results"
    )

    args = parser.parse_args()

    # Initialize system
    tester = AutomatedSOTATesting(
        datasets_dir=args.datasets_dir,
        results_dir=args.results_dir
    )

    # Execute based on mode
    if args.mode == "full":
        results = tester.run_full_benchmark()
    elif args.mode == "quick":
        results = tester.run_quick_validation()
    elif args.mode == "resume":
        results = tester.run_full_benchmark()  # Uses checkpointing automatically
    elif args.mode == "report-only":
        print("Report-only mode not yet implemented")
        return

    # Print summary
    print(f"\n{'='*70}")
    print(f"AUTOMATED SOTA TESTING COMPLETE")
    print(f"{'='*70}")
    print(f"\nBEIR Aggregate: {results.weighted_avg_ndcg:.4f}")
    print(f"BEIR Rank: #{results.beir_rank}")
    print(f"Datasets Beating SOTA: {results.datasets_beating_sota}/{results.total_datasets}")

    if results.beats_aggregate_sota:
        print(f"\nüéâ NEW BEIR SOTA! üéâ")
    else:
        print(f"\nGap to SOTA: {abs(results.improvement_vs_sota_pct):.1f}%")

    print(f"\n{'='*70}\n")


if __name__ == "__main__":
    main()
