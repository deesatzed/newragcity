
this new concept demo example is confusing to me. Can you break it down and explain it. """ðŸš€ DeepConf on vLLM

Minimal Patch & Usage Guide for Confidence-Based Early Stopping

This document consolidates everything you need toÂ use DeepConf on vLLM:

How to modify vLLM to implement DeepConfÂ - A minimal patch guide showing what to change, where, and why.

Or skip the modificationsÂ - Directly download our PR with pre-implemented changes.

Example code to run DeepConfÂ - Complete working examples demonstrating the feature.

Option 1: Directly download our PR

Related PR:Â https://github.com/vllm-project/vllm/pull/23201



Option 2: Modify vLLM

This README documents a minimal set of edits to integrateÂ DeepConfÂ (confidence-based early stopping) intoÂ vLLM, how to enable it via the OpenAI-compatible API, and pointers to the example notebook.

Tested Environment

vLLM: commitÂ 31f09c615f4f067dba765ce5fe7d00d880212a6d

Python: 3.12.0

CUDA: 12.8

High-Level Changes

We modify only two places in vLLM:

ExtendÂ LogprobsProcessorÂ to maintain a sliding-window confidence and exposeÂ check_conf_stop().

InÂ output_processor.py, insert a single early-stop check before constructingÂ RequestOutput.

Enabling via OpenAI-Compatible API

The feature is toggledÂ per requestÂ via the OpenAI-compatibleÂ chat.completionsÂ endpoint. The arguments are passed throughÂ extra_body["vllm_xargs"]Â and forwarded by vLLM toÂ SamplingParams.extra_args.

# Code: Enable confidence-based early stopping via OpenAI-compatible API

responses = client.chat.completions.create(

model=args.model_path,

messages=messages,

max_tokens=args.max_tokens,

temperature=0.6,

top_p=0.95,

logprobs=True,

top_logprobs=20, # request candidate logprobs (>=2)

n=real_gen,

extra_body={

"top_k": 0,

"vllm_xargs": {

"enable_conf": True,

"window_size": 2048,

"threshold": conf_threshold

}

}

)

Notes:

The early-stop logic is inactive unlessÂ logprobs=TrueÂ andÂ top_logprobs>=2.

window_sizeÂ is the confidence window length;Â thresholdÂ is the cutoff used by our method.

top_k=0Â (optional) disables top-k truncation.

Exact Edits (Copy-Paste Guidance)

No patch tools are required; copy the snippets below into the indicated files. We recommend pinning to the commit above to avoid API drift.

File:Â vllm/v1/engine/logprobs.py

Step 1: Import (near the top):

from collections import deque

from typing import Optional, List

Step 2: Extend the dataclass (class LogprobsProcessor):

# --- fields for confidence-based early stopping ---

conf_grouped: float

conf_list: Optional[List[float]]

conf_group_list: Optional[deque]

conf_group_size: int

conf_threshold: Optional[float]

Step 3: Initialize from the requestÂ (insideÂ from_new_request(...), right beforeÂ return cls(...)):

if hasattr(request.sampling_params, "extra_args") \

and request.sampling_params.extra_args is not None \

and request.sampling_params.extra_args.get("enable_conf", False):

conf_group_size = request.sampling_params.extra_args.get("window_size", 2048)

conf_threshold = request.sampling_params.extra_args.get("threshold", 17)

conf_grouped = 0.0

conf_group_list = deque(maxlen=conf_group_size)

conf_list = []

else:

conf_group_size = -1

conf_threshold = None

conf_grouped = 0.0

conf_group_list = None

conf_list = None

Then include the fields below in theÂ return cls(...)Â call:

conf_group_size=conf_group_size,

conf_grouped=conf_grouped,

conf_list=conf_list,

conf_threshold=conf_threshold,

conf_group_list=conf_group_list,

Step 4: Stop-check helperÂ (add this method inside the class):

def check_conf_stop(self) -> bool:

"""Return True if the confidence window triggers early stopping."""

if self.conf_group_list is None or len(self.conf_group_list) == 0:

return False

# Require a full window; trigger when the moving average is below threshold.

return (len(self.conf_group_list) >= self.conf_group_size

and self.conf_grouped / len(self.conf_group_list) < self.conf_threshold)

Step 5: Update confidence during samplingÂ (at the end ofÂ _update_sample_logprobs(...), after appending the logprob dict):

if self.conf_list is not None:

# logprobs[0] is the sampled token; use the remaining candidates

if len(logprobs) > 1:

new_conf = -sum(logprobs[1:]) / len(logprobs[1:])

else:

new_conf = 0.0

self.conf_list.append(new_conf)



if len(self.conf_group_list) < self.conf_group_size:

self.conf_group_list.append(new_conf)

self.conf_grouped += new_conf

else:

self.conf_grouped -= self.conf_group_list.popleft()

self.conf_group_list.append(new_conf)

self.conf_grouped += new_conf

File:Â vllm/v1/engine/output_processor.py

Step 6: Invoke the stop-check in the decode loop.

ImmediatelyÂ after:

req_state.logprobs_processor.update_from_output(engine_core_output)

insert:

# Confidence-based early stopping (ours)

if req_state.logprobs_processor.check_conf_stop():

finish_reason = FinishReason.STOP

stop_reason = f"<gconf<{req_state.logprobs_processor.conf_threshold}>"

(Leave the subsequent logic that buildsÂ RequestOutputÂ unchanged.)



Additional Notes

The feature is inactive unlessÂ enable_conf=TrueÂ andÂ logprobs>0Â (we useÂ top_logprobs=20).

ConfidenceÂ is the moving average of theÂ negative mean candidate logprobsÂ over a fixed window (window_size).

When triggered, we setÂ FinishReason.STOPÂ and annotateÂ stop_reasonÂ withÂ <gconf<THR>>Â for traceability.

Go to this PR and download the vllm version with the above changes.



Running Example

Install vLLM

Download the vllm and modify the vllm code as above, or download the version in the prÂ https://github.com/vllm-project/vllm/pull/23201Â and build it as follows.

VLLM_USE_PRECOMPILED=1 uv pip install --editable .

Install dependencies for the example code

git clone https://github.com/hao-ai-lab/Dynasor.git

cd Dynasor && pip install . && cd -

Example 1: Offline Generation

import openai

import json

from tqdm import tqdm

import time

import os

import requests

from datetime import datetime

from transformers import AutoTokenizer

import concurrent.futures

import threading

from functools import partial


# ===========================

# Model Configurations

# ===========================


MODEL_CONFIGS = {

"Qwen/Qwen3-8B": {

"temperature": 0.6,

"top_p": 0.95,

"top_k": 20,

"max_tokens": 32000,

"template": "qwen3"

},

"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B": {

"temperature": 0.6,

"top_p": 0.95,

"top_k": 0,

"max_tokens": 64000,

"template": "dpsk_qwen_0528"

},

"openai/gpt-oss-20b": {

"temperature": 1.0,

"top_p": 1.0,

"top_k": 40,

"max_tokens": 14000,

"template": "gpt"

},

# Add more model configurations as needed

}


# ===========================

# Main Configuration

# ===========================


# Select your model

MODEL_NAME = "Qwen/Qwen3-8B" # Change this to your desired model

SAMPLES_PER_QUESTION = 4 # Number of traces to generate per question

DATASET_FILE = "aime25.jsonl" # Input dataset file

REASONING_EFFORT = "high" # For GPT models: low, medium, high


# Parallel processing configuration

MAX_WORKERS = 8 # Maximum number of concurrent workers (adjust based on your server capacity)

MAX_WORKERS_PER_QUESTION = 4 # Maximum workers for traces within a single question


# Get model-specific config

model_config = MODEL_CONFIGS.get(MODEL_NAME)


# General Configuration

CONFIG = {

"model_path": MODEL_NAME,

"server_port": 8000,

"temperature": model_config["temperature"],

"top_p": model_config["top_p"],

"top_k": model_config["top_k"],

"max_tokens": model_config["max_tokens"],

"template": model_config["template"],

"reasoning_effort": REASONING_EFFORT,


# Dataset and sampling configuration

"dataset": DATASET_FILE, # Input dataset file

"max_samples_per_question": SAMPLES_PER_QUESTION, # Number of traces per question

"output_dir": f"output_{MODEL_NAME.replace('/', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",


# Parallel processing configuration

"max_workers": MAX_WORKERS,

"max_workers_per_question": MAX_WORKERS_PER_QUESTION,

}


# Thread-safe file writing lock

file_lock = threading.Lock()


# ===========================

# Initialize OpenAI Client

# ===========================


# Note: Make sure vLLM server is already running on the specified port

# Example command to start vLLM server:

# vllm serve MODEL_NAME --port 8000 -tp 1 --gpu-memory-utilization 0.7 --enable-prefix-caching


print(f"Connecting to vLLM server...")

print(f"Model: {CONFIG['model_path']}")

print(f"Server URL: http://localhost:{CONFIG['server_port']}/v1")

print(f"Max concurrent workers: {CONFIG['max_workers']}")

print(f"Max workers per question: {CONFIG['max_workers_per_question']}")


# Initialize OpenAI client

client = openai.OpenAI(

api_key="None",

base_url=f"http://localhost:{CONFIG['server_port']}/v1",

timeout=None

)


# Initialize tokenizer for GPT models

if CONFIG['template'] == 'gpt':

tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_path'])

else:

tokenizer = None


# Test connection

try:

response = requests.get(

f"http://localhost:{CONFIG['server_port']}/v1/models",

headers={"Authorization": "Bearer None"},

)

if response.status_code == 200:

print("âœ… Successfully connected to vLLM server")

else:

print(f"âš ï¸ Server returned status code: {response.status_code}")

except requests.exceptions.RequestException as e:

print(f"âŒ Failed to connect to vLLM server: {e}")

print("Please ensure vLLM server is running on the specified port")


# ===========================

# Core Processing Functions

# ===========================


def get_gpt_token_probabilities(messages, max_tokens=50):

"""

Function to get token probabilities for GPT models using completions API

"""

response = client.completions.create(

model=CONFIG['model_path'],

prompt=messages,

max_tokens=max_tokens,

temperature=CONFIG['temperature'],

top_p=CONFIG['top_p'],

logprobs=20,

extra_body={

"top_k": CONFIG['top_k']

},

)


# Extract generated text

generated_text = response.choices[0].text


# Extract token probabilities

token_probs = []

mean_confs = []

tokens = []

log_probs = []


if response.choices[0].logprobs and response.choices[0].logprobs.tokens:

for i, token_data in enumerate(response.choices[0].logprobs.tokens):

step_probs = {

"s": i, # step -> s

"t": response.choices[0].logprobs.tokens[i], # generated_token -> t

"lp": round(response.choices[0].logprobs.token_logprobs[i], 2), # logprob of generated token

"a": [] # top_20_tokens -> a (alternatives)

}


# Add only top 5 alternatives to save space

if response.choices[0].logprobs.top_logprobs:

for tok, value in response.choices[0].logprobs.top_logprobs[i].items(): # Only top 5

step_probs["a"].append([

tok,

round(value, 2)

]) # Use array instead of dict


token_probs.append(step_probs)

if step_probs['a']:

mean_confs.append(round(-sum(p[1] for p in step_probs['a']) / len(step_probs['a']), 2))

else:

mean_confs.append(0)

tokens.append(response.choices[0].logprobs.tokens[i])

log_probs.append(round(response.choices[0].logprobs.token_logprobs[i], 2))


return {

"text": generated_text,

"probs": token_probs, # token_probabilities -> probs,

"mean_confidences": mean_confs, # mean_confidences -> mean_confs

"tokens": tokens,

"log_probs": log_probs # log_probs -> log_probs

}


def get_token_probabilities(prompt, messages):

"""Get token probabilities from the vLLM server using chat completions API."""

response = client.chat.completions.create(

model=CONFIG['model_path'],

messages=messages,

max_tokens=CONFIG['max_tokens'],

temperature=CONFIG['temperature'],

top_p=CONFIG['top_p'],

logprobs=True,

top_logprobs=20,

extra_body={"top_k": CONFIG['top_k']},

)


generated_text = response.choices[0].message.content

token_probs = []

mean_confs = []

tokens = []

log_probs = []


if response.choices[0].logprobs and response.choices[0].logprobs.content:

for i, token_data in enumerate(response.choices[0].logprobs.content):

step_probs = {

"s": i,

"t": token_data.token,

"lp": round(token_data.logprob, 2),

"a": []

}


if token_data.top_logprobs:

for logprob_data in token_data.top_logprobs[:5]: # Top 5 alternatives

step_probs["a"].append([

logprob_data.token,

round(logprob_data.logprob, 2)

])


token_probs.append(step_probs)

if step_probs['a']:

mean_confs.append(round(-sum(p[1] for p in step_probs['a']) / len(step_probs['a']), 2))

else:

mean_confs.append(0)

tokens.append(token_data.token)

log_probs.append(round(token_data.logprob, 2))


return {

"text": generated_text,

"probs": token_probs,

"mean_confidences": mean_confs,

"tokens": tokens,

"log_probs": log_probs

}


def prepare_messages(prompt, template):

"""Prepare messages based on template."""

if template == "dpsk_qwen_0528":

return [

{"role": "system", "content": "è¯¥åŠ©æ‰‹ä¸ºDeepSeek-R1ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ ã€‚\nä»Šå¤©æ˜¯2025å¹´5æœˆ28æ—¥ï¼Œæ˜ŸæœŸä¸€ã€‚\n"},

{"role": "user", "content": prompt}

]

elif template == 'qwen3':

return [

{"role": "user", "content": prompt + "\nPlease reason step by step, and put your final answer within \\boxed{}."}

]

elif template == 'gpt':

# For GPT models, we'll prepare a simple string message first

return prompt + "\nPlease reason step by step, and put your final answer within \\boxed{{}}."

else:

return [{"role": "user", "content": prompt}]


def generate_single_trace(question_meta, trace_idx, output_dir):

"""Generate a single trace for a question. This function will be run in parallel."""

try:

prompt = question_meta["prompt"]

q_idx = question_meta["question_id"]


messages = prepare_messages(prompt, CONFIG['template'])


# Handle GPT models differently

if CONFIG['template'] == 'gpt':

# Apply chat template with reasoning effort for GPT models

if tokenizer:

formatted_messages = tokenizer.apply_chat_template(

conversation=[

{"role": "user", "content": messages}

],

add_generation_prompt=True,

reasoning_effort=CONFIG['reasoning_effort'],

tokenize=False,

)

result = get_gpt_token_probabilities(messages=formatted_messages, max_tokens=CONFIG['max_tokens'])

else:

# Fallback if tokenizer is not available

result = get_gpt_token_probabilities(messages=messages, max_tokens=CONFIG['max_tokens'])

else:

# Use chat completions for other models

result = get_token_probabilities(prompt, messages)


# Prepare trace data

trace_data_processed = {

"question_meta": question_meta,

"trace_id": trace_idx,

"response": result["text"],

"tokens": result["tokens"],

"mean_confidences": result["mean_confidences"],

"log_probs": result["log_probs"],

"messages": messages,

}


# Thread-safe file writing

processed_file = os.path.join(output_dir, f"{q_idx}_processed.jsonl")

with file_lock:

with open(processed_file, "a", encoding="utf-8") as f:

f.write(json.dumps(trace_data_processed, ensure_ascii=False) + "\n")


return True, None


except Exception as e:

return False, f"Error in question {question_meta['question_id']}, trace {trace_idx}: {e}"


def process_question_parallel(question, q_idx, output_dir):

"""Process a single question and generate multiple traces in parallel."""

prompt = question.get("problem", question.get("question", question.get("prompt", "")))


if not prompt:

print(f"Warning: No prompt found in question {q_idx}")

return 0


question_meta = {

"question_id": q_idx,

"original_question": question,

"prompt": prompt,

}


# Generate traces in parallel

completed_traces = 0

with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['max_workers_per_question']) as executor:

# Submit all trace generation tasks

future_to_trace = {

executor.submit(generate_single_trace, question_meta, trace_idx, output_dir): trace_idx

for trace_idx in range(CONFIG['max_samples_per_question'])

}


# Collect results as they complete

for future in concurrent.futures.as_completed(future_to_trace):

trace_idx = future_to_trace[future]

try:

success, error_msg = future.result()

if success:

completed_traces += 1

else:

print(f"Error: {error_msg}")

except Exception as e:

print(f"Exception in trace {trace_idx}: {e}")


return completed_traces


def process_single_question_wrapper(args):

"""Wrapper function for processing a single question (needed for parallel execution)."""

question, q_idx, output_dir = args

return q_idx, process_question_parallel(question, q_idx, output_dir)


def process_dataset_parallel(dataset_file, output_dir):

"""Process entire dataset with parallel processing."""

os.makedirs(output_dir, exist_ok=True)

print(f"Created output directory: {output_dir}")


# Load dataset

questions = []

try:

with open(dataset_file, "r", encoding="utf-8") as f:

for line in f:

questions.append(json.loads(line.strip()))

print(f"Loaded {len(questions)} questions from {dataset_file}")

except FileNotFoundError:

print(f"Error: {dataset_file} not found!")

return None


# Process questions in parallel

all_results = []

total_traces = 0


# Prepare arguments for parallel processing

question_args = [(question, q_idx, output_dir) for q_idx, question in enumerate(questions)]


print(f"Processing {len(questions)} questions with up to {CONFIG['max_workers']} parallel workers...")


with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:

# Submit all question processing tasks

future_to_question = {

executor.submit(process_single_question_wrapper, args): args[1]

for args in question_args

}


# Use tqdm to track progress

with tqdm(total=len(questions), desc="Processing questions") as pbar:

for future in concurrent.futures.as_completed(future_to_question):

q_idx = future_to_question[future]

try:

result_q_idx, traces_completed = future.result()

total_traces += traces_completed

all_results.append({

"question_id": result_q_idx,

"total_traces": traces_completed,

"file_path": os.path.join(output_dir, f"{result_q_idx}.jsonl")

})

pbar.update(1)

pbar.set_postfix({

'completed_traces': total_traces,

'avg_traces': f"{total_traces / len(all_results):.1f}" if all_results else "0"

})

except Exception as e:

print(f"Exception processing question {q_idx}: {e}")

pbar.update(1)


# Save summary

summary_file = os.path.join(output_dir, "summary.json")

summary = {

"model": CONFIG['model_path'],

"model_config": model_config,

"dataset_file": dataset_file,

"total_questions": len(questions),

"completed_questions": len(all_results),

"total_traces": total_traces,

"average_traces_per_question": total_traces / len(all_results) if all_results else 0,

"output_directory": output_dir,

"timestamp": datetime.now().isoformat(),

"reasoning_effort": CONFIG.get('reasoning_effort', 'N/A'),

"parallel_config": {

"max_workers": CONFIG['max_workers'],

"max_workers_per_question": CONFIG['max_workers_per_question']

}

}


with open(summary_file, "w", encoding="utf-8") as f:

json.dump(summary, f, indent=2, ensure_ascii=False)


print(f"\nâœ… Completed! Generated {total_traces} total traces")

print(f"ðŸ“ Results saved to: {output_dir}")

print(f"ðŸ“Š Summary: {summary_file}")

print(f"ðŸ“ˆ Average traces per question: {total_traces / len(all_results):.1f}")


return output_dir


def check_results(output_dir):

"""Check and display results from output directory."""

if not os.path.exists(output_dir):

print(f"Directory {output_dir} not found!")

return


# Load summary

summary_file = os.path.join(output_dir, "summary.json")

if os.path.exists(summary_file):

with open(summary_file, "r", encoding="utf-8") as f:

summary = json.load(f)

print(f"\nSummary:")

print(f" Model: {summary['model']}")

print(f" Total questions: {summary['total_questions']}")

print(f" Total traces: {summary['total_traces']}")

print(f" Average traces per question: {summary['average_traces_per_question']:.1f}")

print(f" Reasoning effort: {summary.get('reasoning_effort', 'N/A')}")

if 'parallel_config' in summary:

print(f" Max workers: {summary['parallel_config']['max_workers']}")

print(f" Max workers per question: {summary['parallel_config']['max_workers_per_question']}")


# Check individual files

question_files = [f for f in os.listdir(output_dir) if f.endswith('.jsonl')]

question_files.sort(key=lambda x: int(x.split('.')[0].split('_')[0]) if x.split('.')[0].split('_')[0].isdigit() else float('inf'))


print(f"\nFound {len(question_files)} question files")


# Show sample results

for filename in question_files[:3]:

if '_processed' in filename:

continue


filepath = os.path.join(output_dir, filename)

if os.path.exists(filepath):

with open(filepath, "r", encoding="utf-8") as f:

lines = f.readlines()


if lines:

first_trace = json.loads(lines[0].strip())

print(f"\n{filename}:")

print(f" Traces: {len(lines)}")

print(f" First response preview: {first_trace['response'][:150]}...")


# ===========================

# Performance Monitoring

# ===========================


def monitor_performance():

"""Monitor and suggest optimal worker configuration."""

import psutil


cpu_count = psutil.cpu_count()

memory_gb = psutil.virtual_memory().total / (1024**3)


print(f"\nðŸ”§ System Information:")

print(f" CPU cores: {cpu_count}")

print(f" Total memory: {memory_gb:.1f} GB")


# Suggest optimal configuration

suggested_workers = min(cpu_count * 2, 16) # Generally good for I/O bound tasks

suggested_workers_per_q = min(4, suggested_workers // 2)


print(f"\nðŸ’¡ Suggested Configuration:")

print(f" MAX_WORKERS: {suggested_workers}")

print(f" MAX_WORKERS_PER_QUESTION: {suggested_workers_per_q}")


if CONFIG['max_workers'] > suggested_workers:

print(f"âš ï¸ Current MAX_WORKERS ({CONFIG['max_workers']}) might be too high for your system")


return suggested_workers, suggested_workers_per_q


# ===========================

# Main Execution

# ===========================


# Monitor system performance

monitor_performance()


print(f"\nðŸš€ Starting parallel processing:")

print(f" Model: {CONFIG['model_path']}")

print(f" Template: {CONFIG['template']}")

print(f" Dataset: {CONFIG['dataset']}")

print(f" Traces per question: {CONFIG['max_samples_per_question']}")

print(f" Max workers: {CONFIG['max_workers']}")

print(f" Max workers per question: {CONFIG['max_workers_per_question']}")

print(f" Output directory: {CONFIG['output_dir']}")

if CONFIG['template'] == 'gpt':

print(f" Reasoning effort: {CONFIG['reasoning_effort']}")


start_time = time.time()


# Process the dataset with parallel processing

output_dir = process_dataset_parallel(

dataset_file=CONFIG['dataset'],

output_dir=CONFIG['output_dir']

)


end_time = time.time()

processing_time = end_time - start_time


# Check results

if output_dir:

print("\n" + "="*50)

print("Checking generated results...")

check_results(output_dir)


print(f"\nâš¡ Processing completed in {processing_time:.2f} seconds!")

print(f"ðŸš€ Speed improvement with parallel processing!")

print("Note: vLLM server is still running. Stop it manually if needed.")

Example 2: Offline Voting

import os

import json

import pickle

import numpy as np

import pandas as pd

from collections import Counter, defaultdict

from tqdm import tqdm

import glob

import random

from typing import Dict, List, Tuple, Optional, Any

from dynasor.core.evaluator import math_equal



TRACE_DIRS = [YOURDIR1, YOURDIR2, ...]


# ============================================

# SECTION 1: Math Evaluation Functions

# ============================================



def parse_func(s):

for f in [parse_latex, parse_expr, latex2sympy]:

try:

return f(s.replace("\\\\", "\\"))

except:

try:

return f(s)

except:

pass

return s


def quick_parse(text):

"""Quick parse to remove LaTeX text formatting"""

if '\\text{' in text and '}' in text:

while '\\text{' in text:

start = text.find('\\text{')

if start == -1:

break

end = text.find('}', start)

if end == -1:

break

content = text[start + 6:end]

text = text[:start] + content + text[end + 1:]

return text


# ============================================

# SECTION 2: Answer Extraction

# ============================================


def extract_answer(text):

"""Extract boxed answer from text"""

if "boxed" in text:

ans = text.split("boxed")[-1]

if len(ans) == 0:

return ""

elif ans[0] == "{":

stack = 1

a = ""

for c in ans[1:]:

if c == "{":

stack += 1

a += c

elif c == "}":

stack -= 1

if stack == 0:

break

a += c

else:

a += c

else:

a = ans.split("$")[0].strip()

return a.strip()

return None


# ============================================

# SECTION 3: Confidence Metrics Calculation

# ============================================


def calculate_confidence_stats(conf_list, tokens):

"""Calculate various confidence statistics"""

if not conf_list:

return {}


assert len(conf_list) == len(tokens), "Confidence list and tokens must have same length"


conf_array = np.array(conf_list)

total_tokens = len(conf_array)


stats = {

'mean_confidence': np.mean(conf_array)

}


# First/Last N tokens

for n in [2048]:

if total_tokens >= n:

stats[f'tail_{n}_mean_conf'] = np.mean(conf_array[-n:])

else:

stats[f'tail_{n}_mean_conf'] = np.mean(conf_array)


# First/Last percentage

for ratio in [0.1]:

n_tokens = max(1, int(total_tokens * ratio))

stats[f'tail_{ratio}_mean_conf'] = np.mean(conf_array[-n_tokens:])


# Sliding window metrics

window_sizes = [2048]

bottom_percentages = [0.1, 0.5]


for window_size in window_sizes:

if total_tokens < window_size:

stats[f'min_sliding_{window_size}_mean_conf'] = np.mean(conf_array)

for percent in bottom_percentages:

stats[f'bottom_{percent}_sliding_{window_size}_mean_conf'] = np.mean(conf_array)

else:

# Optimized sliding window

cumsum = np.cumsum(conf_array)

window_sums = cumsum[window_size-1:]

window_sums[1:] -= cumsum[:-window_size]

window_means = window_sums / window_size

stats[f'min_sliding_{window_size}_mean_conf'] = np.min(window_means)


sorted_means = np.sort(window_means)


for percent in bottom_percentages:

idx = int(len(sorted_means) * percent)

stats[f'bottom_{percent}_sliding_{window_size}_mean_conf'] = sorted_means[:idx].mean()


return stats


# ============================================

# SECTION 4: Voting Strategies

# ============================================


def majority_vote(traces):

"""Perform majority voting based on extracted answers"""

if not traces:

return None, None


answer_counts = {}

answer_to_parsed = {}


for trace in traces:

extracted_answer = trace.get('extracted_answer')

parsed_answer = trace.get('parsed_answer')


if extracted_answer is not None:

answer_str = str(extracted_answer)

answer_counts[answer_str] = answer_counts.get(answer_str, 0) + 1

if answer_str not in answer_to_parsed:

answer_to_parsed[answer_str] = parsed_answer


if not answer_counts:

return None, None


voted_answer = max(answer_counts.keys(), key=lambda x: answer_counts[x])

voted_parsed = answer_to_parsed[voted_answer]


return voted_answer, voted_parsed


def weighted_majority_vote(traces, weight_key='mean_confidence'):

"""Perform weighted majority voting"""

if not traces:

return None, None


answer_weights = {}

answer_to_parsed = {}


for trace in traces:

extracted_answer = trace.get('extracted_answer')

parsed_answer = trace.get('parsed_answer')

weight = trace.get(weight_key)


if extracted_answer is not None and weight is not None:

answer_str = str(extracted_answer)

answer_weights[answer_str] = answer_weights.get(answer_str, 0.0) + float(weight)

if answer_str not in answer_to_parsed:

answer_to_parsed[answer_str] = parsed_answer


if not answer_weights:

return None, None


voted_answer = max(answer_weights.keys(), key=lambda x: answer_weights[x])

voted_parsed = answer_to_parsed[voted_answer]


return voted_answer, voted_parsed


def top_percent_vote(traces, weight_key='mean_confidence', top_percent=0.1, vote_strategy='majority'):

"""

First filter top percent of traces by weight_key, then perform voting


Args:

traces: List of trace dictionaries

weight_key: Key to use for filtering (e.g., 'mean_confidence')

top_percent: Percentage of top traces to keep (e.g., 0.1 for top 10%)

vote_strategy: 'majority' or 'weighted'


Returns:

voted_answer, voted_parsed

"""

if not traces:

return None, None


# Filter traces that have the weight_key and valid answers

valid_traces = [t for t in traces if weight_key in t and t.get('extracted_answer') is not None]


if not valid_traces:

return None, None


# Sort traces by weight_key in descending order (higher is better)

sorted_traces = sorted(valid_traces, key=lambda x: x[weight_key], reverse=True)


# Select top percent

n_top = max(1, int(len(sorted_traces) * top_percent))

top_traces = sorted_traces[:n_top]


# Apply voting strategy on filtered traces

if vote_strategy == 'majority':

return majority_vote(top_traces)

elif vote_strategy == 'weighted':

return weighted_majority_vote(top_traces, weight_key)

else:

raise ValueError(f"Unknown vote_strategy: {vote_strategy}")


# ============================================

# SECTION 5: JSONL Processing

# ============================================


def process_jsonl_file(file_path, ground_truth=None):

"""Process a single JSONL file and extract traces with metrics"""

traces = []


with open(file_path, 'r') as f:

lines = f.readlines()


for line_num, line in enumerate(lines):

if not line.strip():

continue


try:

data = json.loads(line)


# Extract response and confidence data

response = data.get('response', '')

mean_confidences = data.get('mean_confidences', [])

tokens = data.get('tokens', [])

question_meta = data['question_meta']['original_question']


# Extract answer

extracted_answer = extract_answer(response)

parsed_answer = parse_func(extracted_answer) if extracted_answer else None


# Get ground truth

if ground_truth is None:

# Try to extract from question_meta

for field in ['answer', 'solution', 'target']:

if field in question_meta:

ground_truth = str(question_meta[field]).strip()

break


# Calculate confidence statistics

conf_stats = calculate_confidence_stats(mean_confidences, tokens)


# Check correctness

is_correct = False

if extracted_answer is not None and ground_truth is not None:

is_correct = math_equal(extracted_answer, ground_truth)


# Create trace entry

trace = {

'trace_id': data.get('trace_id', line_num),

'extracted_answer': extracted_answer,

'parsed_answer': parsed_answer,

'is_correct': is_correct,

'ground_truth': ground_truth,

'response': response,

**conf_stats

}


traces.append(trace)


except Exception as e:

print(f"Error processing line {line_num} in {file_path}: {e}")

continue


return traces


def process_multiple_jsonls(file_pattern, ground_truth_map=None):

"""Process multiple JSONL files matching a pattern"""

files = glob.glob(file_pattern)

all_data = defaultdict(list)


for file_path in tqdm(files, desc="Processing JSONL files"):

# Extract question ID from filename if possible

filename = os.path.basename(file_path)

question_id = None


# Try to extract question ID (adjust pattern as needed)

if '_processed.jsonl' in filename:

try:

question_id = int(filename.replace('_processed.jsonl', ''))

except:

question_id = filename

else:

question_id = filename


# Get ground truth for this question

ground_truth = None

if ground_truth_map and question_id in ground_truth_map:

ground_truth = ground_truth_map[question_id]


# Process the file

traces = process_jsonl_file(file_path, ground_truth)


if traces:

all_data[question_id] = traces


return dict(all_data)


def process_multiple_dirs_jsonls(trace_dirs, file_pattern="*_processed.jsonl", ground_truth_map=None):

"""

Process JSONL files from multiple directories and merge traces with same filename


Args:

trace_dirs: List of directory paths to search for JSONL files

file_pattern: File pattern to match (e.g., "*_processed.jsonl")

ground_truth_map: Optional dictionary mapping question IDs to ground truth answers


Returns:

Dictionary where keys are question IDs and values are lists of merged traces

"""

all_data = defaultdict(list)


# First, collect all unique filenames across all directories

all_filenames = set()

dir_file_mapping = defaultdict(list) # Track which dirs have which files


for trace_dir in trace_dirs:

if not os.path.exists(trace_dir):

print(f"Warning: Directory {trace_dir} does not exist, skipping...")

continue


pattern = os.path.join(trace_dir, file_pattern)

files = glob.glob(pattern)

print(f"Found {len(files)} files in {trace_dir}")


for file_path in files:

filename = os.path.basename(file_path)

all_filenames.add(filename)

dir_file_mapping[filename].append(trace_dir)


print(f"Total unique filenames found: {len(all_filenames)}")


# Process each unique filename across all directories

for filename in tqdm(all_filenames, desc="Processing unique files"):

# Extract question ID from filename

question_id = None

if '_processed.jsonl' in filename:

try:

question_id = int(filename.replace('_processed.jsonl', ''))

except:

question_id = filename

else:

question_id = filename


# Get ground truth for this question

ground_truth = None

if ground_truth_map and question_id in ground_truth_map:

ground_truth = ground_truth_map[question_id]


# Collect traces from all directories for this filename

merged_traces = []

dirs_with_file = dir_file_mapping[filename]


for trace_dir in dirs_with_file:

file_path = os.path.join(trace_dir, filename)

if os.path.exists(file_path):

try:

traces = process_jsonl_file(file_path, ground_truth)


# Add directory info to each trace for identification

for i, trace in enumerate(traces):

trace['source_dir'] = trace_dir

trace['source_file'] = filename

# Create unique trace ID combining dir and original trace ID

original_trace_id = trace.get('trace_id', i)

trace['trace_id'] = f"{os.path.basename(trace_dir)}_{original_trace_id}"


merged_traces.extend(traces)


except Exception as e:

print(f"Error processing {file_path}: {e}")

continue


if merged_traces:

all_data[question_id] = merged_traces

print(f"Question {question_id}: Merged {len(merged_traces)} traces from {len(dirs_with_file)} directories")


return dict(all_data)


# ============================================

# SECTION 6: Analysis and Evaluation

# ============================================


def analyze_voting_performance(data, voting_sizes=[1, 2, 4, 8, 16, 32],

strategy='majority', weight_key='mean_confidence',

n_trials=1, seed=42, top_percent=None):

"""Analyze voting performance across different ensemble sizes"""


random.seed(seed)

np.random.seed(seed)


results = {}

for vote_size in voting_sizes:

accuracies = []


for trial in range(n_trials):

correct = 0

total = 0


for question_id, traces in data.items():

if len(traces) < vote_size:

continue


# Sample traces

sampled = random.sample(traces, vote_size)


# Apply voting strategy

if strategy == 'majority':

voted_answer, _ = majority_vote(sampled)

elif strategy == 'weighted':

voted_answer, _ = weighted_majority_vote(sampled, weight_key)

elif strategy == 'top_percent':

voted_answer, _ = top_percent_vote(sampled, weight_key, top_percent, 'majority')

elif strategy == 'top_percent_weighted':

voted_answer, _ = top_percent_vote(sampled, weight_key, top_percent, 'weighted')

else:

raise ValueError(f"Unknown strategy: {strategy}")


# Check correctness

if voted_answer is not None and sampled[0]['ground_truth'] is not None:

ground_truth = sampled[0]['ground_truth']

if math_equal(voted_answer, ground_truth):

correct += 1

total += 1


if total > 0:

accuracies.append(correct / total)


if accuracies:

results[vote_size] = {

'accuracy_mean': np.mean(accuracies),

'accuracy_std': np.std(accuracies)

}


return results


def analyze_top_percent_strategies(data, voting_sizes=[1, 2, 4, 8],

weight_keys=['mean_confidence', 'tail_2048_mean_conf'],

top_percents=[0.1, 0.2, 0.3, 0.5],

n_trials=1, seed=42):

"""

Comprehensive analysis of top percent filtering strategies


Args:

data: Processed trace data

voting_sizes: List of ensemble sizes to test

weight_keys: List of confidence metrics to use for filtering

top_percents: List of top percentages to test (e.g., [0.1, 0.2] for top 10%, 20%)

n_trials: Number of random trials per configuration

seed: Random seed for reproducibility


Returns:

Dictionary with results for each configuration

"""


results = {}


# Test each combination of parameters

for weight_key in weight_keys:

for top_percent in top_percents:

for vote_strategy in ['weighted']:

strategy_name = f"top_{int(top_percent*100)}%_{vote_strategy}_{weight_key}"


print(f"Testing {strategy_name}...")


strategy = 'top_percent' if vote_strategy == 'majority' else 'top_percent_weighted'


strategy_results = analyze_voting_performance(

data,

voting_sizes=voting_sizes,

strategy=strategy,

weight_key=weight_key,

n_trials=n_trials,

seed=seed,

top_percent=top_percent

)


results[strategy_name] = strategy_results


return results


def analyze_directory_distribution(data):

"""Analyze the distribution of traces across source directories"""

print("\n" + "="*60)

print("DIRECTORY DISTRIBUTION ANALYSIS")

print("="*60)


dir_stats = defaultdict(lambda: {'total_traces': 0, 'questions': set()})


for question_id, traces in data.items():

for trace in traces:

source_dir = trace.get('source_dir', 'unknown')

dir_stats[source_dir]['total_traces'] += 1

dir_stats[source_dir]['questions'].add(question_id)


print(f"{'Directory':<50} {'Traces':<10} {'Questions':<10}")

print("-" * 72)


for dir_name, stats in dir_stats.items():

short_name = os.path.basename(dir_name) if dir_name != 'unknown' else 'unknown'

print(f"{short_name:<50} {stats['total_traces']:<10} {len(stats['questions']):<10}")


return dir_stats


# ============================================

# MAIN EXECUTION CELL

# ============================================


def main_analysis_multi_dir(trace_dirs=None, file_pattern="*_processed.jsonl",

ground_truth_file=None, output_dir="./results"):

"""

Main analysis function for multiple directories


Args:

trace_dirs: List of directory paths to search for JSONL files

file_pattern: File pattern to match (e.g., "*_processed.jsonl")

ground_truth_file: Optional pickle file with ground truth answers

output_dir: Directory to save results

"""


if trace_dirs is None:

trace_dirs = TRACE_DIRS


print("="*60)

print("ENHANCED MULTI-DIRECTORY JSONL VOTING ANALYSIS")

print("="*60)

print(f"Processing {len(trace_dirs)} directories:")

for i, dir_path in enumerate(trace_dirs):

print(f" {i+1}. {dir_path}")


# Load ground truth if provided

ground_truth_map = None

if ground_truth_file and os.path.exists(ground_truth_file):

with open(ground_truth_file, 'rb') as f:

ground_truth_map = pickle.load(f)

print(f"Loaded ground truth from {ground_truth_file}")


# Process JSONL files from multiple directories

print(f"\nProcessing files with pattern: {file_pattern}")

data = process_multiple_dirs_jsonls(trace_dirs, file_pattern, ground_truth_map)


print(f"Processed {len(data)} questions")

total_traces = sum(len(traces) for traces in data.values())

print(f"Total traces: {total_traces}")


# Analyze directory distribution

dir_stats = analyze_directory_distribution(data)


# Calculate per-question statistics

print("\n" + "="*60)

print("PER-QUESTION STATISTICS")

print("="*60)


question_items = list(data.items())

for q_id, traces in question_items[:5]: # Show first 5 questions

correct = sum(1 for t in traces if t['is_correct'])

print(f"Question {q_id}: {correct}/{len(traces)} correct ({correct/len(traces):.2%})")

if traces:

mean_conf = np.mean([t.get('mean_confidence', 0) for t in traces if 'mean_confidence' in t])

print(f" Mean confidence: {mean_conf:.4f}")


# Show directory breakdown

dir_breakdown = defaultdict(int)

for trace in traces:

dir_name = os.path.basename(trace.get('source_dir', 'unknown'))

dir_breakdown[dir_name] += 1

print(f" Directory breakdown: {dict(dir_breakdown)}")


# Test baseline strategies

print("\n" + "="*60)

print("BASELINE VOTING STRATEGIES")

print("="*60)


baseline_strategies = [

('majority', 'Majority Vote', None),

('weighted', 'Weighted Vote (mean_conf)', 'mean_confidence'),

('weighted', 'Weighted Vote (tail_2048)', 'tail_2048_mean_conf'),


]


voting_sizes = [1, 2, 4, 8, 16, 32, 64, 128]


all_results = {}


for strategy, name, weight_key in baseline_strategies:

print(f"\n{name}:")

results = analyze_voting_performance(

data,

voting_sizes=voting_sizes,

strategy=strategy,

weight_key=weight_key,

n_trials=10

)


all_results[name] = results


print(f"{'Size':<6} {'Accuracy':<12} {'Std Dev':<10}")

print("-" * 30)

for size in voting_sizes:

if size in results:

acc = results[size]['accuracy_mean']

std = results[size]['accuracy_std']

print(f"{size:<6} {acc:<12.4f} {std:<10.4f}")


# Test top percent filtering strategies

print("\n" + "="*60)

print("TOP PERCENT FILTERING STRATEGIES")

print("="*60)


top_percent_results = analyze_top_percent_strategies(

data,

voting_sizes=voting_sizes, # Use smaller sizes for top percent

weight_keys=['mean_confidence', 'tail_2048_mean_conf', 'bottom_0.1_sliding_2048_mean_conf'],

top_percents=[0.1, 0.9],

n_trials=10

)


all_results.update(top_percent_results)


# Display top percent results

for strategy_name, strategy_results in top_percent_results.items():

print(f"\n{strategy_name}:")

print(f"{'Size':<6} {'Accuracy':<12} {'Std Dev':<10}")

print("-" * 30)

for size in voting_sizes:

if size in strategy_results:

acc = strategy_results[size]['accuracy_mean']

std = strategy_results[size]['accuracy_std']

print(f"{size:<6} {acc:<12.4f} {std:<10.4f}")


# Save results

os.makedirs(output_dir, exist_ok=True)


# Save processed data

data_path = os.path.join(output_dir, "processed_data_multi_dir.pkl")

with open(data_path, 'wb') as f:

pickle.dump(data, f)

print(f"\nâœ“ Saved processed data to {data_path}")


# Save voting results

results_path = os.path.join(output_dir, "voting_results_multi_dir.pkl")

with open(results_path, 'wb') as f:

pickle.dump(all_results, f)

print(f"âœ“ Saved voting results to {results_path}")


# Create comprehensive summary DataFrame

summary_data = []

for strategy_name, strategy_results in all_results.items():

for size, metrics in strategy_results.items():

summary_data.append({

'Strategy': strategy_name,

'Ensemble Size': size,

'Accuracy': metrics['accuracy_mean'],

'Std Dev': metrics['accuracy_std']

})


df_summary = pd.DataFrame(summary_data)

csv_path = os.path.join(output_dir, "voting_summary_multi_dir.csv")

df_summary.to_csv(csv_path, index=False)

print(f"âœ“ Saved summary CSV to {csv_path}")


# Find best performing strategies

print("\n" + "="*60)

print("BEST PERFORMING STRATEGIES")

print("="*60)


# Group by ensemble size and find best accuracy for each

for size in voting_sizes:

size_results = df_summary[df_summary['Ensemble Size'] == size]

if not size_results.empty:

best_row = size_results.loc[size_results['Accuracy'].idxmax()]

print(f"Size {size}: {best_row['Strategy']} (Accuracy: {best_row['Accuracy']:.4f})")


return data, all_results, df_summary, dir_stats


# ============================================

# EXAMPLE USAGE

# ============================================


if __name__ == "__main__":

# Example usage - modify these paths for your data


# Use the predefined TRACE_DIRS or specify your own

custom_dirs = None # Set to your list of directories if different from TRACE_DIRS


file_pattern = "*_processed.jsonl" # Adjust this pattern

ground_truth_file = None # Optional: "./ground_truth.pkl"

output_directory = "./voting_results_multi_dir"


# Run the analysis

data, results, summary_df, dir_stats = main_analysis_multi_dir(

trace_dirs=custom_dirs, # Will use TRACE_DIRS if None

file_pattern=file_pattern,

ground_truth_file=ground_truth_file,

output_dir=output_directory

)


# Display summary

print("\n" + "="*60)

print("FINAL SUMMARY")

print("="*60)

print(summary_df.to_string(index=False))


# Show some example merged data

print("\n" + "="*60)

print("EXAMPLE MERGED DATA")

print("="*60)


for q_id, traces in list(data.items())[:2]: # Show first 2 questions

print(f"\nQuestion {q_id} ({len(traces)} traces):")

for trace in traces[:3]: # Show first 3 traces

source_dir = os.path.basename(trace.get('source_dir', 'unknown'))

correct = trace.get('is_correct', False)

conf = trace.get('mean_confidence', 0)

answer = trace.get('extracted_answer', 'None')

print(f" {source_dir}: {answer} (correct: {correct}, conf: {conf:.4f})")

Example 3: Online Generation

python

import openai

import json

from tqdm import tqdm

import time

import os

from datetime import datetime

import numpy as np

from collections import Counter

from dynasor.core.evaluator import math_equal


# ===========================

# Configuration

# ===========================

MODEL_PATH = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"

MAX_TOKENS = 64000

RID = 0

QID = 0

PORT = 8000

DATASET_FILE = "aime25.jsonl"


# Online algorithm parameters

WARMUP_TRACES = 16

TOTAL_BUDGET = 32

CONFIDENCE_PERCENTILE = 90

WINDOW_SIZE = 2048


# ===========================

# Answer Extraction Functions

# ===========================

def extract_answer(text):

"""Extract boxed answer from text"""

if "boxed" in text:

ans = text.split("boxed")[-1]

if len(ans) == 0:

return ""

elif ans[0] == "{":

stack = 1

a = ""

for c in ans[1:]:

if c == "{":

stack += 1

a += c

elif c == "}":

stack -= 1

if stack == 0:

break

a += c

else:

a += c

else:

a = ans.split("$")[0].strip()

return a.strip()

return None


def parse_func(s):

for f in [parse_latex, parse_expr, latex2sympy]:

try:

return f(s.replace("\\\\", "\\"))

except:

try:

return f(s)

except:

pass

return s



# ===========================

# Data Loading

# ===========================

def load_aime25_jsonl(file_path=DATASET_FILE):

"""Load data from aime25.jsonl file"""

data = []

with open(file_path, 'r', encoding='utf-8') as file:

for line in file:

data.append(json.loads(line.strip()))

return data


# ===========================

# Confidence Calculation

# ===========================

def compute_confidence(logprobs):

"""Compute confidence score from logprobs."""

confs = []

for lp in logprobs:

confs.append(round(-sum([l.logprob for l in lp]) / len(lp), 3))

return confs


def compute_least_grouped(confs, group_size=WINDOW_SIZE):

"""Compute sliding window mean confidence with specified group size."""

if len(confs) < group_size:

return [sum(confs) / len(confs)] if confs else [0]


sliding_means = []

for i in range(len(confs) - group_size + 1):

window = confs[i:i + group_size]

sliding_means.append(round(sum(window) / len(window), 3))


return sliding_means


# ===========================

# Voting Functions

# ===========================

def weighted_majority_vote(answers, weights):

"""Perform weighted majority voting"""

if not answers:

return None


answer_weights = {}

for answer, weight in zip(answers, weights):

if answer is not None:

answer_str = str(answer)

answer_weights[answer_str] = answer_weights.get(answer_str, 0.0) + float(weight)


if not answer_weights:

return None


voted_answer = max(answer_weights.keys(), key=lambda x: answer_weights[x])

return voted_answer


# ===========================

# Trace Processing and Accuracy Calculation

# ===========================

def process_trace(choice, trace_id, ground_truth):

"""Process a single trace and calculate accuracy"""

# Extract basic info

text = choice.message.content

tokens = [t.token for t in choice.logprobs.content]


# Calculate confidence

confs = compute_confidence([t.top_logprobs for t in choice.logprobs.content])

sliding_window = compute_least_grouped(confs, group_size=WINDOW_SIZE)


# Extract and parse answer

extracted_answer = extract_answer(text)

parsed_answer = parse_func(extracted_answer) if extracted_answer else None


# Calculate correctness

is_correct = False

if extracted_answer is not None and ground_truth is not None:

is_correct = math_equal(extracted_answer, ground_truth)


trace_data = {

"trace_id": trace_id,

"stop_reason": choice.stop_reason,

"finish_reason": choice.finish_reason,

"text": text,

"tokens": tokens,

"token_count": len(tokens),

"confs": confs,

"group_confs": sliding_window,

"min_conf": min(sliding_window) if sliding_window else 0,

"extracted_answer": extracted_answer,

"parsed_answer": parsed_answer,

"is_correct": is_correct,

}


return trace_data


def calculate_statistics(traces, phase_name=""):

"""Calculate statistics for a list of traces"""

if not traces:

return {}


total_traces = len(traces)

correct_traces = sum(1 for t in traces if t['is_correct'])

total_tokens = sum(t['token_count'] for t in traces)


# Confidence statistics

min_confs = [t['min_conf'] for t in traces]


stats = {

f"{phase_name}_traces": total_traces,

f"{phase_name}_correct": correct_traces,

f"{phase_name}_accuracy": correct_traces / total_traces if total_traces > 0 else 0,

f"{phase_name}_total_tokens": total_tokens,

f"{phase_name}_avg_tokens_per_trace": total_tokens / total_traces if total_traces > 0 else 0,

f"{phase_name}_min_conf_mean": np.mean(min_confs) if min_confs else 0,

f"{phase_name}_min_conf_std": np.std(min_confs) if min_confs else 0,

}


return stats


# ===========================

# Problem Processing Function (like problemx in reference)

# ===========================

def process_problem_voting(test_json, ground_truth):

"""

Process a problem result JSON and perform voting

Similar to problemx function in reference code

"""

answers = []

bar = test_json['conf_bar']

weights = []

tokens = 0


print(f"Warmup traces: {len(test_json['warmup_traces'])}, Final traces: {len(test_json['final_traces'])}")


# Process warmup traces

for i in range(len(test_json['warmup_traces'])):

answer = extract_answer(test_json['warmup_traces'][i]['text'])

minx = min(test_json['warmup_traces'][i]['group_confs'])

tokens += len(test_json['warmup_traces'][i]['tokens'])

if minx < bar:

continue

if answer is not None:

answers.append(answer)

weights.append(1) # Use weight 1 for consistency with reference


# Process final traces

for i in range(len(test_json['final_traces'])):

tokens += len(test_json['final_traces'][i]['tokens'])

# Skip traces stopped by gconf

if test_json['final_traces'][i]['stop_reason'] is not None and 'gconf' in test_json['final_traces'][i]['stop_reason']:

continue

answer = extract_answer(test_json['final_traces'][i]['text'])

minx = min(test_json['final_traces'][i]['group_confs'])

if answer is not None:

answers.append(answer)

weights.append(1)


# Perform voting

voted = weighted_majority_vote(answers, weights)

is_correct = str(voted) == str(ground_truth) if voted is not None else False


print(f'Bar: {bar:.4f}, Voted: {voted}, Ground truth: {ground_truth}, Correct: {is_correct}, Voting answers: {len(answers)}')


return is_correct, len(answers), tokens


# ===========================

# Main Function

# ===========================

def main():

print("="*60)

print("ONLINE ALGORITHM WITH ACCURACY CALCULATION")

print("="*60)

print(f"Model: {MODEL_PATH}")

print(f"Question ID: {QID}")

print(f"Run ID: {RID}")

print(f"Warmup traces: {WARMUP_TRACES}")

print(f"Total budget: {TOTAL_BUDGET}")

print(f"Confidence percentile: {CONFIDENCE_PERCENTILE}")


# Load the data

data = load_aime25_jsonl()

print(f"Loaded {len(data)} items from {DATASET_FILE}")


# Initialize client

client = openai.OpenAI(

api_key="None",

base_url=f"http://localhost:{PORT}/v1",

timeout=None

)


# Get question and ground truth

prompt = data[QID]['question']

ground_truth = str(data[QID].get('answer', '')).strip()


messages = [

{"role": "system", "content": "è¯¥åŠ©æ‰‹ä¸ºDeepSeek-R1ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ ã€‚\nä»Šå¤©æ˜¯2025å¹´5æœˆ28æ—¥ï¼Œæ˜ŸæœŸä¸€ã€‚\n"},

{"role": "user", "content": prompt}

]


print(f"\nQuestion: {prompt}")

print(f"Ground Truth: {ground_truth}")


# ===========================

# WARMUP PHASE

# ===========================

print(f"\n{'-'*40}")

print("WARMUP PHASE")

print(f"{'-'*40}")


t0 = time.time()

responses = client.chat.completions.create(

model=MODEL_PATH,

messages=messages,

max_tokens=MAX_TOKENS,

temperature=0.6,

top_p=0.95,

logprobs=True,

top_logprobs=20,

n=WARMUP_TRACES,

extra_body={"top_k": 0},

)

t1 = time.time()


# Process warmup traces

warmup_traces = []

min_confs = []


for j in range(WARMUP_TRACES):

trace_data = process_trace(responses.choices[j], j, ground_truth)

warmup_traces.append(trace_data)

min_confs.append(trace_data["min_conf"])


# Calculate confidence bar

conf_bar = float(np.percentile(min_confs, CONFIDENCE_PERCENTILE))


# Calculate warmup statistics

warmup_stats = calculate_statistics(warmup_traces, "warmup")


print(f"Warmup time: {t1 - t0:.2f}s")

print(f"Confidence bar (P{CONFIDENCE_PERCENTILE}): {conf_bar:.4f}")

print(f"Warmup accuracy: {warmup_stats['warmup_accuracy']:.4f} ({warmup_stats['warmup_correct']}/{warmup_stats['warmup_traces']})")

print(f"Warmup total tokens: {warmup_stats['warmup_total_tokens']}")

print(f"Warmup avg tokens per trace: {warmup_stats['warmup_avg_tokens_per_trace']:.1f}")


# Show some example results

print(f"\nFirst 3 warmup traces:")

for i, trace in enumerate(warmup_traces[:3]):

print(f" Trace {i}: {trace['extracted_answer']} (correct: {trace['is_correct']}, conf: {trace['min_conf']:.4f}, tokens: {trace['token_count']})")


# ===========================

# FINAL PHASE

# ===========================

print(f"\n{'-'*40}")

print("FINAL PHASE (with early stopping)")

print(f"{'-'*40}")


real_gen = TOTAL_BUDGET - WARMUP_TRACES


t3 = time.time()

responses = client.chat.completions.create(

model=MODEL_PATH,

messages=messages,

max_tokens=MAX_TOKENS,

temperature=0.6,

top_p=0.95,

logprobs=True,

top_logprobs=20,

n=real_gen,

extra_body={

"top_k": 0,

"vllm_xargs": {

'enable_conf': True,

'window_size': WINDOW_SIZE,

'threshold': conf_bar

}

}

)

t4 = time.time()


# Process final traces

final_traces = []

for j in range(len(responses.choices)):

trace_data = process_trace(responses.choices[j], WARMUP_TRACES + j, ground_truth)

final_traces.append(trace_data)


# Calculate final statistics

final_stats = calculate_statistics(final_traces, "final")


print(f"Final time: {t4 - t3:.2f}s")

print(f"Final traces generated: {len(final_traces)} (requested: {real_gen})")

print(f"Final accuracy: {final_stats['final_accuracy']:.4f} ({final_stats['final_correct']}/{final_stats['final_traces']})")

print(f"Final total tokens: {final_stats['final_total_tokens']}")

print(f"Final avg tokens per trace: {final_stats['final_avg_tokens_per_trace']:.1f}")


# Show some example results

print(f"\nFirst 3 final traces:")

for i, trace in enumerate(final_traces[:3]):

print(f" Trace {i}: {trace['extracted_answer']} (correct: {trace['is_correct']}, conf: {trace['min_conf']:.4f}, tokens: {trace['token_count']})")


# ===========================

# OVERALL STATISTICS

# ===========================

print(f"\n{'-'*40}")

print("OVERALL STATISTICS")

print(f"{'-'*40}")


all_traces = warmup_traces + final_traces

overall_stats = calculate_statistics(all_traces, "overall")


total_time = t4 - t0

warmup_time = t1 - t0

final_time = t4 - t3


print(f"Total time: {total_time:.2f}s (warmup: {warmup_time:.2f}s, final: {final_time:.2f}s)")

print(f"Overall traces: {len(all_traces)}")

print(f"Overall accuracy: {overall_stats['overall_accuracy']:.4f} ({overall_stats['overall_correct']}/{overall_stats['overall_traces']})")

print(f"Overall total tokens: {overall_stats['overall_total_tokens']}")

print(f"Overall avg tokens per trace: {overall_stats['overall_avg_tokens_per_trace']:.1f}")


# Efficiency metrics

tokens_per_second = overall_stats['overall_total_tokens'] / total_time

traces_per_second = len(all_traces) / total_time


print(f"Tokens per second: {tokens_per_second:.1f}")

print(f"Traces per second: {traces_per_second:.2f}")


# Early stopping analysis

if final_traces:

above_threshold = sum(1 for t in final_traces if t['min_conf'] >= conf_bar)

print(f"Final traces above threshold: {above_threshold}/{len(final_traces)} ({above_threshold/len(final_traces):.2%})")


# ===========================

# SAVE RESULTS

# ===========================

results = {

"question_id": QID,

"run_id": RID,

"question": prompt,

"ground_truth": ground_truth,

"conf_bar": conf_bar,

"warmup_traces": warmup_traces,

"final_traces": final_traces,

"statistics": {

**warmup_stats,

**final_stats,

**overall_stats,

"total_time": total_time,

"warmup_time": warmup_time,

"final_time": final_time,

"tokens_per_second": tokens_per_second,

"traces_per_second": traces_per_second,

},

"config": {

"model_path": MODEL_PATH,

"warmup_traces": WARMUP_TRACES,

"total_budget": TOTAL_BUDGET,

"confidence_percentile": CONFIDENCE_PERCENTILE,

"window_size": WINDOW_SIZE,

},

"timestamp": datetime.now().isoformat(),

}


# Save results to JSON file

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

os.makedirs("outputs", exist_ok=True)

pickle.dump(results, open(f"outputs/q{QID}_r{RID}_{timestamp}.pkl", 'wb'))

print(f"Results saved to outputs/q{QID}_r{RID}_{timestamp}.pkl")

output_filename = f"outputs/q{QID}_r{RID}_{timestamp}.pkl"


print(f"\nâœ… Results saved to outputs/q{QID}_r{RID}_{timestamp}.pkl")


# ===========================

# COMPARISON WITH BASELINE

# ===========================

print(f"\n{'-'*40}")

print("COMPARISON ANALYSIS")

print(f"{'-'*40}")


# Compare warmup vs final phase

if warmup_stats['warmup_traces'] > 0 and final_stats['final_traces'] > 0:

acc_improvement = final_stats['final_accuracy'] - warmup_stats['warmup_accuracy']

print(f"Accuracy change (final - warmup): {acc_improvement:+.4f}")


token_efficiency = final_stats['final_avg_tokens_per_trace'] / warmup_stats['warmup_avg_tokens_per_trace']

print(f"Token efficiency (final/warmup): {token_efficiency:.2f}x")


# Early stopping effectiveness

total_requested = WARMUP_TRACES + (TOTAL_BUDGET - WARMUP_TRACES)

total_generated = len(all_traces)

saving_ratio = (total_requested - total_generated) / total_requested


print(f"Traces requested: {total_requested}")

print(f"Traces generated: {total_generated}")

print(f"Early stopping saving: {saving_ratio:.2%}")


# ===========================

# VOTING FOR FINAL ANSWER

# ===========================

print(f"\n{'-'*40}")

print("VOTING FOR FINAL ANSWER")

print(f"{'-'*40}")


# Collect answers above threshold for voting

voting_answers = []

voting_weights = []

total_voting_tokens = 0


# Add warmup traces above threshold (use confidence as weight)

for trace in warmup_traces:

minx = trace['min_conf']

total_voting_tokens += trace['token_count']

if minx >= conf_bar:

answer = trace['extracted_answer']

if answer is not None:

voting_answers.append(answer)

voting_weights.append(minx)


# Add final traces above threshold (use weight 1)

for trace in final_traces:

total_voting_tokens += trace['token_count']

# Skip traces that were stopped by gconf (early stopping)

if trace['stop_reason'] is not None and 'gconf' in trace['stop_reason']:

continue


minx = trace['min_conf']

# Note: final traces might not need threshold check since they were already filtered

# But keeping it consistent with the reference code

if minx >= conf_bar:

answer = trace['extracted_answer']

if answer is not None:

voting_answers.append(answer)

voting_weights.append(1)


print(f"Traces used for voting: {len(voting_answers)}")

print(f"Total tokens in voting traces: {total_voting_tokens}")


# Perform weighted majority vote

voted_answer = weighted_majority_vote(voting_answers, voting_weights)


# Check if voted answer is correct

is_voted_correct = False

if voted_answer is not None and ground_truth:

is_voted_correct = str(voted_answer) == str(ground_truth)

# Also try math_equal for more robust comparison

try:

is_voted_correct_math = math_equal(voted_answer, ground_truth)

if is_voted_correct != is_voted_correct_math:

print(f"Warning: String comparison ({is_voted_correct}) != math comparison ({is_voted_correct_math})")

is_voted_correct = is_voted_correct_math # Use math_equal as more reliable

except:

pass # Fallback to string comparison


print(f"Voted answer: {voted_answer}")

print(f"Ground truth: {ground_truth}")

print(f"Voted answer correct: {is_voted_correct}")


# Show voting breakdown

if voting_answers:

answer_counts = Counter(voting_answers)

print(f"\nVoting breakdown:")

for answer, count in answer_counts.most_common():

print(f" {answer}: {count} votes")


# ===========================

# UPDATE RESULTS WITH VOTING

# ===========================

voting_results = {

"voting_answers": voting_answers,

"voting_weights": voting_weights,

"voted_answer": voted_answer,

"is_voted_correct": is_voted_correct,

"voting_traces_count": len(voting_answers),

"voting_total_tokens": total_voting_tokens,

}


results["voting"] = voting_results

results["statistics"]["voting_traces_count"] = len(voting_answers)

results["statistics"]["voting_total_tokens"] = total_voting_tokens

results["statistics"]["is_voted_correct"] = is_voted_correct


# Update the saved file with voting results

# with open(output_filename, 'w', encoding='utf-8') as f:

# pickle.dump(results, f)


# print(f"\nâœ… Results updated with voting information: {output_filename}")


# ===========================

# FINAL SUMMARY

# ===========================

print(f"\n{'='*60}")

print("FINAL SUMMARY")

print(f"{'='*60}")

print(f"Question ID: {QID}")

print(f"Confidence bar: {conf_bar:.4f}")

print(f"Total traces generated: {len(all_traces)}")

print(f"Traces used for voting: {len(voting_answers)}")

print(f"Total tokens: {overall_stats['overall_total_tokens']}")

print(f"Voting answer: {voted_answer}")

print(f"Ground truth: {ground_truth}")

print(f"Final result: {'âœ… CORRECT' if is_voted_correct else 'âŒ INCORRECT'}")


return results


if __name__ == "__main__":

results = main()


âš ï¸ Important Notes

Make sure vLLM server is running before executing the examples

Adjust the configuration parameters based on your specific requirements

The examples use the Dynasor library for evaluation - ensure it's properly installed

For production use, consider adding proper error handling and logging

Summary

This guide has provided you with:

âœ… Complete instructions for modifying vLLM to implement DeepConf

âœ… Alternative option to download the pre-implemented PR

âœ… Three comprehensive examples for different use cases:Offline Generation with parallel processing

Offline Voting with multiple strategies

Online Generation with confidence-based early stopping

For more information and updates, visit theÂ official PR on GitHub."""




Show thinking
Of course! This document describes a new feature called DeepConf for vLLM, a popular tool for running large language models (LLMs) efficiently. Let's break it down concept by concept.

What's the Big Idea? ðŸš€
The main goal is to save time and computing power by stopping a language model from generating text when it becomes "unconfident" about what it's saying.

Imagine you ask an LLM a complex question. It might start generating a great answer, but then trail off into repetitive, nonsensical, or irrelevant text. DeepConf is a technique to detect this drop in quality and automatically stop the generation process, a concept known as early stopping.

How Does "Confidence-Based Early Stopping" Work?
What is Model "Confidence"?
When an LLM generates the next word (or "token"), it doesn't just pick one; it calculates probabilities for many possible next tokens. The "confidence" in this context is a measure of how certain the model was about its chosen token compared to the alternatives.

High Confidence: The chosen token was much more likely than any other option. The model is "sure" of its choice.

Low Confidence: There were several other tokens that were almost as likely. The model is "uncertain."
DeepConf calculates this confidence at each step.

The Sliding Window and Threshold:
DeepConf doesn't just look at the confidence of the very last token. Instead, it calculates the average confidence over a "sliding window" of recent tokens (e.g., the last 2048 tokens).

You set a threshold (a confidence score).

If the average confidence in the window drops below this threshold, the system assumes the model's output quality is degrading and stops it.

Why vLLM?
vLLM is a very fast and popular software library for serving LLMs. This feature is implemented as a modification (a "patch") to vLLM, allowing users to add this smart, resource-saving capability to their high-performance model server.

How to Implement It
The document gives you two ways to add DeepConf to your vLLM setup:

Option 1 (Easy): Download a ready-made version from a GitHub Pull Request (PR). This is the simplest way, as all the code changes are already done for you.

Option 2 (Manual): Manually copy and paste small code snippets into two specific vLLM files. This is for users who want to understand the changes or apply them to a specific version of vLLM. The changes essentially do two things:

logprobs.py: This is the core logic. The code adds variables to track the confidence score, the window, and the threshold. It also adds the check_conf_stop() function that performs the check.

output_processor.py: This is where the stop-check is actually used. A single if statement is added to call check_conf_stop() while the model is generating tokens. If it returns True, the generation is halted.

Understanding the Examples ðŸ§
The three examples show a complete workflow for using DeepConf effectively.

Example 1: Offline Generation (The Data Collection Phase)
Purpose: To generate a large amount of data from a model without early stopping.

What it does: It runs a model on many questions, generating multiple answers ("traces") for each. Crucially, it saves all the detailed probability information (logprobs) for every token generated.

Why it's needed: You need this raw data to analyze the model's confidence patterns and figure out what a "good" confidence threshold would be for your specific task.

Example 2: Offline Voting (The Analysis Phase)
Purpose: To use the confidence data collected in Example 1 to pick the best answer from many generated traces.

What it does: This script reads the output files from Example 1. It calculates different confidence metrics (e.g., average confidence, minimum confidence in a window). It then uses these scores to perform "weighted voting." An answer generated with higher confidence is given more weight.

Why it's useful: This shows that confidence scores are not just for stopping generation; they are a powerful signal of answer quality. You can use them to improve the accuracy of your system by trusting high-confidence answers more.

Example 3: Online Generation (The Real-Time Implementation)
Purpose: To demonstrate DeepConf's early stopping in a real-time scenario.

What it does: This is the grand finale.

Warmup Phase: It generates a small number of traces (WARMUP_TRACES) just like in Example 1. It analyzes their confidence scores to automatically calculate a good threshold on the fly (e.g., the 90th percentile of confidence scores).

Final Phase: It then generates the rest of the traces, but this time it enables DeepConf using the threshold it just calculated. Generations that become "unconfident" will now be stopped early.

Why it's important: This shows the full, practical application. It adapts to the problem, sets a threshold, and then uses that threshold to save computational resources on subsequent runs.







deepconf_arxiv
PDF
Here is the full article. Analyze it for the intended task of adapting a local AI LLM used in an application to utilize the confidence based techniques



Show thinking
To adapt a local AI LLM using the techniques from the paper, you should implement a system that calculates the model's "confidence" during generation and uses this score to either stop unpromising outputs early (online mode) or to better select the final answer from multiple outputs (offline mode). The most effective approach highlighted is the online mode, which uses the Lowest Group Confidence metric to terminate generation in real-time when confidence drops below a dynamically set threshold, significantly saving computational resources.

Core Concept: What is DeepConf?
DeepConf (

Deep Think with Confidence) is a method designed to make Large Language Models (LLMs) more accurate and efficient at reasoning tasks. Standard techniques often generate many possible answers ("reasoning traces") and pick the most common one (majority voting). This is computationally expensive and can be inaccurate if many of the reasoning paths are low-quality.





DeepConf improves this process by using the model's internal

confidenceâ€”a measure of its certainty in its predictionsâ€”to filter out these low-quality reasoning traces. It can be applied in two ways:



Online: During generation, it stops creating a trace as soon as its confidence drops too low, saving compute time.


Offline: After generating a full set of traces, it weighs the votes so that answers from high-confidence traces count for more.

This method requires no extra model training and can be integrated directly into existing LLM serving frameworks.

Key Confidence Metrics to Implement
The paper moves beyond a simple average confidence score, finding that localized metrics are better at identifying when a model's reasoning goes wrong. You should focus on implementing these more effective, fine-grained measurements.


Token Confidence (C
i
â€‹
 ): The foundational metric, defined as the negative average log-probability of the top-k most likely next tokens. High confidence (a less negative number) means the model is very certain about its prediction.



Group Confidence (C
G
i
â€‹

â€‹
 ): This is a localized, sliding-window average of Token Confidence over a set number of previous tokens (e.g., 2048). It provides a smoother, more stable signal of the model's confidence in an intermediate reasoning step.




Lowest Group Confidence (C
least
â€‹
 ): This is the single lowest Group Confidence value found throughout the entire trace. It is highly effective for online early stopping because a sharp drop in confidence at any point often signals a critical reasoning failure.





Bottom 10% Group Confidence: Instead of just the single lowest value, this metric averages the scores of the 10% of groups with the lowest confidence. This helps capture segments with sustained low confidence.



Tail Confidence (C
tail
â€‹
 ): This metric focuses only on the final portion of the generated text (e.g., the last 2048 tokens). It is motivated by the observation that reasoning quality often degrades near the end, and the final steps are critical for a correct answer.



Experiments show that

local metrics like Tail Confidence and Bottom 10% Group Confidence are more effective at separating correct traces from incorrect ones compared to a simple mean confidence over the entire trace.

How to Apply DeepConf: Online vs. Offline Modes
You can adapt your LLM application using two primary strategies, each serving a different goal.

Offline Mode: Improving Accuracy with Post-Processing
In this mode, you first generate a complete set of

N reasoning traces for a given prompt. You then use the confidence metrics calculated for each trace to aggregate the answers more intelligently than a simple majority vote.



Confidence-Weighted Majority Voting: Instead of giving each trace's answer one vote, you weight each vote by the trace's confidence score (e.g., its Tail Confidence or Lowest Group Confidence). This ensures that answers from more confident reasoning paths have a greater influence on the final result.




Confidence Filtering: Before voting, you can filter out the least confident traces. The paper explores two main strategies:


Aggressive Filtering (top 10%): Keep only the top 10% of traces with the highest confidence scores. This yields the largest accuracy gains in most cases but carries a risk if the model is "confidently wrong".




Conservative Filtering (top 90%): Discard only the worst 10% of traces. This is a safer approach that maintains answer diversity and consistently matches or slightly improves accuracy over standard voting.



Online Mode: Saving Compute with Real-Time Early Stopping
This is the most powerful application for efficiency. It dynamically terminates the generation of low-quality traces to avoid wasting resources. It works in a two-phase process:


Offline Warmup: For a new problem, you first generate a small, fixed number of traces (e.g., N
init
â€‹
 =16). You calculate the

Lowest Group Confidence for each of these warmup traces and use their distribution to set a stopping threshold, s. For an aggressive

DeepConf-low strategy, s would be the 90th percentile of these confidence scores; for a conservative DeepConf-high, it would be the 10th percentile.


Online Generation with Early Stopping: Now, as you generate subsequent traces, you continuously calculate the Group Confidence in a sliding window. If at any point the current


Group Confidence drops below the threshold s, the generation for that trace is immediately halted. This effectively approximates the offline filtering process in real-time, saving significant token generation costs.



Practical Implementation Steps (for vLLM)
The paper provides an appendix with the exact code edits needed to implement DeepConf in the

vLLM serving framework. This is a direct guide for adapting a system that uses vLLM.


Target Files: You only need to modify two files: vllm/v1/engine/logprobs.py and vllm/v1/engine/output_processor.py.

In logprobs.py:


Extend LogprobsProcessor: Add new fields to the class to store and manage a sliding window of confidence scores (conf_group_list, conf_grouped, conf_group_size, conf_threshold).



Initialize from Request: Add logic to read parameters like enable_conf, window_size, and threshold from the API request and initialize the new confidence-tracking variables.


Update Confidence: At the end of the _update_sample_logprobs method, add code to calculate the new token's confidence and update the sliding window average.


Add Stop-Check Helper: Implement a new method, check_conf_stop(), that returns True if the average confidence in the window has dropped below the threshold, but only once the window is full.

In output_processor.py:


Invoke Stop-Check: Insert a single if statement that calls the new req_state.logprobs_processor.check_conf_stop() method during the decoding loop. If it returns true, set the finish reason to


STOP.



Enabling via API: The feature is controlled per request through the OpenAI-compatible API. You pass the parameters inside

extra_body["vllm_xargs"], for example: "enable_conf": True, "window_size": 2048, and "threshold": 17.0. Note that you must also set

logprobs=True and top_logprobs>=2 for the logic to be active.

Summary of Findings and Recommendations
Performance: DeepConf is highly effective. In offline mode, it pushed the GPT-OSS-120B model to

99.9% accuracy on the AIME 2025 benchmark, up from 97.0% with standard voting. In online mode, it



reduced generated tokens by up to 84.7% while maintaining or improving accuracy.



Best Metrics: Local confidence signals that focus on the weakest parts of the reasoning process, like Lowest Group Confidence and Tail Confidence, consistently outperform global average confidence.


Recommended Strategy: For applications where computational efficiency is a priority, the online adaptive DeepConf is the clear choice.

Use

DeepConf-low (filtering to the top 10% of traces) for the best balance of efficiency and high accuracy, as it can reduce tokens by 43-79%.


Use DeepConf-high (filtering to the top 90%) when preserving accuracy is paramount and even small drops are unacceptable. It is a safer choice that still provides significant token savings (18-59%) with almost no change in accuracy.



Sources





Now analyze this codebase specifically for the ability to utilize like methods for agentic systems """Directory structure:

â””â”€â”€ agent-on-the-fly-memento/

Â  Â  â”œâ”€â”€ README.md

Â  Â  â”œâ”€â”€ requirements.txt

Â  Â  â”œâ”€â”€ client/

Â  Â  â”‚Â  Â â””â”€â”€ agent.py

Â  Â  â”œâ”€â”€ searxng-docker/

Â  Â  â”‚Â  Â â”œâ”€â”€ README.md

Â  Â  â”‚Â  Â â”œâ”€â”€ Caddyfile

Â  Â  â”‚Â  Â â”œâ”€â”€ docker-compose.yaml

Â  Â  â”‚Â  Â â”œâ”€â”€ LICENSE

Â  Â  â”‚Â  Â â”œâ”€â”€ searxng-docker.service.template

Â  Â  â”‚Â  Â â””â”€â”€ searxng/

Â  Â  â”‚Â  Â  Â  Â â”œâ”€â”€ limiter.toml

Â  Â  â”‚Â  Â  Â  Â â”œâ”€â”€ settings.yml

Â  Â  â”‚Â  Â  Â  Â â””â”€â”€ uwsgi.ini

Â  Â  â””â”€â”€ server/

Â  Â  Â  Â  â”œâ”€â”€ code_agent.py

Â  Â  Â  Â  â”œâ”€â”€ craw_page.py

Â  Â  Â  Â  â”œâ”€â”€ documents_tool.py

Â  Â  Â  Â  â”œâ”€â”€ excel_tool.py

Â  Â  Â  Â  â”œâ”€â”€ image_tool.py

Â  Â  Â  Â  â”œâ”€â”€ math_tool.py

Â  Â  Â  Â  â”œâ”€â”€ search_tool.py

Â  Â  Â  Â  â”œâ”€â”€ serp_search.py

Â  Â  Â  Â  â”œâ”€â”€ video_tool.py

Â  Â  Â  Â  â””â”€â”€ interpreters/

Â  Â  Â  Â  Â  Â  â”œâ”€â”€ base.py

Â  Â  Â  Â  Â  Â  â”œâ”€â”€ docker_interpreter.py

Â  Â  Â  Â  Â  Â  â”œâ”€â”€ e2b_interpreter.py

Â  Â  Â  Â  Â  Â  â”œâ”€â”€ internal_python_interpreter.py

Â  Â  Â  Â  Â  Â  â”œâ”€â”€ interpreter_error.py

Â  Â  Â  Â  Â  Â  â”œâ”€â”€ interpreters.py

Â  Â  Â  Â  Â  Â  â”œâ”€â”€ ipython_interpreter.py

Â  Â  Â  Â  Â  Â  â”œâ”€â”€ logger.py

Â  Â  Â  Â  Â  Â  â””â”€â”€ subprocess_interpreter.py





Files Content:



================================================

FILE: README.md

================================================

# Memento: Fine-tuning LLM Agents **without** Fine-tuning LLMs



> A memory-based, continual-learning framework that helps LLM agents improve from experience **without** updating model weights.



<p align="center">

Â  <b>Plannerâ€“Executor Architecture</b> â€¢ <b>Case-Based Reasoning</b> â€¢ <b>MCP Tooling</b> â€¢ <b>Memory-Augmented Learning</b>

</p>



---



<table>

Â  <tr>

Â  Â  <td align="center" width="50%">

Â  Â  Â  <img src="Figure/f1_val_test.jpg" width="90%"/>

Â  Â  Â  <br/>

Â  Â  Â  <sub><b>Memento vs. Baselines on GAIA validation and test sets.</b></sub>

Â  Â  </td>

Â  Â  <td align="center" width="50%">

Â  Â  Â  <img src="Figure/f1_tasks.jpg" width="90%"/>

Â  Â  Â  <br/>

Â  Â  Â  <sub><b>Ablation study of Memento across benchmarks.</b></sub>

Â  Â  </td>

Â  </tr>

Â  <tr>

Â  Â  <td align="center" width="50%">

Â  Â  Â  <img src="Figure/f1_iteration.jpg" width="90%"/>

Â  Â  Â  <br/>

Â  Â  Â  <sub><b>Continual learning curves across memory designs.</b></sub>

Â  Â  </td>

Â  Â  <td align="center" width="50%">

Â  Â  Â  <img src="Figure/f1_ood.jpg" width="90%"/>

Â  Â  Â  <br/>

Â  Â  Â  <sub><b>Mementoâ€™s accuracy improvement on OOD datasets.</b></sub>

Â  Â  </td>

Â  </tr>

</table>



## ðŸ“° News



- We add the SerpAPI (https://serpapi.com/search-api) MCP tool to help you avoid using the search Docker and speed up development.Â 



## ðŸ”¥ Key Features



- **No LLM weight updates.** Memento reframes continual learning as **memory-based online reinforcement learning** over a **memory-augmented MDP**. A neural **case-selection policy** guides actions; experiences are stored and reused via efficient Read/Write operations.

- **Two-stage plannerâ€“executor loop.** A CBR-driven **Planner** decomposes tasks and retrieves relevant cases; an **Executor** runs each subtask as an MCP client, orchestrating tools and writing back outcomes.

- **Comprehensive tool ecosystem.** Built-in support for web search, document processing, code execution, image/video analysis, and more through a unified MCP interface.

- **Strong benchmark performance.** Achieves competitive results across GAIA, DeepResearcher, SimpleQA, and HLE benchmarks.



---



## ðŸ§  Core Concept



**Learn from experiences, not gradients.** Memento logs successful & failed trajectories into a **Case Bank** and **retrieves by value** to steer planning and executionâ€”enabling low-cost, transferable, and online continual learning.



---



## ðŸ—ï¸ Architecture



### Core Components



- **Meta-Planner**: Breaks down high-level queries into executable subtasks using GPT-4.1

- **Executor**: Executes individual subtasks using o3 or other models via MCP tools

- **Case Memory**: Stores final-step tuples **(s_T, a_T, r_T)** for experience replay

- **MCP Tool Layer**: Unified interface for external tools and services



### Tool Ecosystem



- **Web Research**: Live search and controlled crawling via SearxNG

- **Document Processing**: Multi-format support (PDF, Office, images, audio, video)

- **Code Execution**: Sandboxed Python workspace with security controls

- **Data Analysis**: Excel processing, mathematical computations

- **Media Analysis**: Image captioning, video narration, audio transcription



---



## ðŸš€ Quick Start



### Prerequisites



- Python 3.10+

- OpenAI API key (or compatible API endpoint)

- SearxNG instance for web search



### Environment Setup



### Installation





```bash

# Create and activate conda environment



git clone https://github.com/Agent-on-the-Fly/Memento

cd Memento



conda create -n Memento python=3.11 -y

conda activate Memento



# Navigate to client directory

cd Memento/client



# Create environment file

touch .env

```





### Environment Variables Configuration



After creating the `.env` file, you need to configure the following API keys and service endpoints:



```bash

# OPENAI API

OPENAI_API_KEY=your_openai_api_key_here

OPENAI_BASE_URL=https://api.openai.com/v1Â  # or your custom endpoint



#===========================================

# Tools & Services API

#===========================================

# Chunkr API (https://chunkr.ai/)

CHUNKR_API_KEY=your_chunkr_api_key_here



# Jina API

JINA_API_KEY=your_jina_api_key_here



# ASSEMBLYAI APIÂ 

ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here

```



**Note**: Replace `your_*_api_key_here` with your actual API keys. Some services are optional depending on which tools you plan to use.



### Dependencies Installation



#### Web Scraping & Search



```bash

# Web crawling and search capabilities

pip install -U crawl4ai

crawl4ai-setup

crawl4ai-doctor

playwright install

```



#### Utility Libraries



```bash

pip install -r requirements.txt

```



### SearxNG Setup



For web search capabilities, set up SearxNG:Â 

You can follow https://github.com/searxng/searxng-docker/ to set the docker and use our setting.



```bash

# In a new terminal

cd ./Memento/searxng-docker

docker compose up -d

```





### Basic Usage



#### Interactive Mode



```bash

python client/agent.py

```



---



## ðŸ”§ Configuration



### Model Selection



- **Planner Model**: Defaults to `gpt-4.1` for task decomposition

- **Executor Model**: Defaults to `o3` for task execution

- **Custom Models**: Support for any OpenAI-compatible API



### Tool Configuration



- **Search**: Configure SearxNG instance URL

- **Code Execution**: Customize import whitelist and security settings

- **Document Processing**: Set cache directories and processing limits



---



## ðŸ“Š Performance



### Benchmark Results



- **GAIA**: 87.88% (Val, Pass@3 Top-1) and **79.40%** (Test)

- **DeepResearcher**: **66.6% F1 / 80.4% PM**, with **+4.7â€“9.6** absolute gains on OOD datasets

- **SimpleQA**: **95.0%**

- **HLE**: **24.4% PM** (close to GPT-5 at 25.32%)



### Key Insights



- **Small, high-quality memory works best**: Retrieval **K=4** yields peak F1/PM

- **Planning + CBR consistently improves performance**

- **Concise, structured planning outperforms verbose deliberation**



---



## ðŸ› ï¸ Development



### Project Structure



```

Memento/

â”œâ”€â”€ client/Â  Â  Â  Â  Â  Â  Â  Â  Â # Main agent implementation

â”‚Â  Â â””â”€â”€ agent.pyÂ  Â  Â  Â  Â  # Hierarchical client with planner-executor

â”œâ”€â”€ server/Â  Â  Â  Â  Â  Â  Â  Â  # MCP tool servers

â”‚Â  Â â”œâ”€â”€ code_agent.pyÂ  Â  Â # Code execution and workspace management

â”‚Â  Â â”œâ”€â”€ search_tool.pyÂ  Â  # Web search via SearxNG

â”‚Â  Â â”œâ”€â”€ documents_tool.py # Multi-format document processing

â”‚Â  Â â”œâ”€â”€ image_tool.pyÂ  Â  Â # Image analysis and captioning

â”‚Â  Â â”œâ”€â”€ video_tool.pyÂ  Â  Â # Video processing and narration

â”‚Â  Â â”œâ”€â”€ excel_tool.pyÂ  Â  Â # Spreadsheet processing

â”‚Â  Â â”œâ”€â”€ math_tool.pyÂ  Â  Â  # Mathematical computations

â”‚Â  Â â””â”€â”€ craw_page.pyÂ  Â  Â  # Web page crawling

â””â”€â”€ interpreters/Â  Â  Â  Â  Â  # Code execution backends

Â  Â  â”œâ”€â”€ docker_interpreter.py

Â  Â  â”œâ”€â”€ e2b_interpreter.py

Â  Â  â”œâ”€â”€ internal_python_interpreter.py

Â  Â  â””â”€â”€ subprocess_interpreter.py

```



### Adding New Tools



1. Create a new FastMCP server in the `server/` directory

2. Implement your tool functions with proper error handling

3. Register the tool with the MCP protocol

4. Update the client's server list in `agent.py`



### Custom Interpreters



Extend the `interpreters/` module to add new execution backends:



```python

from interpreters.base import BaseInterpreter



class CustomInterpreter(BaseInterpreter):

Â  Â  async def execute(self, code: str) -> str:

Â  Â  Â  Â  # Your custom execution logic

Â  Â  Â  Â  pass

```



---



## ðŸ“‹ TODO



### Upcoming Features & Improvements



- [ ] **Add Case Bank Reasoning**: Implement memory-based case retrieval and reasoning system

- [ ] **Add User Personal Memory Mechanism**: Implement user-preference searchÂ 

- [ ] **Refine Tools & Add More Tools**: Enhance existing tools and expand the tool ecosystem

- [ ] **Test More New Benchmarks**: Evaluate performance on additional benchmark datasets



---



### Limitations



- **Long-horizon tasks**: GAIA Level-3 remains challenging due to compounding errors

- **Frontier knowledge**: HLE performance limited by tooling alone

- **Open-source coverage**: Limited executor validation in fully open pipelines



---



## ðŸ™ Acknowledgement



* Some parts of the code in the toolkits and interpreters are adapted from [Camel-AI](https://github.com/camel-ai/camel).



---



## ðŸ“š Citation



If Memento helps your work, please cite:



```bibtex

@techreport{Memento2025,

Â  titleÂ  Â  Â  Â  = {Memento: Fine-tuning LLM Agents without Fine-tuning LLMs},

Â  authorÂ  Â  Â  Â = {Huichi Zhou and Yihang Chen and Siyuan Guo and Xue Yan and

Â  Â  Â  Â  Â  Â  Â  Â  Â  Kin Hei Lee and Zihan Wang and Ka Yiu Lee and Guchun Zhang and

Â  Â  Â  Â  Â  Â  Â  Â  Â  Kun Shao and Linyi Yang and Jun Wang},

Â  yearÂ  Â  Â  Â  Â = {2025},

Â  githubÂ  Â  Â  Â = {https://github.com/Agent-on-the-Fly/Memento}

}

```



---



## ðŸ¤ Contributing



We welcome contributions! Please see our contributing guidelines for:



- Bug reports and feature requests

- Code contributions and pull requests

- Documentation improvements

- Tool and interpreter extensions



---



## Star History



[![Star History Chart](https://api.star-history.com/svg?repos=Agent-on-the-Fly/Memento&type=Date)](https://www.star-history.com/#Agent-on-the-Fly/Memento&Date)



## ðŸ™ Acknowledgments



Thanks to the open-source community and contributors who made this project possible.







================================================

FILE: requirements.txt

================================================

python-dotenv==1.1.0

fastmcp==2.7.0

openai==1.75.0

tenacity==9.1.2

colorlog==6.9.0

tiktoken==0.9.0Â 

colorama==0.4.6Â 

loguru==0.7.3

retry==0.9.2Â 

pandas==2.2.3

ffmpeg==7.1.1

yt_dlp

opencv-python==4.11.0.86

scenedetect==0.6.6Â 

assemblyai==0.40.2Â Â 

python-pptx==1.0.2Â Â 

docx2markdown==0.1.1Â  Â Â 

chunkr_ai

xmltodict==0.14.2





================================================

FILE: client/agent.py

================================================

from __future__ import annotations

import asyncio

import argparse

import os

import uuid

from pathlib import Path

from typing import Dict, Any, List



from dotenv import load_dotenv

from mcp import ClientSession, StdioServerParameters

from mcp.client.stdio import stdio_client

from openai import AsyncOpenAI

from tenacity import retry, stop_after_attempt, wait_exponential, before_sleep_log

import logging

import colorlog

import json

import tiktoken



# ---------------------------------------------------------------------------

#Â  Â Logging setup

# ---------------------------------------------------------------------------

LOG_FORMAT = '%(log_color)s%(levelname)-8s%(reset)s %(message)s'

colorlog.basicConfig(level=logging.INFO, format=LOG_FORMAT)

logger = logging.getLogger(__name__)



# ---------------------------------------------------------------------------

#Â  Â Constants & templates (same as original)

# ---------------------------------------------------------------------------

META_SYSTEM_PROMPT = (

Â  Â  "You are the METAâ€‘PLANNER in a hierarchical AI system. A user will ask a\n"

Â  Â  "highâ€‘level question. **First**: break the problem into a *minimal sequence*\n"

Â  Â  "of executable tasks. Reply ONLY in JSON with the schema:\n"

Â  Â  "{ \"plan\": [ {\"id\": INT, \"description\": STRING} â€¦ ] }\n\n"

Â  Â  "After each task is executed by the EXECUTOR you will receive its result.\n"

Â  Â  "Please carefully consider the descriptions of the time of web pages and events in the task, and take these factors into account when planning and giving the final answer.\n"

Â  Â  "If the final answer is complete, output it with the template:\n"

Â  Â  "FINAL ANSWER: <answer>\n\n" \

Â  Â  " YOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings. If you are asked for a number, don't use comma to write your number neither use units such as $ or percent sign unless specified otherwise. If you are asked for a string, don't use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise. If you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.\n"

Â  Â  "Please ensure that the final answer strictly follows the question requirements, without any additional analysis.\n"

Â  Â  "If the final ansert is not complete, emit a *new* JSON plan for the remaining work. Keep cycles as\n"

Â  Â  "few as possible. Never call tools yourself â€” that's the EXECUTOR's job."\

Â  Â  "âš ï¸Â  Reply with *pure JSON only*."

)



EXEC_SYSTEM_PROMPT = (

Â  Â  "You are the EXECUTOR sub-agent. You receive one task description at a time\n"

Â  Â  "from the meta-planner. Your job is to complete the task, using available\n"

Â  Â  "tools via function calling if needed. Always think step by step but reply\n"

Â  Â  "with the minimal content needed for the meta-planner. If you must call a\n"

Â  Â  "tool, produce the appropriate function call instead of natural language.\n"

Â  Â  "When done, output a concise result. Do NOT output FINAL ANSWER."

)



MAX_CTX = 175000

EXE_MODEL = "o3"



# ---------------------------------------------------------------------------

#Â  Â OpenAI backend

# ---------------------------------------------------------------------------

class ChatBackend:

Â  Â  async def chat(self, *_, **__) -> Dict[str, Any]:

Â  Â  Â  Â  raise NotImplementedError



class OpenAIBackend(ChatBackend):

Â  Â  def __init__(self, model: str):

Â  Â  Â  Â  self.model = model

Â  Â  Â  Â  self.client = AsyncOpenAI(

Â  Â  Â  Â  Â  Â  api_key=os.getenv("OPENAI_API_KEY"),

Â  Â  Â  Â  Â  Â  base_url=os.getenv("OPENAI_BASE_URL"),

Â  Â  Â  Â  )



Â  Â  @retry(

Â  Â  Â  Â  stop=stop_after_attempt(3),

Â  Â  Â  Â  wait=wait_exponential(multiplier=1, min=2, max=10),

Â  Â  Â  Â  reraise=True,

Â  Â  Â  Â  before_sleep=before_sleep_log(logger, logging.WARNING),

Â  Â  )

Â  Â  async def chat(

Â  Â  Â  Â  self,

Â  Â  Â  Â  messages: List[Dict[str, Any]],

Â  Â  Â  Â  tools: List[Dict[str, Any]] | None = None,

Â  Â  Â  Â  tool_choice: str | None = "auto",

Â  Â  Â  Â  max_tokens: int = 15000,

Â  Â  ) -> Dict[str, Any]:

Â  Â  Â  Â  payload: Dict[str, Any] = {

Â  Â  Â  Â  Â  Â  "model": self.model,

Â  Â  Â  Â  Â  Â  "messages": messages,

Â  Â  Â  Â  Â  Â  "max_tokens": max_tokens,

Â  Â  Â  Â  }

Â  Â  Â  Â  if tools:

Â  Â  Â  Â  Â  Â  payload["tools"] = tools

Â  Â  Â  Â  Â  Â  payload["tool_choice"] = tool_choice

Â  Â  Â  Â  resp = await self.client.chat.completions.create(**payload)Â  # type: ignore[arg-type]

Â  Â  Â  Â  msg = resp.choices[0].message

Â  Â  Â  Â  raw_calls = getattr(msg, "tool_calls", None)

Â  Â  Â  Â  tool_calls = None

Â  Â  Â  Â  if raw_calls:

Â  Â  Â  Â  Â  Â  tool_calls = [

Â  Â  Â  Â  Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "id": tc.id,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "type": tc.type,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "function": {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "name": tc.function.name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "arguments": tc.function.arguments,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  for tc in raw_calls

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  return {"content": msg.content, "tool_calls": tool_calls}



# ---------------------------------------------------------------------------

#Â  Â Hierarchical client (trimmed: only essentials kept)

# ---------------------------------------------------------------------------

MAX_TURNS_MEMORY = 50



def _strip_fences(text: str) -> str:

Â  Â  import re

Â  Â  text = text.strip()

Â  Â  if text.startswith("```"):

Â  Â  Â  Â  text = re.sub(r"^```[^\n]*\n", "", text)

Â  Â  Â  Â  text = re.sub(r"\n?```$", "", text)

Â  Â  Â  Â  return text.strip()

Â  Â  m = re.search(r"{[\\s\\S]*}", text)

Â  Â  return m.group(0) if m else text



def _count_tokens(msg: Dict[str, str], enc) -> int:

Â  Â  role_tokens = 4

Â  Â  content = msg.get("content") or ""

Â  Â  return role_tokens + len(enc.encode(content))



def _get_tokenizer(model: str):

Â  Â  """Return a tokenizer; fall back to cl100k_base if model is unknown."""

Â  Â  try:

Â  Â  Â  Â  return tiktoken.encoding_for_model(model)

Â  Â  except KeyError:

Â  Â  Â  Â  return tiktoken.get_encoding("cl100k_base")

Â  Â Â 

def trim_messages(messages: List[Dict[str, str]], max_tokens: int, model="gpt-3.5-turbo"):



Â  Â  enc = _get_tokenizer(model)

Â  Â  total = sum(_count_tokens(m, enc) for m in messages) + 2

Â  Â  if total <= max_tokens:

Â  Â  Â  Â  return messages

Â  Â  system_msg = messages[0]

Â  Â  kept: List[Dict[str, str]] = [system_msg]

Â  Â  total = _count_tokens(system_msg, enc) + 2

Â  Â  for msg in reversed(messages[1:]):

Â  Â  Â  Â  t = _count_tokens(msg, enc)

Â  Â  Â  Â  if total + t > max_tokens:

Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  kept.insert(1, msg)

Â  Â  Â  Â  total += t

Â  Â  return kept



class HierarchicalClient:

Â  Â  MAX_CYCLES = 3



Â  Â  def __init__(self, meta_model: str, exec_model: str):

Â  Â  Â  Â  self.meta_llm = OpenAIBackend(meta_model)

Â  Â  Â  Â  self.exec_llm = OpenAIBackend(exec_model)

Â  Â  Â  Â  self.sessions: Dict[str, ClientSession] = {}

Â  Â  Â  Â  self.shared_history: List[Dict[str, str]] = []



Â  Â  # ---------- Tool management ----------

Â  Â  async def connect_to_servers(self, scripts: List[str]):

Â  Â  Â  Â  from contextlib import AsyncExitStack

Â  Â  Â  Â  self.exit_stack = AsyncExitStack()

Â  Â  Â  Â  for script in scripts:

Â  Â  Â  Â  Â  Â  path = Path(script)

Â  Â  Â  Â  Â  Â  cmd = "python" if path.suffix == ".py" else "node"

Â  Â  Â  Â  Â  Â  params = StdioServerParameters(command=cmd, args=[str(path)])

Â  Â  Â  Â  Â  Â  stdio, write = await self.exit_stack.enter_async_context(stdio_client(params))

Â  Â  Â  Â  Â  Â  session = await self.exit_stack.enter_async_context(ClientSession(stdio, write))

Â  Â  Â  Â  Â  Â  await session.initialize()

Â  Â  Â  Â  Â  Â  for tool in (await session.list_tools()).tools:

Â  Â  Â  Â  Â  Â  Â  Â  if tool.name in self.sessions:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise RuntimeError(f"Duplicate tool name '{tool.name}'.")

Â  Â  Â  Â  Â  Â  Â  Â  self.sessions[tool.name] = session

Â  Â  Â  Â  print("Connected tools:", list(self.sessions.keys()))



Â  Â  async def _tools_schema(self) -> List[Dict[str, Any]]:

Â  Â  Â  Â  result, cached = [], {}

Â  Â  Â  Â  for session in self.sessions.values():

Â  Â  Â  Â  Â  Â  tools_resp = cached.get(id(session)) or await session.list_tools()

Â  Â  Â  Â  Â  Â  cached[id(session)] = tools_resp

Â  Â  Â  Â  Â  Â  for tool in tools_resp.tools:

Â  Â  Â  Â  Â  Â  Â  Â  result.append(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "type": "function",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "function": {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "name": tool.name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "description": tool.description,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "parameters": tool.inputSchema,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  return result



Â  Â  # ---------- Main processing ----------

Â  Â  async def process_query(self, query: str, file: str, task_id: str = "interactive") -> str:

Â  Â  Â  Â  tools_schema = await self._tools_schema()

Â  Â  Â  Â  self.shared_history = []

Â  Â  Â  Â  self.shared_history.append({"role": "user", "content": f"{query}\ntask_id: {task_id}\nfile_path: {file}\n"})

Â  Â  Â  Â  planner_msgs = [{"role": "system", "content": META_SYSTEM_PROMPT}] + self.shared_history



Â  Â  Â  Â  for cycle in range(self.MAX_CYCLES):

Â  Â  Â  Â  Â  Â  meta_reply = await self.meta_llm.chat(planner_msgs)

Â  Â  Â  Â  Â  Â  meta_content = meta_reply["content"] or ""

Â  Â  Â  Â  Â  Â  self.shared_history.append({"role": "assistant", "content": meta_content})



Â  Â  Â  Â  Â  Â  if meta_content.startswith("FINAL ANSWER:"):

Â  Â  Â  Â  Â  Â  Â  Â  return meta_content[len("FINAL ANSWER:"):].strip()



Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  tasks = json.loads(_strip_fences(meta_content))["plan"]

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  return f"[planner error] {e}: {meta_content}"



Â  Â  Â  Â  Â  Â  for task in tasks:

Â  Â  Â  Â  Â  Â  Â  Â  task_desc = f"Task {task['id']}: {task['description']}"

Â  Â  Â  Â  Â  Â  Â  Â  exec_msgs = (

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  [{"role": "system", "content": EXEC_SYSTEM_PROMPT}] +

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.shared_history +

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  [{"role": "user", "content": task_desc}]

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  while True:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  exec_msgs = trim_messages(exec_msgs, MAX_CTX, model=EXE_MODEL)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  exec_reply = await self.exec_llm.chat(exec_msgs, tools_schema)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if exec_reply["content"]:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result_text = str(exec_reply["content"])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.shared_history.append({"role": "assistant", "content": f"Task {task['id']} result: {result_text}"})

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for call in exec_reply.get("tool_calls") or []:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  t_name = call["function"]["name"]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  t_args = json.loads(call["function"].get("arguments") or "{}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  session = self.sessions[t_name]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result_msg = await session.call_tool(t_name, t_args)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result_text = str(result_msg.content)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  exec_msgs.extend([

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"role": "assistant", "content": None, "tool_calls": [call]},

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"role": "tool", "tool_call_id": call["id"], "name": t_name, "content": result_text},

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ])

Â  Â  Â  Â  Â  Â  planner_msgs = [{"role": "system", "content": META_SYSTEM_PROMPT}] + self.shared_history

Â  Â  Â  Â  return meta_content.strip()



Â  Â  async def cleanup(self):

Â  Â  Â  Â  if hasattr(self, "exit_stack"):

Â  Â  Â  Â  Â  Â  await self.exit_stack.aclose()



# ---------------------------------------------------------------------------

#Â  Â Commandâ€‘line & main routine

# ---------------------------------------------------------------------------



def parse_args():

Â  Â  parser = argparse.ArgumentParser(description="AgentFly â€“ interactive version")

Â  Â  parser.add_argument("-q", "--question", type=str, help="Your question")

Â  Â  parser.add_argument("-f", "--file", type=str, default="", help="Optional file path")

Â  Â  parser.add_argument("-m", "--meta_model", type=str, default="gpt-4.1", help="Metaâ€‘planner model")

Â  Â  parser.add_argument("-e", "--exec_model", type=str, default="o3-2025-04-16", help="Executor model")

Â  Â  parser.add_argument("-s", "--servers", type=str, nargs="*", default=[

Â  Â  Â  Â  "../server/code_agent.py",

Â  Â  Â  Â  "../server/craw_page.py",

Â  Â  Â  Â  "../server/documents_tool.py",

Â  Â  Â  Â  "../server/excel_tool.py",

Â  Â  Â  Â  "../server/image_tool.py",

Â  Â  Â  Â  "../server/math_tool.py",

Â  Â  Â  Â  "../server/search_tool.py",

Â  Â  Â  Â  "../server/video_tool.py",

Â  Â  ], help="Paths of tool server scripts")

Â  Â  return parser.parse_args()



async def run_single_query(client: HierarchicalClient, question: str, file_path: str):

Â  Â  answer = await client.process_query(question, file_path, str(uuid.uuid4()))

Â  Â  print("\nFINAL ANSWER:", answer)



async def main_async(args):

Â  Â  load_dotenv()

Â  Â  client = HierarchicalClient(args.meta_model, args.exec_model)

Â  Â  await client.connect_to_servers(args.servers)



Â  Â  try:

Â  Â  Â  Â  if args.question:

Â  Â  Â  Â  Â  Â  await run_single_query(client, args.question, args.file)

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  print("Enter 'exit' to quit.")

Â  Â  Â  Â  Â  Â  while True:

Â  Â  Â  Â  Â  Â  Â  Â  q = input("\nQuestion: ").strip()

Â  Â  Â  Â  Â  Â  Â  Â  if q.lower() in {"exit", "quit", "q"}:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  Â  Â  f = input("File path (optional): ").strip()

Â  Â  Â  Â  Â  Â  Â  Â  await run_single_query(client, q, f)

Â  Â  finally:

Â  Â  Â  Â  await client.cleanup()



if __name__ == "__main__":

Â  Â  arg_ns = parse_args()

Â  Â  asyncio.run(main_async(arg_ns))







================================================

FILE: searxng-docker/README.md

================================================

# searxng-docker



Create a new SearXNG instance in five minutes using Docker



## What is included?



| Name | Description | Docker image | Dockerfile |

| -- | -- | -- | -- |

| [Caddy](https://github.com/caddyserver/caddy) | Reverse proxy (create a LetsEncrypt certificate automatically) | [docker.io/library/caddy:2-alpine](https://hub.docker.com/_/caddy)Â  Â  Â  Â  Â  Â | [Dockerfile](https://github.com/caddyserver/caddy-docker/blob/master/Dockerfile.tmpl) |

| [SearXNG](https://github.com/searxng/searxng) | SearXNG by itselfÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  | [docker.io/searxng/searxng:latest](https://hub.docker.com/r/searxng/searxng) | [Dockerfile](https://github.com/searxng/searxng/blob/master/Dockerfile)Â  Â  Â  Â  Â  Â  Â  Â |

| [Valkey](https://github.com/valkey-io/valkey) | In-memory databaseÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â | [docker.io/valkey/valkey:8-alpine](https://hub.docker.com/r/valkey/valkey)Â  Â  Â  Â  | [Dockerfile](https://github.com/valkey-io/valkey-container/blob/mainline/Dockerfile.template)Â  Â  Â  Â  Â  Â  Â |



## How to use it

There are two ways to host SearXNG. The first one doesn't require any prior knowledge about self-hosting and thus is recommended for beginners. It includes caddy as a reverse proxy and automatically deals with the TLS certificates for you. The second one is recommended for more advanced users that already have their own reverse proxy (e.g. Nginx, HAProxy, ...) and probably some other services running on their machine. The first few steps are the same for both installation methods however.



1. [Install docker](https://docs.docker.com/install/)

2. Get searxng-docker

Â  ```sh

Â  cd /usr/local

Â  git clone https://github.com/searxng/searxng-docker.git

Â  cd searxng-docker

Â  ```

3. Edit the [.env](https://github.com/searxng/searxng-docker/blob/master/.env) file to set the hostname and an email

4. Generate the secret key `sed -i "s|ultrasecretkey|$(openssl rand -hex 32)|g" searxng/settings.yml`Â Â 

Â  Â On a Mac: `sed -i '' "s|ultrasecretkey|$(openssl rand -hex 32)|g" searxng/settings.yml`

5. Edit [searxng/settings.yml](https://github.com/searxng/searxng-docker/blob/master/searxng/settings.yml) according to your needs



> [!NOTE]

> On the first run, you must remove `cap_drop: - ALL` from the `docker-compose.yaml` file for the `searxng` service to successfully create `/etc/searxng/uwsgi.ini`. This is necessary because the `cap_drop: - ALL` directive removes all capabilities, including those required for the creation of the `uwsgi.ini` file. After the first run, you should re-add `cap_drop: - ALL` to the `docker-compose.yaml` file for security reasons.



> [!NOTE]

> Windows users can use the following powershell script to generate the secret key:

> ```powershell

> $randomBytes = New-Object byte[] 32

> (New-Object Security.Cryptography.RNGCryptoServiceProvider).GetBytes($randomBytes)

> $secretKey = -join ($randomBytes | ForEach-Object { "{0:x2}" -f $_ })

> (Get-Content searxng/settings.yml) -replace 'ultrasecretkey', $secretKey | Set-Content searxng/settings.yml

> ```



### Method 1: With Caddy included (recommended for beginners)

6. Run SearXNG in the background: `docker compose up -d`



### Method 2: Bring your own reverse proxy (experienced users)

6. Remove the caddy related parts in `docker-compose.yaml` such as the caddy service and its volumes.

7. Point your reverse proxy to the port set for the `searxng` service in `docker-compose.yml` (8080 by default).

8. Generate and configure the required TLS certificates with the reverse proxy of your choice.

9. Run SearXNG in the background: `docker compose up -d`



> [!NOTE]

> You can change the port `searxng` listens on inside the docker container (e.g. if you want to operate in `host` network mode) with the `BIND_ADDRESS` environment variable (defaults to `0.0.0.0:8080`). The environment variable can be set directly inside `docker-compose.yaml`.



## Troubleshooting - How to access the logs



To access the logs from all the containers use: `docker compose logs -f`.



To access the logs of one specific container:



- Caddy: `docker compose logs -f caddy`

- SearXNG: `docker compose logs -f searxng`

- Valkey: `docker compose logs -f redis`



### Start SearXNG with systemd



You can skip this step if you don't use systemd.

1. Copy the service template file:

Â  Â ```sh

Â  Â cp searxng-docker.service.template searxng-docker.service

Â  Â ```

Â Â 

2. Edit the content of ```WorkingDirectory``` in the ```searxng-docker.service``` file (only if the installation path is different from ```/usr/local/searxng-docker```)

Â  Â 

3. Enable the service:

Â  Â ```sh

Â  Â systemctl enable $(pwd)/searxng-docker.service

Â  Â ```



4. Start the service:

Â  Â ```sh

Â  Â systemctl start searxng-docker.service

Â  Â ```



**Note:** Ensure the service file path matches your installation directory before enabling it.



## Note on the image proxy feature



The SearXNG image proxy is activated by default.



The default [Content-Security-Policy](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy) allows the browser to access to ```${SEARXNG_HOSTNAME}``` and ```https://*.tile.openstreetmap.org;```.



If some users want to disable the image proxy, you have to modify [./Caddyfile](https://github.com/searxng/searxng-docker/blob/master/Caddyfile). Replace the ```img-src 'self' data: https://*.tile.openstreetmap.org;``` by ```img-src * data:;```.



## Multi Architecture Docker images



Supported architecture:



- amd64

- arm64

- arm/v7



## How to update ?



To update the SearXNG stack:



```sh

git pull

docker compose pull

docker compose up -d

```



Or the old way (with the old docker-compose version):



```sh

git pull

docker-compose pull

docker-compose up -d

```







================================================

FILE: searxng-docker/Caddyfile

================================================

{

admin off



log {

output stderr

format filter {

# Preserves first 8 bits from IPv4 and 32 bits from IPv6

request>remote_ip ip_mask 8 32

request>client_ip ip_mask 8 32



# Remove identificable information

request>remote_port delete

request>headers delete

request>uri query {

delete url

delete h

delete q

}

}

}

}



{$SEARXNG_HOSTNAME}



tls {$SEARXNG_TLS}



encode zstd gzip



@api {

path /config

path /healthz

path /stats/errors

path /stats/checker

}



@search {

path /search

}



@imageproxy {

path /image_proxy

}



@static {

path /static/*

}



header {

# CSP (https://content-security-policy.com)

Content-Security-Policy "upgrade-insecure-requests; default-src 'none'; script-src 'self'; style-src 'self' 'unsafe-inline'; form-action 'self' https://github.com/searxng/searxng/issues/new; font-src 'self'; frame-ancestors 'self'; base-uri 'self'; connect-src 'self' https://overpass-api.de; img-src * data:; frame-src https://www.youtube-nocookie.com https://player.vimeo.com https://www.dailymotion.com https://www.deezer.com https://www.mixcloud.com https://w.soundcloud.com https://embed.spotify.com;"



# Disable some browser features

Permissions-Policy "accelerometer=(),camera=(),geolocation=(),gyroscope=(),magnetometer=(),microphone=(),payment=(),usb=()"



# Set referrer policy

Referrer-Policy "no-referrer"



# Force clients to use HTTPS

Strict-Transport-Security "max-age=31536000"



# Prevent MIME type sniffing from the declared Content-Type

X-Content-Type-Options "nosniff"



# X-Robots-Tag (comment to allow site indexing)

X-Robots-Tag "noindex, noarchive, nofollow"



# Remove "Server" header

-Server

}



header @api {

Access-Control-Allow-Methods "GET, OPTIONS"

Access-Control-Allow-Origin "*"

}



route {

# Cache policy

header Cache-Control "max-age=0, no-store"

header @search Cache-Control "max-age=5, private"

header @imageproxy Cache-Control "max-age=604800, public"

header @static Cache-Control "max-age=31536000, public, immutable"

}



# SearXNG (uWSGI)

reverse_proxy localhost:8080 {

header_up X-Forwarded-Port {http.request.port}

header_up X-Real-IP {http.request.remote.host}



# https://github.com/searx/searx-docker/issues/24

header_up Connection "close"

}







================================================

FILE: searxng-docker/docker-compose.yaml

================================================

version: "3.7"



services:

Â  caddy:

Â  Â  container_name: caddy

Â  Â  image: docker.io/library/caddy:2-alpine

Â  Â  network_mode: host

Â  Â  restart: unless-stopped

Â  Â  volumes:

Â  Â  Â  - ./Caddyfile:/etc/caddy/Caddyfile:ro

Â  Â  Â  - caddy-data:/data:rw

Â  Â  Â  - caddy-config:/config:rw

Â  Â  environment:

Â  Â  Â  - SEARXNG_HOSTNAME=${SEARXNG_HOSTNAME:-http://localhost}

Â  Â  Â  - SEARXNG_TLS=${LETSENCRYPT_EMAIL:-internal}

Â  Â  # cap_drop:

Â  Â  #Â  Â - ALL

Â  Â  cap_add:

Â  Â  Â  - NET_BIND_SERVICE

Â  Â  logging:

Â  Â  Â  driver: "json-file"

Â  Â  Â  options:

Â  Â  Â  Â  max-size: "1m"

Â  Â  Â  Â  max-file: "1"



Â  redis:

Â  Â  container_name: redis

Â  Â  image: docker.io/valkey/valkey:8-alpine

Â  Â  command: valkey-server --save 30 1 --loglevel warning

Â  Â  restart: unless-stopped

Â  Â  networks:

Â  Â  Â  - searxng

Â  Â  volumes:

Â  Â  Â  - valkey-data2:/data

Â  Â  cap_drop:

Â  Â  Â  - ALL

Â  Â  cap_add:

Â  Â  Â  - SETGID

Â  Â  Â  - SETUID

Â  Â  Â  - DAC_OVERRIDE

Â  Â  logging:

Â  Â  Â  driver: "json-file"

Â  Â  Â  options:

Â  Â  Â  Â  max-size: "1m"

Â  Â  Â  Â  max-file: "1"



Â  searxng:

Â  Â  container_name: searxng

Â  Â  image: docker.io/searxng/searxng:latest

Â  Â  restart: unless-stopped

Â  Â  networks:

Â  Â  Â  - searxng

Â  Â  ports:

Â  Â  Â  - "8080:8080"

Â  Â  volumes:

Â  Â  Â  - ./searxng:/etc/searxng:rw

Â  Â  environment:

Â  Â  Â  - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/

Â  Â  Â  - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}

Â  Â  Â  - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}

Â  Â  cap_drop:

Â  Â  Â  - ALL

Â  Â  cap_add:

Â  Â  Â  - CHOWN

Â  Â  Â  - SETGID

Â  Â  Â  - SETUID

Â  Â  logging:

Â  Â  Â  driver: "json-file"

Â  Â  Â  options:

Â  Â  Â  Â  max-size: "1m"

Â  Â  Â  Â  max-file: "1"



networks:

Â  searxng:



volumes:

Â  caddy-data:

Â  caddy-config:

Â  valkey-data2:







================================================

FILE: searxng-docker/LICENSE

================================================

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  GNU AFFERO GENERAL PUBLIC LICENSE

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Version 3, 19 November 2007



Â Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>

Â Everyone is permitted to copy and distribute verbatim copies

Â of this license document, but changing it is not allowed.



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Preamble



Â  The GNU Affero General Public License is a free, copyleft license for

software and other kinds of works, specifically designed to ensure

cooperation with the community in the case of network server software.



Â  The licenses for most software and other practical works are designed

to take away your freedom to share and change the works.Â  By contrast,

our General Public Licenses are intended to guarantee your freedom to

share and change all versions of a program--to make sure it remains free

software for all its users.



Â  When we speak of free software, we are referring to freedom, not

price.Â  Our General Public Licenses are designed to make sure that you

have the freedom to distribute copies of free software (and charge for

them if you wish), that you receive source code or can get it if you

want it, that you can change the software or use pieces of it in new

free programs, and that you know you can do these things.



Â  Developers that use our General Public Licenses protect your rights

with two steps: (1) assert copyright on the software, and (2) offer

you this License which gives you legal permission to copy, distribute

and/or modify the software.



Â  A secondary benefit of defending all users' freedom is that

improvements made in alternate versions of the program, if they

receive widespread use, become available for other developers to

incorporate.Â  Many developers of free software are heartened and

encouraged by the resulting cooperation.Â  However, in the case of

software used on network servers, this result may fail to come about.

The GNU General Public License permits making a modified version and

letting the public access it on a server without ever releasing its

source code to the public.



Â  The GNU Affero General Public License is designed specifically to

ensure that, in such cases, the modified source code becomes available

to the community.Â  It requires the operator of a network server to

provide the source code of the modified version running there to the

users of that server.Â  Therefore, public use of a modified version, on

a publicly accessible server, gives the public access to the source

code of the modified version.



Â  An older license, called the Affero General Public License and

published by Affero, was designed to accomplish similar goals.Â  This is

a different license, not a version of the Affero GPL, but Affero has

released a new version of the Affero GPL which permits relicensing under

this license.



Â  The precise terms and conditions for copying, distribution and

modification follow.



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â TERMS AND CONDITIONS



Â  0. Definitions.



Â  "This License" refers to version 3 of the GNU Affero General Public License.



Â  "Copyright" also means copyright-like laws that apply to other kinds of

works, such as semiconductor masks.



Â  "The Program" refers to any copyrightable work licensed under this

License.Â  Each licensee is addressed as "you".Â  "Licensees" and

"recipients" may be individuals or organizations.



Â  To "modify" a work means to copy from or adapt all or part of the work

in a fashion requiring copyright permission, other than the making of an

exact copy.Â  The resulting work is called a "modified version" of the

earlier work or a work "based on" the earlier work.



Â  A "covered work" means either the unmodified Program or a work based

on the Program.



Â  To "propagate" a work means to do anything with it that, without

permission, would make you directly or secondarily liable for

infringement under applicable copyright law, except executing it on a

computer or modifying a private copy.Â  Propagation includes copying,

distribution (with or without modification), making available to the

public, and in some countries other activities as well.



Â  To "convey" a work means any kind of propagation that enables other

parties to make or receive copies.Â  Mere interaction with a user through

a computer network, with no transfer of a copy, is not conveying.



Â  An interactive user interface displays "Appropriate Legal Notices"

to the extent that it includes a convenient and prominently visible

feature that (1) displays an appropriate copyright notice, and (2)

tells the user that there is no warranty for the work (except to the

extent that warranties are provided), that licensees may convey the

work under this License, and how to view a copy of this License.Â  If

the interface presents a list of user commands or options, such as a

menu, a prominent item in the list meets this criterion.



Â  1. Source Code.



Â  The "source code" for a work means the preferred form of the work

for making modifications to it.Â  "Object code" means any non-source

form of a work.



Â  A "Standard Interface" means an interface that either is an official

standard defined by a recognized standards body, or, in the case of

interfaces specified for a particular programming language, one that

is widely used among developers working in that language.



Â  The "System Libraries" of an executable work include anything, other

than the work as a whole, that (a) is included in the normal form of

packaging a Major Component, but which is not part of that Major

Component, and (b) serves only to enable use of the work with that

Major Component, or to implement a Standard Interface for which an

implementation is available to the public in source code form.Â  A

"Major Component", in this context, means a major essential component

(kernel, window system, and so on) of the specific operating system

(if any) on which the executable work runs, or a compiler used to

produce the work, or an object code interpreter used to run it.



Â  The "Corresponding Source" for a work in object code form means all

the source code needed to generate, install, and (for an executable

work) run the object code and to modify the work, including scripts to

control those activities.Â  However, it does not include the work's

System Libraries, or general-purpose tools or generally available free

programs which are used unmodified in performing those activities but

which are not part of the work.Â  For example, Corresponding Source

includes interface definition files associated with source files for

the work, and the source code for shared libraries and dynamically

linked subprograms that the work is specifically designed to require,

such as by intimate data communication or control flow between those

subprograms and other parts of the work.



Â  The Corresponding Source need not include anything that users

can regenerate automatically from other parts of the Corresponding

Source.



Â  The Corresponding Source for a work in source code form is that

same work.



Â  2. Basic Permissions.



Â  All rights granted under this License are granted for the term of

copyright on the Program, and are irrevocable provided the stated

conditions are met.Â  This License explicitly affirms your unlimited

permission to run the unmodified Program.Â  The output from running a

covered work is covered by this License only if the output, given its

content, constitutes a covered work.Â  This License acknowledges your

rights of fair use or other equivalent, as provided by copyright law.



Â  You may make, run and propagate covered works that you do not

convey, without conditions so long as your license otherwise remains

in force.Â  You may convey covered works to others for the sole purpose

of having them make modifications exclusively for you, or provide you

with facilities for running those works, provided that you comply with

the terms of this License in conveying all material for which you do

not control copyright.Â  Those thus making or running the covered works

for you must do so exclusively on your behalf, under your direction

and control, on terms that prohibit them from making any copies of

your copyrighted material outside their relationship with you.



Â  Conveying under any other circumstances is permitted solely under

the conditions stated below.Â  Sublicensing is not allowed; section 10

makes it unnecessary.



Â  3. Protecting Users' Legal Rights From Anti-Circumvention Law.



Â  No covered work shall be deemed part of an effective technological

measure under any applicable law fulfilling obligations under article

11 of the WIPO copyright treaty adopted on 20 December 1996, or

similar laws prohibiting or restricting circumvention of such

measures.



Â  When you convey a covered work, you waive any legal power to forbid

circumvention of technological measures to the extent such circumvention

is effected by exercising rights under this License with respect to

the covered work, and you disclaim any intention to limit operation or

modification of the work as a means of enforcing, against the work's

users, your or third parties' legal rights to forbid circumvention of

technological measures.



Â  4. Conveying Verbatim Copies.



Â  You may convey verbatim copies of the Program's source code as you

receive it, in any medium, provided that you conspicuously and

appropriately publish on each copy an appropriate copyright notice;

keep intact all notices stating that this License and any

non-permissive terms added in accord with section 7 apply to the code;

keep intact all notices of the absence of any warranty; and give all

recipients a copy of this License along with the Program.



Â  You may charge any price or no price for each copy that you convey,

and you may offer support or warranty protection for a fee.



Â  5. Conveying Modified Source Versions.



Â  You may convey a work based on the Program, or the modifications to

produce it from the Program, in the form of source code under the

terms of section 4, provided that you also meet all of these conditions:



Â  Â  a) The work must carry prominent notices stating that you modified

Â  Â  it, and giving a relevant date.



Â  Â  b) The work must carry prominent notices stating that it is

Â  Â  released under this License and any conditions added under section

Â  Â  7.Â  This requirement modifies the requirement in section 4 to

Â  Â  "keep intact all notices".



Â  Â  c) You must license the entire work, as a whole, under this

Â  Â  License to anyone who comes into possession of a copy.Â  This

Â  Â  License will therefore apply, along with any applicable section 7

Â  Â  additional terms, to the whole of the work, and all its parts,

Â  Â  regardless of how they are packaged.Â  This License gives no

Â  Â  permission to license the work in any other way, but it does not

Â  Â  invalidate such permission if you have separately received it.



Â  Â  d) If the work has interactive user interfaces, each must display

Â  Â  Appropriate Legal Notices; however, if the Program has interactive

Â  Â  interfaces that do not display Appropriate Legal Notices, your

Â  Â  work need not make them do so.



Â  A compilation of a covered work with other separate and independent

works, which are not by their nature extensions of the covered work,

and which are not combined with it such as to form a larger program,

in or on a volume of a storage or distribution medium, is called an

"aggregate" if the compilation and its resulting copyright are not

used to limit the access or legal rights of the compilation's users

beyond what the individual works permit.Â  Inclusion of a covered work

in an aggregate does not cause this License to apply to the other

parts of the aggregate.



Â  6. Conveying Non-Source Forms.



Â  You may convey a covered work in object code form under the terms

of sections 4 and 5, provided that you also convey the

machine-readable Corresponding Source under the terms of this License,

in one of these ways:



Â  Â  a) Convey the object code in, or embodied in, a physical product

Â  Â  (including a physical distribution medium), accompanied by the

Â  Â  Corresponding Source fixed on a durable physical medium

Â  Â  customarily used for software interchange.



Â  Â  b) Convey the object code in, or embodied in, a physical product

Â  Â  (including a physical distribution medium), accompanied by a

Â  Â  written offer, valid for at least three years and valid for as

Â  Â  long as you offer spare parts or customer support for that product

Â  Â  model, to give anyone who possesses the object code either (1) a

Â  Â  copy of the Corresponding Source for all the software in the

Â  Â  product that is covered by this License, on a durable physical

Â  Â  medium customarily used for software interchange, for a price no

Â  Â  more than your reasonable cost of physically performing this

Â  Â  conveying of source, or (2) access to copy the

Â  Â  Corresponding Source from a network server at no charge.



Â  Â  c) Convey individual copies of the object code with a copy of the

Â  Â  written offer to provide the Corresponding Source.Â  This

Â  Â  alternative is allowed only occasionally and noncommercially, and

Â  Â  only if you received the object code with such an offer, in accord

Â  Â  with subsection 6b.



Â  Â  d) Convey the object code by offering access from a designated

Â  Â  place (gratis or for a charge), and offer equivalent access to the

Â  Â  Corresponding Source in the same way through the same place at no

Â  Â  further charge.Â  You need not require recipients to copy the

Â  Â  Corresponding Source along with the object code.Â  If the place to

Â  Â  copy the object code is a network server, the Corresponding Source

Â  Â  may be on a different server (operated by you or a third party)

Â  Â  that supports equivalent copying facilities, provided you maintain

Â  Â  clear directions next to the object code saying where to find the

Â  Â  Corresponding Source.Â  Regardless of what server hosts the

Â  Â  Corresponding Source, you remain obligated to ensure that it is

Â  Â  available for as long as needed to satisfy these requirements.



Â  Â  e) Convey the object code using peer-to-peer transmission, provided

Â  Â  you inform other peers where the object code and Corresponding

Â  Â  Source of the work are being offered to the general public at no

Â  Â  charge under subsection 6d.



Â  A separable portion of the object code, whose source code is excluded

from the Corresponding Source as a System Library, need not be

included in conveying the object code work.



Â  A "User Product" is either (1) a "consumer product", which means any

tangible personal property which is normally used for personal, family,

or household purposes, or (2) anything designed or sold for incorporation

into a dwelling.Â  In determining whether a product is a consumer product,

doubtful cases shall be resolved in favor of coverage.Â  For a particular

product received by a particular user, "normally used" refers to a

typical or common use of that class of product, regardless of the status

of the particular user or of the way in which the particular user

actually uses, or expects or is expected to use, the product.Â  A product

is a consumer product regardless of whether the product has substantial

commercial, industrial or non-consumer uses, unless such uses represent

the only significant mode of use of the product.



Â  "Installation Information" for a User Product means any methods,

procedures, authorization keys, or other information required to install

and execute modified versions of a covered work in that User Product from

a modified version of its Corresponding Source.Â  The information must

suffice to ensure that the continued functioning of the modified object

code is in no case prevented or interfered with solely because

modification has been made.



Â  If you convey an object code work under this section in, or with, or

specifically for use in, a User Product, and the conveying occurs as

part of a transaction in which the right of possession and use of the

User Product is transferred to the recipient in perpetuity or for a

fixed term (regardless of how the transaction is characterized), the

Corresponding Source conveyed under this section must be accompanied

by the Installation Information.Â  But this requirement does not apply

if neither you nor any third party retains the ability to install

modified object code on the User Product (for example, the work has

been installed in ROM).



Â  The requirement to provide Installation Information does not include a

requirement to continue to provide support service, warranty, or updates

for a work that has been modified or installed by the recipient, or for

the User Product in which it has been modified or installed.Â  Access to a

network may be denied when the modification itself materially and

adversely affects the operation of the network or violates the rules and

protocols for communication across the network.



Â  Corresponding Source conveyed, and Installation Information provided,

in accord with this section must be in a format that is publicly

documented (and with an implementation available to the public in

source code form), and must require no special password or key for

unpacking, reading or copying.



Â  7. Additional Terms.



Â  "Additional permissions" are terms that supplement the terms of this

License by making exceptions from one or more of its conditions.

Additional permissions that are applicable to the entire Program shall

be treated as though they were included in this License, to the extent

that they are valid under applicable law.Â  If additional permissions

apply only to part of the Program, that part may be used separately

under those permissions, but the entire Program remains governed by

this License without regard to the additional permissions.



Â  When you convey a copy of a covered work, you may at your option

remove any additional permissions from that copy, or from any part of

it.Â  (Additional permissions may be written to require their own

removal in certain cases when you modify the work.)Â  You may place

additional permissions on material, added by you to a covered work,

for which you have or can give appropriate copyright permission.



Â  Notwithstanding any other provision of this License, for material you

add to a covered work, you may (if authorized by the copyright holders of

that material) supplement the terms of this License with terms:



Â  Â  a) Disclaiming warranty or limiting liability differently from the

Â  Â  terms of sections 15 and 16 of this License; or



Â  Â  b) Requiring preservation of specified reasonable legal notices or

Â  Â  author attributions in that material or in the Appropriate Legal

Â  Â  Notices displayed by works containing it; or



Â  Â  c) Prohibiting misrepresentation of the origin of that material, or

Â  Â  requiring that modified versions of such material be marked in

Â  Â  reasonable ways as different from the original version; or



Â  Â  d) Limiting the use for publicity purposes of names of licensors or

Â  Â  authors of the material; or



Â  Â  e) Declining to grant rights under trademark law for use of some

Â  Â  trade names, trademarks, or service marks; or



Â  Â  f) Requiring indemnification of licensors and authors of that

Â  Â  material by anyone who conveys the material (or modified versions of

Â  Â  it) with contractual assumptions of liability to the recipient, for

Â  Â  any liability that these contractual assumptions directly impose on

Â  Â  those licensors and authors.



Â  All other non-permissive additional terms are considered "further

restrictions" within the meaning of section 10.Â  If the Program as you

received it, or any part of it, contains a notice stating that it is

governed by this License along with a term that is a further

restriction, you may remove that term.Â  If a license document contains

a further restriction but permits relicensing or conveying under this

License, you may add to a covered work material governed by the terms

of that license document, provided that the further restriction does

not survive such relicensing or conveying.



Â  If you add terms to a covered work in accord with this section, you

must place, in the relevant source files, a statement of the

additional terms that apply to those files, or a notice indicating

where to find the applicable terms.



Â  Additional terms, permissive or non-permissive, may be stated in the

form of a separately written license, or stated as exceptions;

the above requirements apply either way.



Â  8. Termination.



Â  You may not propagate or modify a covered work except as expressly

provided under this License.Â  Any attempt otherwise to propagate or

modify it is void, and will automatically terminate your rights under

this License (including any patent licenses granted under the third

paragraph of section 11).



Â  However, if you cease all violation of this License, then your

license from a particular copyright holder is reinstated (a)

provisionally, unless and until the copyright holder explicitly and

finally terminates your license, and (b) permanently, if the copyright

holder fails to notify you of the violation by some reasonable means

prior to 60 days after the cessation.



Â  Moreover, your license from a particular copyright holder is

reinstated permanently if the copyright holder notifies you of the

violation by some reasonable means, this is the first time you have

received notice of violation of this License (for any work) from that

copyright holder, and you cure the violation prior to 30 days after

your receipt of the notice.



Â  Termination of your rights under this section does not terminate the

licenses of parties who have received copies or rights from you under

this License.Â  If your rights have been terminated and not permanently

reinstated, you do not qualify to receive new licenses for the same

material under section 10.



Â  9. Acceptance Not Required for Having Copies.



Â  You are not required to accept this License in order to receive or

run a copy of the Program.Â  Ancillary propagation of a covered work

occurring solely as a consequence of using peer-to-peer transmission

to receive a copy likewise does not require acceptance.Â  However,

nothing other than this License grants you permission to propagate or

modify any covered work.Â  These actions infringe copyright if you do

not accept this License.Â  Therefore, by modifying or propagating a

covered work, you indicate your acceptance of this License to do so.



Â  10. Automatic Licensing of Downstream Recipients.



Â  Each time you convey a covered work, the recipient automatically

receives a license from the original licensors, to run, modify and

propagate that work, subject to this License.Â  You are not responsible

for enforcing compliance by third parties with this License.



Â  An "entity transaction" is a transaction transferring control of an

organization, or substantially all assets of one, or subdividing an

organization, or merging organizations.Â  If propagation of a covered

work results from an entity transaction, each party to that

transaction who receives a copy of the work also receives whatever

licenses to the work the party's predecessor in interest had or could

give under the previous paragraph, plus a right to possession of the

Corresponding Source of the work from the predecessor in interest, if

the predecessor has it or can get it with reasonable efforts.



Â  You may not impose any further restrictions on the exercise of the

rights granted or affirmed under this License.Â  For example, you may

not impose a license fee, royalty, or other charge for exercise of

rights granted under this License, and you may not initiate litigation

(including a cross-claim or counterclaim in a lawsuit) alleging that

any patent claim is infringed by making, using, selling, offering for

sale, or importing the Program or any portion of it.



Â  11. Patents.



Â  A "contributor" is a copyright holder who authorizes use under this

License of the Program or a work on which the Program is based.Â  The

work thus licensed is called the contributor's "contributor version".



Â  A contributor's "essential patent claims" are all patent claims

owned or controlled by the contributor, whether already acquired or

hereafter acquired, that would be infringed by some manner, permitted

by this License, of making, using, or selling its contributor version,

but do not include claims that would be infringed only as a

consequence of further modification of the contributor version.Â  For

purposes of this definition, "control" includes the right to grant

patent sublicenses in a manner consistent with the requirements of

this License.



Â  Each contributor grants you a non-exclusive, worldwide, royalty-free

patent license under the contributor's essential patent claims, to

make, use, sell, offer for sale, import and otherwise run, modify and

propagate the contents of its contributor version.



Â  In the following three paragraphs, a "patent license" is any express

agreement or commitment, however denominated, not to enforce a patent

(such as an express permission to practice a patent or covenant not to

sue for patent infringement).Â  To "grant" such a patent license to a

party means to make such an agreement or commitment not to enforce a

patent against the party.



Â  If you convey a covered work, knowingly relying on a patent license,

and the Corresponding Source of the work is not available for anyone

to copy, free of charge and under the terms of this License, through a

publicly available network server or other readily accessible means,

then you must either (1) cause the Corresponding Source to be so

available, or (2) arrange to deprive yourself of the benefit of the

patent license for this particular work, or (3) arrange, in a manner

consistent with the requirements of this License, to extend the patent

license to downstream recipients.Â  "Knowingly relying" means you have

actual knowledge that, but for the patent license, your conveying the

covered work in a country, or your recipient's use of the covered work

in a country, would infringe one or more identifiable patents in that

country that you have reason to believe are valid.



Â  If, pursuant to or in connection with a single transaction or

arrangement, you convey, or propagate by procuring conveyance of, a

covered work, and grant a patent license to some of the parties

receiving the covered work authorizing them to use, propagate, modify

or convey a specific copy of the covered work, then the patent license

you grant is automatically extended to all recipients of the covered

work and works based on it.



Â  A patent license is "discriminatory" if it does not include within

the scope of its coverage, prohibits the exercise of, or is

conditioned on the non-exercise of one or more of the rights that are

specifically granted under this License.Â  You may not convey a covered

work if you are a party to an arrangement with a third party that is

in the business of distributing software, under which you make payment

to the third party based on the extent of your activity of conveying

the work, and under which the third party grants, to any of the

parties who would receive the covered work from you, a discriminatory

patent license (a) in connection with copies of the covered work

conveyed by you (or copies made from those copies), or (b) primarily

for and in connection with specific products or compilations that

contain the covered work, unless you entered into that arrangement,

or that patent license was granted, prior to 28 March 2007.



Â  Nothing in this License shall be construed as excluding or limiting

any implied license or other defenses to infringement that may

otherwise be available to you under applicable patent law.



Â  12. No Surrender of Others' Freedom.



Â  If conditions are imposed on you (whether by court order, agreement or

otherwise) that contradict the conditions of this License, they do not

excuse you from the conditions of this License.Â  If you cannot convey a

covered work so as to satisfy simultaneously your obligations under this

License and any other pertinent obligations, then as a consequence you may

not convey it at all.Â  For example, if you agree to terms that obligate you

to collect a royalty for further conveying from those to whom you convey

the Program, the only way you could satisfy both those terms and this

License would be to refrain entirely from conveying the Program.



Â  13. Remote Network Interaction; Use with the GNU General Public License.



Â  Notwithstanding any other provision of this License, if you modify the

Program, your modified version must prominently offer all users

interacting with it remotely through a computer network (if your version

supports such interaction) an opportunity to receive the Corresponding

Source of your version by providing access to the Corresponding Source

from a network server at no charge, through some standard or customary

means of facilitating copying of software.Â  This Corresponding Source

shall include the Corresponding Source for any work covered by version 3

of the GNU General Public License that is incorporated pursuant to the

following paragraph.



Â  Notwithstanding any other provision of this License, you have

permission to link or combine any covered work with a work licensed

under version 3 of the GNU General Public License into a single

combined work, and to convey the resulting work.Â  The terms of this

License will continue to apply to the part which is the covered work,

but the work with which it is combined will remain governed by version

3 of the GNU General Public License.



Â  14. Revised Versions of this License.



Â  The Free Software Foundation may publish revised and/or new versions of

the GNU Affero General Public License from time to time.Â  Such new versions

will be similar in spirit to the present version, but may differ in detail to

address new problems or concerns.



Â  Each version is given a distinguishing version number.Â  If the

Program specifies that a certain numbered version of the GNU Affero General

Public License "or any later version" applies to it, you have the

option of following the terms and conditions either of that numbered

version or of any later version published by the Free Software

Foundation.Â  If the Program does not specify a version number of the

GNU Affero General Public License, you may choose any version ever published

by the Free Software Foundation.



Â  If the Program specifies that a proxy can decide which future

versions of the GNU Affero General Public License can be used, that proxy's

public statement of acceptance of a version permanently authorizes you

to choose that version for the Program.



Â  Later license versions may give you additional or different

permissions.Â  However, no additional obligations are imposed on any

author or copyright holder as a result of your choosing to follow a

later version.



Â  15. Disclaimer of Warranty.



Â  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY

APPLICABLE LAW.Â  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT

HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY

OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,

THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR

PURPOSE.Â  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM

IS WITH YOU.Â  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF

ALL NECESSARY SERVICING, REPAIR OR CORRECTION.



Â  16. Limitation of Liability.



Â  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING

WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS

THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY

GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE

USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF

DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD

PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),

EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF

SUCH DAMAGES.



Â  17. Interpretation of Sections 15 and 16.



Â  If the disclaimer of warranty and limitation of liability provided

above cannot be given local legal effect according to their terms,

reviewing courts shall apply local law that most closely approximates

an absolute waiver of all civil liability in connection with the

Program, unless a warranty or assumption of liability accompanies a

copy of the Program in return for a fee.



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â END OF TERMS AND CONDITIONS



Â  Â  Â  Â  Â  Â  How to Apply These Terms to Your New Programs



Â  If you develop a new program, and you want it to be of the greatest

possible use to the public, the best way to achieve this is to make it

free software which everyone can redistribute and change under these terms.



Â  To do so, attach the following notices to the program.Â  It is safest

to attach them to the start of each source file to most effectively

state the exclusion of warranty; and each file should have at least

the "copyright" line and a pointer to where the full notice is found.



Â  Â  <one line to give the program's name and a brief idea of what it does.>

Â  Â  Copyright (C) <year>Â  <name of author>



Â  Â  This program is free software: you can redistribute it and/or modify

Â  Â  it under the terms of the GNU Affero General Public License as published

Â  Â  by the Free Software Foundation, either version 3 of the License, or

Â  Â  (at your option) any later version.



Â  Â  This program is distributed in the hope that it will be useful,

Â  Â  but WITHOUT ANY WARRANTY; without even the implied warranty of

Â  Â  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.Â  See the

Â  Â  GNU Affero General Public License for more details.



Â  Â  You should have received a copy of the GNU Affero General Public License

Â  Â  along with this program.Â  If not, see <https://www.gnu.org/licenses/>.



Also add information on how to contact you by electronic and paper mail.



Â  If your software can interact with users remotely through a computer

network, you should also make sure that it provides a way for users to

get its source.Â  For example, if your program is a web application, its

interface could display a "Source" link that leads users to an archive

of the code.Â  There are many ways you could offer source, and different

solutions will be better for different programs; see section 13 for the

specific requirements.



Â  You should also get your employer (if you work as a programmer) or school,

if any, to sign a "copyright disclaimer" for the program, if necessary.

For more information on this, and how to apply and follow the GNU AGPL, see

<https://www.gnu.org/licenses/>.







================================================

FILE: searxng-docker/searxng-docker.service.template

================================================

[Unit]

Description=SearXNG service

Requires=docker.service

After=docker.service



[Service]

Restart=on-failure



Environment=SEARXNG_DOCKERCOMPOSEFILE=docker-compose.yaml



WorkingDirectory=/usr/local/searxng-docker

ExecStart=/usr/local/bin/docker compose -f ${SEARXNG_DOCKERCOMPOSEFILE} up --remove-orphans

ExecStop=/usr/local/bin/docker compose -f ${SEARXNG_DOCKERCOMPOSEFILE} down



[Install]

WantedBy=multi-user.target







================================================

FILE: searxng-docker/searxng/limiter.toml

================================================

# This configuration file updates the default configuration file

# See https://github.com/searxng/searxng/blob/master/searx/limiter.toml



[botdetection.ip_limit]

# activate link_token method in the ip_limit method

link_token = true







================================================

FILE: searxng-docker/searxng/settings.yml

================================================

# see https://docs.searxng.org/admin/settings/settings.html#settings-use-default-settings

use_default_settings: true



ui:

Â  static_use_hash: true



engines:

Â  # å¯ç”¨é»˜è®¤ç¦ç”¨çš„å¼•æ“Ž

Â  - name: google

Â  Â  disabled: true

Â  - name: duckduckgo

Â  Â  disabled: true

Â  - name: brave

Â  Â  disabled: true

Â  # - name: startpage

Â  #Â  Â disabled: true

Â  - name: bing

Â  Â  disabled: true

Â  - name: wikipedia

Â  Â  disabled: true

Â  - name: wikidata

Â  Â  disabled: true

Â  # - name: qwant

Â  #Â  Â disabled: true

Â  # - name: startpage

Â  #Â  Â disabled: true

Â  # - name: presearch

Â  #Â  Â disabled: false





Â  # - name: 360search

Â  #Â  Â engine: 360search

Â  #Â  Â shortcut: 360so

Â  #Â  Â disabled: trues

Â Â 

Â  # - name: baidu

Â  #Â  Â engine: baidu

Â  #Â  Â shortcut: baidu

Â  #Â  Â disabled: false

Â Â 

Â  # - name: iqiyi

Â  #Â  Â engine: iqiyi

Â  #Â  Â shortcut: iq

Â  #Â  Â disabled: false



Â  # - name: acfun

Â  #Â  Â engine: acfun

Â  #Â  Â shortcut: acf

Â  #Â  Â disabled: false







Â  Â Â 



server:

Â  # base_url is defined in the SEARXNG_BASE_URL environment variable, see .env and docker-compose.yml

Â  secret_key: "888"Â  # change this!

Â  limiter: trueÂ  # can be disabled for a private instance

Â  image_proxy: true

ui:

Â  static_use_hash: true

valkey:

Â  url: valkey://valkey:6379/0Â 

Â Â 

search:

Â  safe_search: 0

Â  autocomplete: ""

Â  default_lang: ""

Â  formats:

Â  Â  - html

Â  Â  - json

Â  Â  - csv

Â  Â  - rss

ratelimit:

Â  Â  enabled: true

Â  Â  # è°ƒæ•´æ¯ç§’å…è®¸çš„è¯·æ±‚æ•°

Â  Â  per_second: 5Â Â 

Â  Â  # è°ƒæ•´æ¯åˆ†é’Ÿå…è®¸çš„è¯·æ±‚æ•°

Â  Â  per_minute: 60



timeouts:

Â  total: 10.0

Â  connect: 6.0



outgoing:

Â  request_timeout: 20.0

Â Â 

network:

Â  request_args:

Â  Â  max_redirects: 3

Â  Â  allow_redirects: true





================================================

FILE: searxng-docker/searxng/uwsgi.ini

================================================

[uwsgi]

# Listening address

# default value: [::]:8080 (see Dockerfile)

http-socket = $(BIND_ADDRESS)



# Who will run the code

uid = searxng

gid = searxng



# Number of workers (usually CPU count)

# default value: %k (= number of CPU core, see Dockerfile)

workers = 4



# Number of threads per worker

# default value: 4 (see Dockerfile)

threads = 4



# The right granted on the created socket

chmod-socket = 666



# Plugin to use and interpreter config

single-interpreter = true

master = true

lazy-apps = true

enable-threads = 4



# Module to import

module = searx.webapp



# Virtualenv and python path

pythonpath = /usr/local/searxng/

chdir = /usr/local/searxng/searx/



# automatically set processes name to something meaningful

auto-procname = true



# Disable request logging for privacy

disable-logging = true

log-5xx = true



# Set the max size of a request (request-body excluded)

buffer-size = 8192



# No keep alive

# See https://github.com/searx/searx-docker/issues/24

add-header = Connection: close



# Follow SIGTERM convention

# See https://github.com/searxng/searxng/issues/3427

die-on-term



# uwsgi serves the static files

static-map = /static=/usr/local/searxng/searx/static

static-gzip-all = True

offload-threads = 4







================================================

FILE: server/code_agent.py

================================================

"""

code_exec_server.py â€“ Enhanced FastMCP server for code execution with unified workspace

-----------------------------------------------------------------------------------



ðŸŽ¯ WORKFLOW:



This server provides tools for file management and command execution within a sandboxed

workspace. The standard workflow is:

1. Use `write_workspace_file` to create or modify scripts (e.g., Python, shell).

2. Use `execute_terminal_command` to run those scripts (e.g., `python my_script.py`) or any other shell command.

3. Use `read_workspace_file` or `list_workspace_files` to inspect the results and file system.



ðŸ“ DIRECTORY STRUCTURE:

Â  Â  All files are stored and executed within a dedicated workspace directory for each task:

Â  Â  `agent_cache/task_<id>/workspace/`

Â  Â  This directory contains all user-created scripts, data files, and generated outputs

Â  Â  in a flat structure. Subdirectories are not supported by default.



ðŸ”§ EXECUTION ENVIRONMENT:

Â  Â  - All terminal commands run inside the workspace directory.

Â  Â  - The environment is persistent for a given task, meaning files and changes

Â  Â  Â  made in one step are available in subsequent steps.

Â  Â  - Python code execution is sandboxed, with restrictions on imports for security.



ðŸ”‘ IMPORT WHITELIST (For Python scripts):

Â  Â  When `execute_terminal_command` runs a Python script, the script's imports are

Â  Â  validated against a security whitelist. Allowed packages include:

Â  Â  â€¢ Standard Library: os, sys, json, csv, datetime, time, etc.

Â  Â  â€¢ Data Science: numpy, pandas, matplotlib, seaborn, scipy, sklearn.

Â  Â  â€¢ Other common packages like requests, beautifulsoup4, etc.

Â  Â Â 

Â  Â  âš ï¸ Scripts attempting to import non-whitelisted packages will fail.



ðŸ“¤ OUTPUT FORMAT:

Â  Â  Tool outputs are strings containing:

Â  Â  â€¢ Standard output and error from command execution.

Â  Â  â€¢ Confirmation messages for file operations.

Â  Â  â€¢ File listings or content for read operations.



Tools provided:

Â  Â  â€¢ execute_terminal_command(command, task_cache_dir?, verbose?) - Run shell commands in workspace.

Â  Â  â€¢ list_workspace_files(task_cache_dir?) - List files in the workspace.

Â  Â  â€¢ read_workspace_file(filename, task_cache_dir?) - Read the content of a file.

Â  Â  â€¢ write_workspace_file(filename, content, task_cache_dir?) - Create or overwrite a file.

Â  Â  â€¢ get_workspace_info(task_cache_dir?) - Get summary statistics of the workspace.

Â  Â  â€¢ get_workspace_structure(task_cache_dir?) - Get a detailed tree-like structure of the workspace.

"""

from __future__ import annotations



import asyncio

import json

import logging

import os

import shutil

import uuid

import datetime

import tempfile

import subprocess

import ast

import re

from pathlib import Path

from typing import Any, Dict, List, Literal, Optional, Set

import hashlib

import sys



from mcp.server.fastmcp import FastMCP



# The five interpreter wrappers come from camelâ€‘ai

from interpreters.interpreters import (

Â  Â  DockerInterpreter,

Â  Â  E2BInterpreter,

Â  Â  InternalPythonInterpreter,

Â  Â  JupyterKernelInterpreter,

Â  Â  SubprocessInterpreter,

)



# --------------------------------------------------------------------------- #

#Â  Default import whitelist configuration

# --------------------------------------------------------------------------- #



DEFAULT_IMPORT_WHITELIST = [

Â  Â  # Standard library

Â  Â  'os', 'sys', 'json', 'csv', 'datetime', 'time', 'math', 'random', 're',Â 

Â  Â  'urllib', 'pathlib', 'collections', 'itertools', 'functools', 'operator',

Â  Â  'typing', 'copy', 'pickle', 'hashlib', 'base64', 'uuid', 'tempfile',

Â  Â  'shutil', 'glob', 'fnmatch', 'zipfile', 'tarfile', 'gzip', 'bz2',

Â  Â  'io', 'sqlite3', 'configparser', 'argparse', 'logging', 'warnings',

Â  Â Â 

Â  Â  # Data science core libraries

Â  Â  'numpy', 'np', 'pandas', 'pd', 'matplotlib', 'plt', 'seaborn', 'sns',

Â  Â  'scipy', 'sklearn', 'plotly', 'dash', 'streamlit',

Â  Â Â 

Â  Â  # Machine learning frameworks

Â  Â  'torch', 'torchvision', 'torchaudio', 'transformers', 'tensorflow', 'tf',

Â  Â  'keras', 'jax', 'flax', 'optax',

Â  Â Â 

Â  Â  # Image processing

Â  Â  'PIL', 'Image', 'cv2', 'skimage', 'imageio',

Â  Â Â 

Â  Â  # Network and data acquisition

Â  Â  'requests', 'urllib3', 'beautifulsoup4', 'bs4', 'scrapy', 'selenium',

Â  Â Â 

Â  Â  # File processing

Â  Â  'openpyxl', 'xlrd', 'xlwt', 'xlsxwriter', 'h5py', 'netCDF4', 'pyarrow',

Â  Â Â 

Â  Â  # Other common tools

Â  Â  'tqdm', 'joblib', 'multiprocessing', 'concurrent', 'threading',

Â  Â  'psutil', 'memory_profiler', 'line_profiler',

]



# --------------------------------------------------------------------------- #

#Â  Import whitelist validation functions

# --------------------------------------------------------------------------- #



def _extract_imports_from_code(code: str) -> Set[str]:

Â  Â  """

Â  Â  Extract all imported module names from Python code.

Â  Â Â 

Â  Â  Args:

Â  Â  Â  Â  code: Python source code

Â  Â  Â  Â Â 

Â  Â  Returns:

Â  Â  Â  Â  Set of imported module names

Â  Â  """

Â  Â  imports = set()

Â  Â Â 

Â  Â  try:

Â  Â  Â  Â  # Parse the code into an AST

Â  Â  Â  Â  tree = ast.parse(code)

Â  Â  Â  Â Â 

Â  Â  Â  Â  for node in ast.walk(tree):

Â  Â  Â  Â  Â  Â  if isinstance(node, ast.Import):

Â  Â  Â  Â  Â  Â  Â  Â  for name in node.names:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Get the top-level module name

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  module_name = name.name.split('.')[0]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  imports.add(module_name)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  elif isinstance(node, ast.ImportFrom):

Â  Â  Â  Â  Â  Â  Â  Â  if node.module:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Get the top-level module name

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  module_name = node.module.split('.')[0]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  imports.add(module_name)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  except SyntaxError:

Â  Â  Â  Â  # If we can't parse the code, also try regex fallback

Â  Â  Â  Â  pass

Â  Â Â 

Â  Â  # Regex fallback for cases where AST parsing fails

Â  Â  import_patterns = [

Â  Â  Â  Â  r'^\s*import\s+([a-zA-Z_][a-zA-Z0-9_]*)',

Â  Â  Â  Â  r'^\s*from\s+([a-zA-Z_][a-zA-Z0-9_]*)\s+import',

Â  Â  ]

Â  Â Â 

Â  Â  for pattern in import_patterns:

Â  Â  Â  Â  matches = re.findall(pattern, code, re.MULTILINE)

Â  Â  Â  Â  for match in matches:

Â  Â  Â  Â  Â  Â  imports.add(match.split('.')[0])

Â  Â Â 

Â  Â  return imports





def _validate_imports(code: str, whitelist: Optional[List[str]] = None) -> tuple[bool, List[str], List[str]]:

Â  Â  """

Â  Â  Validate that all imports in code are in the whitelist.

Â  Â Â 

Â  Â  Args:

Â  Â  Â  Â  code: Python source code to validate

Â  Â  Â  Â  whitelist: List of allowed module names (defaults to DEFAULT_IMPORT_WHITELIST)

Â  Â  Â  Â Â 

Â  Â  Returns:

Â  Â  Â  Â  Tuple of (is_valid, allowed_imports, forbidden_imports)

Â  Â  """

Â  Â  if whitelist is None:

Â  Â  Â  Â  whitelist = DEFAULT_IMPORT_WHITELIST

Â  Â Â 

Â  Â  whitelist_set = set(whitelist)

Â  Â  imports = _extract_imports_from_code(code)

Â  Â Â 

Â  Â  allowed_imports = []

Â  Â  forbidden_imports = []

Â  Â Â 

Â  Â  for imp in imports:

Â  Â  Â  Â  if imp in whitelist_set:

Â  Â  Â  Â  Â  Â  allowed_imports.append(imp)

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  forbidden_imports.append(imp)

Â  Â Â 

Â  Â  is_valid = len(forbidden_imports) == 0

Â  Â  return is_valid, allowed_imports, forbidden_imports



# --------------------------------------------------------------------------- #

#Â  Text truncation utility

# --------------------------------------------------------------------------- #



def _truncate_text(text: str, max_tokens: int, filename: str = "") -> str:

Â  Â  """

Â  Â  Truncate text to approximately max_tokens, showing start and end.

Â  Â Â 

Â  Â  For code files (.py, .js, .ts, .java, .cpp, .c, .h, .cs, .php, .rb, .go, .rs, .swift),

Â  Â  this function is not applied and full content is returned.

Â  Â Â 

Â  Â  Args:

Â  Â  Â  Â  text: Text to truncate

Â  Â  Â  Â  max_tokens: Maximum tokens to show (approximately)

Â  Â  Â  Â  filename: Filename to check if it's a code file

Â  Â  Â  Â Â 

Â  Â  Returns:

Â  Â  Â  Â  Truncated text or full text for code files

Â  Â  """

Â  Â  # Code file extensions that should show full content

Â  Â  code_extensions = {'.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.cs',Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '.php', '.rb', '.go', '.rs', '.swift', '.jsx', '.tsx',Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '.vue', '.svelte', '.kt', '.scala', '.clj', '.hs', '.ml',Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '.sh', '.bash', '.ps1', '.sql', '.r', '.m', '.ipynb'}

Â  Â Â 

Â  Â  if filename:

Â  Â  Â  Â  file_ext = Path(filename).suffix.lower()

Â  Â  Â  Â  if file_ext in code_extensions:

Â  Â  Â  Â  Â  Â  return text

Â  Â Â 

Â  Â  # Rough approximation: 1 token â‰ˆ 4 characters

Â  Â  max_chars = max_tokens * 4

Â  Â Â 

Â  Â  if len(text) <= max_chars:

Â  Â  Â  Â  return text

Â  Â Â 

Â  Â  # Show first and last portions

Â  Â  chunk_size = max_chars // 2

Â  Â  start_chunk = text[:chunk_size]

Â  Â  end_chunk = text[-chunk_size:]

Â  Â Â 

Â  Â  truncated_chars = len(text) - (2 * chunk_size)

Â  Â Â 

Â  Â  return f"{start_chunk}\n\n... [Truncated {truncated_chars:,} characters] ...\n\n{end_chunk}"



# --------------------------------------------------------------------------- #

#Â  Logger setup

# --------------------------------------------------------------------------- #



logging.basicConfig(

Â  Â  level=logging.INFO,

Â  Â  format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',

Â  Â  handlers=[

Â  Â  Â  Â  logging.FileHandler('code_tool.log'),

Â  Â  Â  Â  logging.StreamHandler()

Â  Â  ]

)

logger = logging.getLogger(__name__)



# --------------------------------------------------------------------------- #

#Â  FastMCP server instance

# --------------------------------------------------------------------------- #



mcp = FastMCP("code_exec")



# --------------------------------------------------------------------------- #

#Â  Simplified workspace management

# --------------------------------------------------------------------------- #



def _get_workspace_dir(task_cache_dir: Optional[str] = None) -> str:

Â  Â  """

Â  Â  Get or create the unified workspace directory for a task.

Â  Â Â 

Â  Â  ðŸ“ DIRECTORY STRUCTURE:

Â  Â  Â  Â  task_cache_dir/workspace/Â  (flat structure - all files here)

Â  Â Â 

Â  Â  Args:

Â  Â  Â  Â  task_cache_dir: Task-specific cache directory path. This is REQUIRED.

Â  Â  Â  Â Â 

Â  Â  Returns:

Â  Â  Â  Â  str: Absolute path to workspace directory

Â  Â  Â  Â Â 

Â  Â  Raises:

Â  Â  Â  Â  ValueError: If task_cache_dir is not provided.

Â  Â  """

Â  Â  if not task_cache_dir:

Â  Â  Â  Â  raise ValueError("task_cache_dir must be provided to locate the workspace.")

Â  Â Â 

Â  Â  workspace_dir = Path(task_cache_dir) / "workspace"

Â  Â  workspace_dir.mkdir(parents=True, exist_ok=True)

Â  Â  return str(workspace_dir)





def _log_execution(operation: str, details: dict, workspace_dir: str) -> None:

Â  Â  """

Â  Â  Log execution details for debugging and tracking.

Â  Â Â 

Â  Â  Args:

Â  Â  Â  Â  operation: Type of operation performed

Â  Â  Â  Â  details: Operation-specific details

Â  Â  Â  Â  workspace_dir: Workspace directory path

Â  Â  """

Â  Â  try:

Â  Â  Â  Â  log_entry = {

Â  Â  Â  Â  Â  Â  "timestamp": datetime.datetime.now().isoformat(),

Â  Â  Â  Â  Â  Â  "operation": operation,

Â  Â  Â  Â  Â  Â  "workspace": workspace_dir,

Â  Â  Â  Â  Â  Â  "details": details

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  Â  Â  log_file = Path(workspace_dir).parent / "execution.log"

Â  Â  Â  Â  with open(log_file, "a", encoding="utf-8") as f:

Â  Â  Â  Â  Â  Â  f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

Â  Â  Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  logger.error(f"Failed to log execution: {e}")





# --------------------------------------------------------------------------- #

#Â  Enhanced sandbox wrapper with unified workspace

# --------------------------------------------------------------------------- #



class _UnifiedWorkspaceSandbox:

Â  Â  """

Â  Â  Unified sandbox with flat workspace structure and import whitelist validation.

Â  Â Â 

Â  Â  ðŸŽ¯ KEY FEATURES:

Â  Â  Â  Â  â€¢ All files in single workspace directory

Â  Â  Â  Â  â€¢ Persistent environment across executions

Â  Â  Â  Â  â€¢ Terminal command support

Â  Â  Â  Â  â€¢ File overwrite behavior (no versioning)

Â  Â  Â  Â  â€¢ Import whitelist validation for security

Â  Â Â 

Â  Â  ðŸ”’ IMPORT SECURITY:

Â  Â  Â  Â  â€¢ Validates all import statements against whitelist

Â  Â  Â  Â  â€¢ Supports torch, transformers, and other ML libraries

Â  Â  Â  Â  â€¢ Blocks unauthorized imports for security

Â  Â  """



Â  Â  def __init__(

Â  Â  Â  Â  self,

Â  Â  Â  Â  workspace_dir: str,

Â  Â  Â  Â  sandbox: Literal[

Â  Â  Â  Â  Â  Â  "internal_python", "jupyter", "docker", "subprocess", "e2b"

Â  Â  Â  Â  ] = "subprocess",

Â  Â  Â  Â  *,

Â  Â  Â  Â  verbose: bool = False,

Â  Â  Â  Â  unsafe_mode: bool = False,

Â  Â  Â  Â  import_whitelist: Optional[list[str]] = None,

Â  Â  Â  Â  require_confirm: bool = False,

Â  Â  ) -> None:

Â  Â  Â  Â  self.workspace_dir = workspace_dir

Â  Â  Â  Â  self.verbose = verbose

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Set up import whitelist

Â  Â  Â  Â  self.import_whitelist = import_whitelist or DEFAULT_IMPORT_WHITELIST.copy()

Â  Â  Â  Â  self.unsafe_mode = unsafe_modeÂ  # If True, skip whitelist validation

Â  Â  Â  Â  self.require_confirm = require_confirm



Â  Â  Â  Â  # Initialize the interpreter, ensuring it uses the correct workspace directory

Â  Â  Â  Â  self.interpreter = self._initialize_interpreter(sandbox, self.workspace_dir)



Â  Â  def _initialize_interpreter(self, sandbox_type: str, work_dir: str):

Â  Â  Â  Â  """Initializes the correct interpreter."""

Â  Â  Â  Â Â 

Â  Â  Â  Â  # NOTE: The working directory is now passed to SubprocessInterpreter

Â  Â  Â  Â  # directly via the workspace_dir parameter, eliminating the need

Â  Â  Â  Â  # for os.chdir() in most cases.

Â  Â  Â  Â Â 

Â  Â  Â  Â  if sandbox_type == "internal_python":

Â  Â  Â  Â  Â  Â  return SubprocessInterpreter(

Â  Â  Â  Â  Â  Â  Â  Â  require_confirm=self.require_confirm,

Â  Â  Â  Â  Â  Â  Â  Â  print_stdout=self.verbose,

Â  Â  Â  Â  Â  Â  Â  Â  print_stderr=self.verbose,

Â  Â  Â  Â  Â  Â  Â  Â  workspace_dir=work_dir,

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  elif sandbox_type == "jupyter":

Â  Â  Â  Â  Â  Â  return JupyterKernelInterpreter(

Â  Â  Â  Â  Â  Â  Â  Â  require_confirm=self.require_confirm,

Â  Â  Â  Â  Â  Â  Â  Â  print_stdout=self.verbose,

Â  Â  Â  Â  Â  Â  Â  Â  print_stderr=self.verbose,

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  elif sandbox_type == "docker":

Â  Â  Â  Â  Â  Â  return DockerInterpreter(

Â  Â  Â  Â  Â  Â  Â  Â  require_confirm=self.require_confirm,

Â  Â  Â  Â  Â  Â  Â  Â  print_stdout=self.verbose,

Â  Â  Â  Â  Â  Â  Â  Â  print_stderr=self.verbose,

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  elif sandbox_type == "e2b":

Â  Â  Â  Â  Â  Â  return E2BInterpreter(require_confirm=self.require_confirm)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Default to SubprocessInterpreter

Â  Â  Â  Â  return SubprocessInterpreter(

Â  Â  Â  Â  Â  Â  require_confirm=self.require_confirm,

Â  Â  Â  Â  Â  Â  print_stdout=self.verbose,

Â  Â  Â  Â  Â  Â  print_stderr=self.verbose,

Â  Â  Â  Â  Â  Â  workspace_dir=work_dir,

Â  Â  Â  Â  )



Â  Â  def execute_code(self, code: str, filename: str) -> str:

Â  Â  Â  Â  """

Â  Â  Â  Â  Execute Python code with import whitelist validation and save to specified filename.

Â  Â  Â  Â Â 

Â  Â  Â  Â  ðŸ”§ EXECUTION PROCESS:

Â  Â  Â  Â  Â  Â  1. Validate imports against whitelist

Â  Â  Â  Â  Â  Â  2. Change to workspace directory

Â  Â  Â  Â  Â  Â  3. Write code to specified filename (OVERWRITES existing)

Â  Â  Â  Â  Â  Â  4. Execute code in persistent environment

Â  Â  Â  Â  Â  Â  5. Return execution result with validation info

Â  Â  Â  Â Â 

Â  Â  Â  Â  ðŸ”’ WHITELIST VALIDATION:

Â  Â  Â  Â  Â  Â  â€¢ Checks all import statements in code

Â  Â  Â  Â  Â  Â  â€¢ Allows: torch, transformers, numpy, pandas, matplotlib, etc.

Â  Â  Â  Â  Â  Â  â€¢ Blocks: unauthorized system modules, network libraries not in whitelist

Â  Â  Â  Â Â 

Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  code: Python code to execute

Â  Â  Â  Â  Â  Â  filename: Target filename (REQUIRED, will overwrite if exists)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  str: Execution output with import validation results and any error messages

Â  Â  Â  Â  """

Â  Â  Â  Â  # Ensure workspace directory exists

Â  Â  Â  Â  Path(self.workspace_dir).mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â Â 

Â  Â  Â  Â  original_cwd = os.getcwd()

Â  Â  Â  Â  os.chdir(self.workspace_dir)

Â  Â  Â  Â Â 

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Validate imports if not in unsafe mode

Â  Â  Â  Â  Â  Â  validation_result = ""

Â  Â  Â  Â  Â  Â  if not self.unsafe_mode:

Â  Â  Â  Â  Â  Â  Â  Â  is_valid, allowed_imports, forbidden_imports = _validate_imports(code, self.import_whitelist)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  if not is_valid:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error_msg = f"âŒ IMPORT VALIDATION FAILED\n"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error_msg += f"Forbidden imports: {', '.join(forbidden_imports)}\n"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error_msg += f"Allowed imports in whitelist:\n"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for item in sorted(self.import_whitelist):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error_msg += f"Â  â€¢ {item}\n"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error_msg += f"\nðŸ’¡ Contact admin to add other packages to whitelist."

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return error_msg

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  if allowed_imports:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  validation_result = f"âœ… IMPORTS VALIDATED: {', '.join(sorted(allowed_imports))}\n"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  validation_result += f"ðŸ”’ Whitelist contains: {len(self.import_whitelist)} approved modules\n"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  validation_result += "=" * 50 + "\n"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Write code to file (OVERWRITE if exists)

Â  Â  Â  Â  Â  Â  code_file = Path(self.workspace_dir) / filename

Â  Â  Â  Â  Â  Â  file_existed = code_file.exists()

Â  Â  Â  Â  Â  Â  code_file.write_text(code, encoding="utf-8")

Â  Â  Â  Â  Â  Â  logger.info(f"Code written to: {filename} ({'OVERWRITTEN' if file_existed else 'CREATED'})")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Execute the code

Â  Â  Â  Â  Â  Â  execution_result = self.interpreter.run(code, code_type="python")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Combine validation and execution results

Â  Â  Â  Â  Â  Â  full_result = validation_result + execution_result

Â  Â  Â  Â  Â  Â  return full_result

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  finally:

Â  Â  Â  Â  Â  Â  os.chdir(original_cwd)



Â  Â  def execute_terminal_command(self, command: str) -> str:

Â  Â  Â  Â  """

Â  Â  Â  Â  Execute a shell command in the workspace directory.

Â  Â  Â  Â Â 

Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  command: The shell command to execute.

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  A string containing the stdout and stderr of the command.

Â  Â  Â  Â  """

Â  Â  Â  Â  if self.verbose:

Â  Â  Â  Â  Â  Â  print(f"Executing terminal command: {command} in {self.workspace_dir}")



Â  Â  Â  Â  log_details = {"command": command}

Â  Â  Â  Â  _log_execution("execute_terminal_command", log_details, self.workspace_dir)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Ensure workspace directory exists before attempting to change to it

Â  Â  Â  Â  Path(self.workspace_dir).mkdir(parents=True, exist_ok=True)

Â  Â  Â  Â Â 

Â  Â  Â  Â  original_cwd = os.getcwd()

Â  Â  Â  Â  os.chdir(self.workspace_dir)

Â  Â  Â  Â Â 

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # The interpreter is already configured with the correct working directory.

Â  Â  Â  Â  Â  Â  # We pass 'bash' to indicate it's a shell command.

Â  Â  Â  Â  Â  Â  result = self.interpreter.run(command, code_type="bash")



Â  Â  Â  Â  Â  Â  # Process result based on its type

Â  Â  Â  Â  Â  Â  if isinstance(result, tuple) and len(result) == 2:

Â  Â  Â  Â  Â  Â  Â  Â  # Assuming (exit_code, logs)

Â  Â  Â  Â  Â  Â  Â  Â  exit_code, logs = result

Â  Â  Â  Â  Â  Â  Â  Â  if isinstance(logs, list):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  output_lines = [log.content for log in logs]

Â  Â  Â  Â  Â  Â  Â  Â  else: # Assuming logs is a string

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  output_lines = [str(logs)]



Â  Â  Â  Â  Â  Â  Â  Â  if exit_code == 0:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  status = "âœ… Command executed successfully."

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  status = f"âš ï¸ Command finished with non-zero exit code: {exit_code}."

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  return f"{status}\n\nSTDOUT/STDERR:\n{''.join(output_lines)}"



Â  Â  Â  Â  Â  Â  # Handle simple string output for older interpreter versions

Â  Â  Â  Â  Â  Â  elif isinstance(result, str):

Â  Â  Â  Â  Â  Â  Â  Â  return f"âœ… Command executed.\n\nOutput:\n{result}"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Handle other potential result formats

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  return f"âœ… Command executed.\n\nResult:\n{str(result)}"



Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  return f"âŒ Error executing terminal command: {str(e)}"

Â  Â  Â  Â  finally:

Â  Â  Â  Â  Â  Â  os.chdir(original_cwd)





# Global sandbox instances per task (for environment persistence)

_task_sandboxes: Dict[str, _UnifiedWorkspaceSandbox] = {}



def _get_or_create_sandbox(

Â  Â  workspace_dir: str,

Â  Â  sandbox: str,

Â  Â  verbose: bool,

Â  Â  unsafe_mode: bool,

Â  Â  import_whitelist: Optional[List[str]] = None

) -> _UnifiedWorkspaceSandbox:

Â  Â  """

Â  Â  Get the existing sandbox for the workspace or create a new one.



Â  Â  This function uses a global dictionary to cache sandbox instances based on

Â  Â  the workspace directory. This ensures that the same sandbox is used for

all operations within the same task, preserving state.

Â  Â Â 

Â  Â  Args:

Â  Â  Â  Â  workspace_dir: The absolute path to the workspace directory.

Â  Â  Â  Â  sandbox: The type of sandbox to create.

Â  Â  Â  Â  verbose: Whether to enable verbose logging.

Â  Â  Â  Â  unsafe_mode: Whether to disable security checks (e.g., import validation).

Â  Â  Â  Â  import_whitelist: A list of allowed Python modules.

Â  Â  Â  Â Â 

Â  Â  Returns:

Â  Â  Â  Â  An instance of _UnifiedWorkspaceSandbox.

Â  Â  """

Â  Â  global _task_sandboxes

Â  Â  if workspace_dir not in _task_sandboxes:

Â  Â  Â  Â  if verbose:

Â  Â  Â  Â  Â  Â  print(f"Creating new sandbox for workspace: {workspace_dir}")

Â  Â  Â  Â  _task_sandboxes[workspace_dir] = _UnifiedWorkspaceSandbox(

Â  Â  Â  Â  Â  Â  workspace_dir=workspace_dir,

Â  Â  Â  Â  Â  Â  sandbox=sandbox,

Â  Â  Â  Â  Â  Â  verbose=verbose,

Â  Â  Â  Â  Â  Â  unsafe_mode=unsafe_mode,

Â  Â  Â  Â  Â  Â  import_whitelist=import_whitelist,

Â  Â  Â  Â  )

Â  Â  return _task_sandboxes[workspace_dir]





# --------------------------------------------------------------------------- #

#Â  Enhanced tools with unified workspace

# --------------------------------------------------------------------------- #



def _normalize_filename(filename: str) -> str:

Â  Â  """

Â  Â  Sanitizes and normalizes a filename to prevent directory traversal.



Â  Â  - Removes leading/trailing whitespace and quotes.

Â  Â  - Replaces backslashes with forward slashes.

Â  Â  - Removes any path components (e.g., '/', '..').

Â  Â  - If the filename becomes empty, it defaults to a UUID-based name.

Â  Â Â 

Â  Â  Args:

Â  Â  Â  Â  filename: The original filename provided by the user or agent.

Â  Â  Â  Â Â 

Â  Â  Returns:

Â  Â  Â  Â  A safe, sanitized filename.

Â  Â  """

Â  Â  if not isinstance(filename, str):

Â  Â  Â  Â  filename = str(filename)

Â  Â  Â  Â Â 

Â  Â  # Strip whitespace and quotes

Â  Â  filename = filename.strip().strip('\'"')

Â  Â Â 

Â  Â  # Standardize path separators

Â  Â  filename = filename.replace('\\', '/')

Â  Â Â 

Â  Â  # Remove any directory traversal components

Â  Â  filename = os.path.basename(filename)

Â  Â Â 

Â  Â  # If the filename is empty after sanitization, create a default name

Â  Â  if not filename:

Â  Â  Â  Â  filename = f"file_{uuid.uuid4().hex[:8]}.txt"

Â  Â  Â  Â Â 

Â  Â  return filename





@mcp.tool()

async def execute_terminal_command(

Â  Â  command: str,

Â  Â  task_cache_dir: str | None = None,

Â  Â  verbose: bool = False,

) -> str:

Â  Â  """

Â  Â  Execute terminal command in workspace directory context.



Â  Â  Workflow Note: To run a script (e.g., Python, shell), first create it

Â  Â  using `write_workspace_file`, then execute it with this command.

Â  Â  Example: `python your_script.py`.



Â  Â  ðŸ–¥ï¸ TERMINAL EXECUTION RULES:

Â  Â  Â  Â  â€¢ Commands run in workspace directory (task_cache_dir/workspace/)

Â  Â  Â  Â  â€¢ Can execute Python files: "python script.py"

Â  Â  Â  Â  â€¢ Can perform file operations: "ls", "cat file.txt", "rm file.py"

Â  Â  Â  Â  â€¢ Can install packages: "pip install matplotlib"

Â  Â  Â  Â  â€¢ Working directory is automatically set to workspace



Â  Â  ðŸ“ INPUT:

Â  Â  Â  Â  â€¢ command: Shell command to execute (REQUIRED)

Â  Â  Â  Â  â€¢ task_cache_dir: Task directory path (auto-injected)



Â  Â  ðŸ“¤ OUTPUT:

Â  Â  Â  Â  Returns command output including:

Â  Â  Â  Â  â€¢ STDOUT from command execution

Â  Â  Â  Â  â€¢ STDERR if any errors occurred

Â  Â  Â  Â  â€¢ Exit code status

Â  Â  Â  Â  â€¢ Execution timeout (60 seconds max)



Â  Â  ðŸ”’ ENVIRONMENT:

Â  Â  Â  Â  â€¢ Same workspace as execute_code() operations

Â  Â  Â  Â  â€¢ Can access all files created by code execution

Â  Â  Â  Â  â€¢ Changes persist for subsequent operations



Â  Â  Args:

Â  Â  Â  Â  command: Shell command to execute in workspace

Â  Â  Â  Â  task_cache_dir: Task cache directory path

Â  Â  Â  Â  verbose: Include detailed execution information



Â  Â  Returns:

Â  Â  Â  Â  Command output, stderr, and exit status



Â  Â  Example Usage:

Â  Â  Â  Â  execute_terminal_command("python analysis.py")

Â  Â  Â  Â  execute_terminal_command("ls -la")

Â  Â  Â  Â  execute_terminal_command("pip install pandas")

Â  Â  Â  Â  execute_terminal_command("cat results.txt")

Â  Â  """

Â  Â  if not command:

Â  Â  Â  Â  return "âŒ Error: Command cannot be empty."

Â  Â Â 

Â  Â  # Get workspace directory

Â  Â  workspace_dir = _get_workspace_dir(task_cache_dir)

Â  Â Â 

Â  Â  # Get or create the sandbox for the workspace

Â  Â  sandbox_instance = _get_or_create_sandbox(workspace_dir, "subprocess", verbose, False)

Â  Â Â 

Â  Â  # Execute the command

Â  Â  result = sandbox_instance.execute_terminal_command(command)

Â  Â  Â  Â Â 

Â  Â  return result





@mcp.tool()

async def list_workspace_files(task_cache_dir: str | None = None) -> str:

Â  Â  """

Â  Â  List all files in the unified workspace directory with tree structure using rich.

Â  Â Â 

Â  Â  Workflow Note: After listing files, you can read a file with `read_workspace_file`

Â  Â  or execute a script with `execute_terminal_command`.

Â  Â Â 

Â  Â  ðŸŽ¯ WHEN TO USE THIS FUNCTION:

Â  Â  Â  Â  âœ… USE list_workspace_files() WHEN:

Â  Â  Â  Â  â€¢ You want a quick overview of files in workspace

Â  Â  Â  Â  â€¢ You need basic file information (names, sizes, dates)

Â  Â  Â  Â  â€¢ You want a simple, readable file listing

Â  Â  Â  Â  â€¢ You're checking if specific files exist



Â  Â  Â  Â  ðŸ“ ALTERNATIVE FUNCTIONS:

Â  Â  Â  Â  â€¢ Use get_workspace_structure() for detailed file structure with types

Â  Â  Â  Â  â€¢ Use read_workspace_file() to read content of specific files

Â  Â  Â  Â  â€¢ Use get_workspace_info() for summary statistics only



Â  Â  ðŸ“‚ LISTING BEHAVIOR:

Â  Â  Â  Â  â€¢ Shows all files in tree structure including subdirectories

Â  Â  Â  Â  â€¢ Uses rich tree format for better visualization

Â  Â  Â  Â  â€¢ Includes file sizes and modification times

Â  Â  Â  Â  â€¢ Clean, hierarchical output format

Â  Â Â 

Â  Â  Args:

Â  Â  Â  Â  task_cache_dir: Task cache directory path

Â  Â  Â  Â Â 

Â  Â  Returns:

Â  Â  Â  Â  Formatted tree structure of workspace files with details



Â  Â  Example Output:

Â  Â  Â  Â  ðŸ“ workspace

Â  Â  Â  Â  â”œâ”€â”€ ðŸ“„ analysis.py (2048 bytes, 2024-01-01 14:30:25)

Â  Â  Â  Â  â”œâ”€â”€ ðŸ“„ data.csv (8192 bytes, 2024-01-01 14:28:10)

Â  Â  Â  Â  â””â”€â”€ ðŸ“ upload_files

Â  Â  Â  Â  Â  Â  â”œâ”€â”€ ðŸ“„ document.pdf (4096 bytes, 2024-01-01 14:31:05)

Â  Â  Â  Â  Â  Â  â””â”€â”€ ðŸ“„ image.png (2048 bytes, 2024-01-01 14:32:00)

Â  Â  """

Â  Â  from rich.tree import Tree

Â  Â  from rich.console import Console

Â  Â  from io import StringIO

Â  Â Â 

Â  Â  workspace_dir = _get_workspace_dir(task_cache_dir)

Â  Â  workspace_path = Path(workspace_dir)

Â  Â Â 

Â  Â  if not workspace_path.exists():

Â  Â  Â  Â  return f"ðŸ“‚ Workspace directory does not exist: {workspace_dir}"

Â  Â Â 

Â  Â  def add_files(path, tree_node):

Â  Â  Â  Â  """Recursively add files and directories to the tree."""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  items = sorted(Path(path).iterdir())

Â  Â  Â  Â  Â  Â  for item in items:

Â  Â  Â  Â  Â  Â  Â  Â  if item.is_dir():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add directory node

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  branch = tree_node.add(f"ðŸ“ {item.name}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  add_files(item, branch)

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add file node with size and modification time

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  size = item.stat().st_size

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mtime = datetime.datetime.fromtimestamp(item.stat().st_mtime)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mtime_str = mtime.strftime("%Y-%m-%d %H:%M:%S")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tree_node.add(f"ðŸ“„ {item.name} ({size} bytes, {mtime_str})")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except (OSError, ValueError):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tree_node.add(f"ðŸ“„ {item.name} (size unknown)")

Â  Â  Â  Â  except PermissionError:

Â  Â  Â  Â  Â  Â  tree_node.add("âŒ Permission denied")

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  tree_node.add(f"âŒ Error: {str(e)}")

Â  Â Â 

Â  Â  # Create tree with workspace as root

Â  Â  tree = Tree(f"ðŸ“ {workspace_path.name}")

Â  Â  add_files(workspace_path, tree)

Â  Â Â 

Â  Â  # Capture console output

Â  Â  console = Console(file=StringIO(), width=120)

Â  Â  console.print(tree)

Â  Â  tree_output = console.file.getvalue()

Â  Â Â 

Â  Â  logger.info(f"Generated tree structure for workspace: {workspace_dir}")

Â  Â  return tree_output





@mcp.tool()

async def read_workspace_file(filename: str, task_cache_dir: str | None = None) -> str:

Â  Â  """

Â  Â  Read content of a file from workspace directory.

Â  Â Â 

Â  Â  Workflow Note: After reading a script, you can execute it using the

Â  Â  `execute_terminal_command` tool.

Â  Â Â 

Â  Â  ðŸŽ¯ WHEN TO USE THIS FUNCTION:

Â  Â  Â  Â  âœ… USE read_workspace_file() WHEN:

Â  Â  Â  Â  â€¢ You want to read the content of an existing file

Â  Â  Â  Â  â€¢ You need to examine data files, code files, or text files

Â  Â  Â  Â  â€¢ You want to check what's inside a specific file

Â  Â  Â  Â  â€¢ You're reviewing results or outputs from previous operations



Â  Â  Â  Â  ðŸ“ ALTERNATIVE FUNCTIONS:

Â  Â  Â  Â  â€¢ Use list_workspace_files() to see what files exist first

Â  Â  Â  Â  â€¢ Use get_workspace_structure() for detailed file information

Â  Â  Â  Â  â€¢ Use execute_code() to create and run new Python scripts



Â  Â  ðŸ“– READING BEHAVIOR:

Â  Â  Â  Â  â€¢ Reads files from flat workspace structure

Â  Â  Â  Â  â€¢ Supports text files (Python, CSV, TXT, JSON, etc.)

Â  Â  Â  Â  â€¢ Binary files return size information only

Â  Â  Â  Â  â€¢ Large files may be truncated for display (full content preserved)

Â  Â  Â  Â  â€¢ Shows file path and size information



Â  Â  ðŸ“‚ SUPPORTED FILE TYPES:

Â  Â  Â  Â  â€¢ Text files: .txt, .md, .csv, .json, .py, .js, .html, .css

Â  Â  Â  Â  â€¢ Configuration files: .ini, .yaml, .toml, .conf

Â  Â  Â  Â  â€¢ Data files: .csv, .tsv, .json, .xml

Â  Â  Â  Â  â€¢ Code files: .py, .js, .sql, .sh (shown in full)

Â  Â  Â  Â  â€¢ Binary files: size info only (cannot display content)

Â  Â Â 

Â  Â  Args:

Â  Â  Â  Â  filename: Name of file to read from workspace (simple filename only)

Â  Â  Â  Â  task_cache_dir: Task cache directory path

Â  Â  Â  Â Â 

Â  Â  Returns:

Â  Â  Â  Â  File content or error message with file information



Â  Â  Example Usage:

Â  Â  Â  Â  âœ… CORRECT USAGE:

Â  Â  Â  Â  read_workspace_file("data.csv")

Â  Â  Â  Â  read_workspace_file("analysis.py")

Â  Â  Â  Â  read_workspace_file("config.json")

Â  Â  Â  Â  read_workspace_file("results.txt")

Â  Â  Â  Â Â 

Â  Â  Â  Â  âŒ INCORRECT USAGE:

Â  Â  Â  Â  read_workspace_file("workspace/data.csv")Â  # Don't include workspace/ prefix

Â  Â  Â  Â  read_workspace_file("/full/path/to/file.txt")Â  # Use simple filename only

Â  Â  """

Â  Â  workspace_dir = _get_workspace_dir(task_cache_dir)

Â  Â  file_path = Path(workspace_dir) / filename

Â  Â Â 

Â  Â  if not file_path.exists():

Â  Â  Â  Â  available_files = [f.name for f in Path(workspace_dir).iterdir() if f.is_file()]

Â  Â  Â  Â  return f"âŒ File not found: {filename}\nAvailable files: {available_files}"

Â  Â Â 

Â  Â  if not file_path.is_file():

Â  Â  Â  Â  return f"âŒ Path is not a file: {filename}"

Â  Â Â 

Â  Â  try:

Â  Â  Â  Â  content = file_path.read_text(encoding="utf-8")

Â  Â  Â  Â  logger.info(f"Read file: {filename} ({len(content)} characters)")

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Apply text truncation for non-code files (1000 tokens max)

Â  Â  Â  Â  display_content = _truncate_text(content, max_tokens=1000, filename=filename)

Â  Â  Â  Â Â 

Â  Â  Â  Â  result = []

Â  Â  Â  Â  result.append(f"ðŸ“„ File: {filename}")

Â  Â  Â  Â  result.append(f"ðŸ“ Workspace: {workspace_dir}")

Â  Â  Â  Â  result.append(f"ðŸ“Š Size: {len(content)} characters")

Â  Â  Â  Â  if display_content != content:

Â  Â  Â  Â  Â  Â  result.append("âš ï¸ Content truncated for display (non-code file)")

Â  Â  Â  Â  result.append("=" * 50)

Â  Â  Â  Â  result.append(display_content)

Â  Â  Â  Â Â 

Â  Â  Â  Â  return "\n".join(result)

Â  Â  Â  Â Â 

Â  Â  except UnicodeDecodeError:

Â  Â  Â  Â  # Binary file

Â  Â  Â  Â  size = file_path.stat().st_size

Â  Â  Â  Â  logger.info(f"Binary file detected: {filename} ({size} bytes)")

Â  Â  Â  Â  return f"ðŸ“„ Binary file: {filename} ({size} bytes)\nâŒ Cannot display binary content as text"





@mcp.tool()

async def write_workspace_file(filename: str, content: str, task_cache_dir: str | None = None) -> str:

Â  Â  """

Â  Â  Write content to a file in workspace directory WITHOUT execution.



Â  Â  Workflow Note: After writing an executable script (e.g., a `.py` or `.sh` file),

Â  Â  your next step should be to call `execute_terminal_command` to run it.



Â  Â  ðŸŽ¯ WHEN TO USE THIS FUNCTION:

Â  Â  Â  Â  âœ… USE write_workspace_file() WHEN:

Â  Â  Â  Â  â€¢ You want to SAVE content to a file WITHOUT running it

Â  Â  Â  Â  â€¢ You're creating data files (CSV, JSON, TXT, etc.)

Â  Â  Â  Â  â€¢ You're saving configuration files or documentation

Â  Â  Â  Â  â€¢ You want to store code for later use without executing it immediately

Â  Â  Â  Â  â€¢ You're creating templates, schemas, or reference files

Â  Â  Â  Â  â€¢ You need to save non-Python content (HTML, CSS, SQL, etc.)



Â  Â  Â  Â  âŒ DO NOT use write_workspace_file() when:

Â  Â  Â  Â  â€¢ You want to execute Python code immediately

Â  Â  Â  Â  â€¢ You need to run calculations, generate outputs, or process data

Â  Â  Â  Â  â€¢ You want to see execution results or error messages



Â  Â  ðŸ“ ALTERNATIVE FUNCTIONS:

Â  Â  Â  Â  â€¢ Use execute_code() to create AND execute Python scripts

Â  Â  Â  Â  â€¢ Use read_workspace_file() to read existing files

Â  Â  Â  Â  â€¢ Use list_workspace_files() to see all files in workspace

Â  Â  Â  Â  â€¢ Use get_workspace_structure() to see detailed file structure



Â  Â  ðŸ“ WRITING BEHAVIOR:

Â  Â  Â  Â  â€¢ Creates new file or OVERWRITES existing file

Â  Â  Â  Â  â€¢ No backup or versioning - direct replacement

Â  Â  Â  Â  â€¢ File saved to flat workspace structure

Â  Â  Â  Â  â€¢ Content written exactly as provided (no processing)



Â  Â  ðŸ“‚ SUPPORTED FILE TYPES:

Â  Â  Â  Â  â€¢ Text files: .txt, .md, .csv, .json, .xml, .html, .css

Â  Â  Â  Â  â€¢ Code files: .py, .js, .sql, .sh, .yaml, .toml

Â  Â  Â  Â  â€¢ Configuration files: .ini, .conf, .properties

Â  Â  Â  Â  â€¢ Data files: .csv, .tsv, .json, .xml

Â  Â  Â  Â  â€¢ Any text-based content



Â  Â  Args:

Â  Â  Â  Â  filename: Target filename in workspace (simple filename only)

Â  Â  Â  Â  content: Content to write to file (string)

Â  Â  Â  Â  task_cache_dir: Task cache directory path



Â  Â  Returns:

Â  Â  Â  Â  Success confirmation with file details



Â  Â  Example Usage:

Â  Â  Â  Â  âœ… CORRECT USAGE:

Â  Â  Â  Â  write_workspace_file("data.csv", "name,age\\nJohn,25\\nJane,30")

Â  Â  Â  Â  write_workspace_file("config.json", '{"api_key": "abc123", "timeout": 30}')

Â  Â  Â  Â  write_workspace_file("readme.md", "# Project Documentation\\n\\nThis is a sample project...")

Â  Â  Â  Â  write_workspace_file("template.py", "# Template code for later use\\nimport pandas as pd\\n# TODO: Add functionality")

Â  Â  Â  Â  write_workspace_file("results.txt", "Analysis completed at 2024-01-01\\nTotal records: 1000")

Â  Â  Â  Â Â 

Â  Â  Â  Â  âŒ INCORRECT USAGE:

Â  Â  Â  Â  write_workspace_file("script.py", "print('hello')") # This only saves the file. To see output, you must then call execute_terminal_command("python script.py").

Â  Â  Â  Â  write_workspace_file("workspace/file.txt", "content") # Don't include workspace/ prefix

Â  Â  """

Â  Â  workspace_dir = _get_workspace_dir(task_cache_dir)

Â  Â Â 

Â  Â  try:

Â  Â  Â  Â  normalized_filename = _normalize_filename(filename)

Â  Â  except ValueError as e:

Â  Â  Â  Â  return f"âŒ Filename error: {e}"

Â  Â  Â  Â Â 

Â  Â  file_path = Path(workspace_dir) / normalized_filename

Â  Â Â 

Â  Â  # Check if file exists

Â  Â  file_existed = file_path.exists()

Â  Â Â 

Â  Â  try:

Â  Â  Â  Â  file_path.write_text(content, encoding="utf-8")

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Log the operation

Â  Â  Â  Â  _log_execution("write_workspace_file", {

Â  Â  Â  Â  Â  Â  "filename": filename,

Â  Â  Â  Â  Â  Â  "file_existed": file_existed,

Â  Â  Â  Â  Â  Â  "content_length": len(content)

Â  Â  Â  Â  }, workspace_dir)

Â  Â  Â  Â Â 

Â  Â  Â  Â  logger.info(f"{'Overwrote' if file_existed else 'Created'} file: {filename}")

Â  Â  Â  Â Â 

Â  Â  Â  Â  result = []

Â  Â  Â  Â  result.append(f"âœ… File {'OVERWRITTEN' if file_existed else 'CREATED'}: {filename}")

Â  Â  Â  Â  result.append(f"ðŸ“ Workspace: {workspace_dir}")

Â  Â  Â  Â  result.append(f"ðŸ“Š Size: {len(content)} characters")

Â  Â  Â  Â Â 

Â  Â  Â  Â  return "\n".join(result)

Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  error_msg = f"Failed to write file {filename}: {e}"

Â  Â  Â  Â  logger.error(error_msg)

Â  Â  Â  Â  return f"âŒ {error_msg}"





@mcp.tool()

async def get_workspace_info(task_cache_dir: str | None = None) -> str:

Â  Â  """

Â  Â  Get comprehensive information about the workspace.

Â  Â Â 

Â  Â  ðŸ“Š INFORMATION PROVIDED:

Â  Â  Â  Â  â€¢ Workspace directory path

Â  Â  Â  Â  â€¢ Total files and sizes

Â  Â  Â  Â  â€¢ File type distribution

Â  Â  Â  Â  â€¢ Sandbox environment status

Â  Â  Â  Â  â€¢ Recent activity summary

Â  Â Â 

Â  Â  Args:

Â  Â  Â  Â  task_cache_dir: Task cache directory path

Â  Â  Â  Â Â 

Â  Â  Returns:

Â  Â  Â  Â  Formatted workspace information summary

Â  Â  """

Â  Â  workspace_dir = _get_workspace_dir(task_cache_dir)

Â  Â  workspace_path = Path(workspace_dir)

Â  Â Â 

Â  Â  # Analyze files

Â  Â  file_types = {}

Â  Â  total_size = 0

Â  Â  file_count = 0

Â  Â Â 

Â  Â  for file_path in workspace_path.iterdir():

Â  Â  Â  Â  if file_path.is_file():

Â  Â  Â  Â  Â  Â  file_count += 1

Â  Â  Â  Â  Â  Â  size = file_path.stat().st_size

Â  Â  Â  Â  Â  Â  total_size += size

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  suffix = file_path.suffix.lower() or "no_extension"

Â  Â  Â  Â  Â  Â  if suffix not in file_types:

Â  Â  Â  Â  Â  Â  Â  Â  file_types[suffix] = {"count": 0, "size": 0}

Â  Â  Â  Â  Â  Â  file_types[suffix]["count"] += 1

Â  Â  Â  Â  Â  Â  file_types[suffix]["size"] += size

Â  Â Â 

Â  Â  # Check sandbox status

Â  Â  sandbox_active = workspace_dir in _task_sandboxes

Â  Â Â 

Â  Â  # Format output

Â  Â  result = []

Â  Â  result.append("ðŸ“Š WORKSPACE INFORMATION")

Â  Â  result.append("=" * 50)

Â  Â  result.append(f"ðŸ“ Directory: {workspace_dir}")

Â  Â  result.append(f"ðŸ“„ Total Files: {file_count}")

Â  Â  result.append(f"ðŸ’¾ Total Size: {total_size:,} bytes")

Â  Â  result.append(f"ðŸ”§ Sandbox Active: {'Yes' if sandbox_active else 'No'}")

Â  Â  result.append(f"ðŸ”’ Environment Persistent: {'Yes' if sandbox_active else 'No'}")

Â  Â Â 

Â  Â  if file_types:

Â  Â  Â  Â  result.append("\nðŸ“ˆ FILE TYPE DISTRIBUTION:")

Â  Â  Â  Â  result.append("-" * 30)

Â  Â  Â  Â  for ext, info in sorted(file_types.items()):

Â  Â  Â  Â  Â  Â  result.append(f"Â  {ext:<15} {info['count']:>3} filesÂ  {info['size']:>8,} bytes")

Â  Â Â 

Â  Â  logger.info(f"Workspace info: {file_count} files, {total_size} bytes")

Â  Â  return "\n".join(result)





@mcp.tool()

async def get_workspace_structure(task_cache_dir: str | None = None) -> str:

Â  Â  """

Â  Â  Get detailed file structure and directory tree of the workspace.



Â  Â  ðŸŽ¯ WHEN TO USE THIS FUNCTION:

Â  Â  Â  Â  âœ… USE get_workspace_structure() WHEN:

Â  Â  Â  Â  â€¢ You want to see ALL files in the workspace with details

Â  Â  Â  Â  â€¢ You need to understand the complete file organization

Â  Â  Â  Â  â€¢ You want to see file sizes, types, and modification times

Â  Â  Â  Â  â€¢ You're exploring what files exist before reading or processing them

Â  Â  Â  Â  â€¢ You need a comprehensive overview of workspace contents



Â  Â  ðŸ“‚ INFORMATION PROVIDED:

Â  Â  Â  Â  â€¢ Complete file listing with full paths

Â  Â  Â  Â  â€¢ File sizes in human-readable format

Â  Â  Â  Â  â€¢ File modification timestamps

Â  Â  Â  Â  â€¢ File type identification based on extensions

Â  Â  Â  Â  â€¢ Total workspace statistics (file count, total size)

Â  Â  Â  Â  â€¢ Directory structure visualization



Â  Â  ðŸ“ ALTERNATIVE FUNCTIONS:

Â  Â  Â  Â  â€¢ Use list_workspace_files() for simple file listing

Â  Â  Â  Â  â€¢ Use get_workspace_info() for summary statistics

Â  Â  Â  Â  â€¢ Use read_workspace_file() to read specific files

Â  Â  Â  Â  â€¢ Use write_workspace_file() to create new files



Â  Â  Args:

Â  Â  Â  Â  task_cache_dir: Task cache directory path



Â  Â  Returns:

Â  Â  Â  Â  Detailed workspace structure with file information



Â  Â  Example Output:

Â  Â  Â  Â  ðŸ“‚ WORKSPACE STRUCTURE

Â  Â  Â  Â  ========================================

Â  Â  Â  Â  ðŸ“ Directory: /path/to/workspace

Â  Â  Â  Â  ðŸ“Š Total Files: 5

Â  Â  Â  Â  ðŸ’¾ Total Size: 15.2 KB

Â  Â  Â  Â Â 

Â  Â  Â  Â  ðŸ“„ FILES:

Â  Â  Â  Â  â”œâ”€â”€ analysis.pyÂ  Â  Â  Â  Â  (2.1 KB)Â  [Python]Â  Â  Â 2024-01-01 14:30:25

Â  Â  Â  Â  â”œâ”€â”€ data.csvÂ  Â  Â  Â  Â  Â  Â (8.5 KB)Â  [Data]Â  Â  Â  Â 2024-01-01 14:28:10

Â  Â  Â  Â  â”œâ”€â”€ chart.pngÂ  Â  Â  Â  Â  Â  (4.2 KB)Â  [Image]Â  Â  Â  2024-01-01 14:31:05

Â  Â  Â  Â  â”œâ”€â”€ config.jsonÂ  Â  Â  Â  Â  (0.3 KB)Â  [Config]Â  Â  Â 2024-01-01 14:25:00

Â  Â  Â  Â  â””â”€â”€ results.txtÂ  Â  Â  Â  Â  (0.1 KB)Â  [Text]Â  Â  Â  Â 2024-01-01 14:32:15

Â  Â  """

Â  Â  workspace_dir = _get_workspace_dir(task_cache_dir)

Â  Â  workspace_path = Path(workspace_dir)

Â  Â Â 

Â  Â  if not workspace_path.exists():

Â  Â  Â  Â  return f"ðŸ“‚ Workspace directory does not exist: {workspace_dir}"

Â  Â Â 

Â  Â  # Collect all files with detailed information

Â  Â  files_info = []

Â  Â  total_size = 0

Â  Â Â 

Â  Â  for file_path in workspace_path.iterdir():

Â  Â  Â  Â  if file_path.is_file():

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  stat = file_path.stat()

Â  Â  Â  Â  Â  Â  Â  Â  size = stat.st_size

Â  Â  Â  Â  Â  Â  Â  Â  total_size += size

Â  Â  Â  Â  Â  Â  Â  Â  mtime = datetime.datetime.fromtimestamp(stat.st_mtime)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Determine file type based on extension

Â  Â  Â  Â  Â  Â  Â  Â  suffix = file_path.suffix.lower()

Â  Â  Â  Â  Â  Â  Â  Â  file_type = _get_file_type_description(suffix)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Format file size

Â  Â  Â  Â  Â  Â  Â  Â  size_str = _format_file_size(size)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  files_info.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'name': file_path.name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'size': size,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'size_str': size_str,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'type': file_type,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'modified': mtime.strftime("%Y-%m-%d %H:%M:%S"),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'extension': suffix

Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  Â  Â  except (OSError, ValueError) as e:

Â  Â  Â  Â  Â  Â  Â  Â  # Handle files that can't be accessed

Â  Â  Â  Â  Â  Â  Â  Â  files_info.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'name': file_path.name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'size': 0,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'size_str': 'N/A',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'type': 'Unknown',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'modified': 'N/A',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'extension': file_path.suffix.lower(),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'error': str(e)

Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â Â 

Â  Â  # Sort files by name

Â  Â  files_info.sort(key=lambda x: x['name'])

Â  Â Â 

Â  Â  # Build output

Â  Â  result = []

Â  Â  result.append("ðŸ“‚ WORKSPACE STRUCTURE")

Â  Â  result.append("=" * 50)

Â  Â  result.append(f"ðŸ“ Directory: {workspace_dir}")

Â  Â  result.append(f"ðŸ“Š Total Files: {len(files_info)}")

Â  Â  result.append(f"ðŸ’¾ Total Size: {_format_file_size(total_size)}")

Â  Â  result.append("")

Â  Â Â 

Â  Â  if not files_info:

Â  Â  Â  Â  result.append("ðŸ“­ Workspace is empty (no files)")

Â  Â  else:

Â  Â  Â  Â  result.append("ðŸ“„ FILES:")

Â  Â  Â  Â Â 

Â  Â  Â  Â  for i, file_info in enumerate(files_info):

Â  Â  Â  Â  Â  Â  is_last = (i == len(files_info) - 1)

Â  Â  Â  Â  Â  Â  prefix = "â””â”€â”€" if is_last else "â”œâ”€â”€"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if 'error' in file_info:

Â  Â  Â  Â  Â  Â  Â  Â  result.append(f"{prefix} {file_info['name']:<20} (ERROR: {file_info['error']})")

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  result.append(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{prefix} {file_info['name']:<20} "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"({file_info['size_str']:>8})Â  "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"[{file_info['type']:<8}] "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{file_info['modified']}"

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â Â 

Â  Â  # Add file type summary

Â  Â  if files_info:

Â  Â  Â  Â  type_counts = {}

Â  Â  Â  Â  for file_info in files_info:

Â  Â  Â  Â  Â  Â  file_type = file_info['type']

Â  Â  Â  Â  Â  Â  type_counts[file_type] = type_counts.get(file_type, 0) + 1

Â  Â  Â  Â Â 

Â  Â  Â  Â  result.append("")

Â  Â  Â  Â  result.append("ðŸ“Š FILE TYPE SUMMARY:")

Â  Â  Â  Â  for file_type, count in sorted(type_counts.items()):

Â  Â  Â  Â  Â  Â  result.append(f"Â  â€¢ {file_type}: {count} file{'s' if count != 1 else ''}")

Â  Â Â 

Â  Â  logger.info(f"Generated workspace structure for {len(files_info)} files")

Â  Â  return "\n".join(result)





def _get_file_type_description(extension: str) -> str:

Â  Â  """Get a human-readable description of file type based on extension."""

Â  Â  type_map = {

Â  Â  Â  Â  '.py': 'Python',

Â  Â  Â  Â  '.js': 'JavaScript',

Â  Â  Â  Â  '.ts': 'TypeScript',

Â  Â  Â  Â  '.html': 'HTML',

Â  Â  Â  Â  '.css': 'CSS',

Â  Â  Â  Â  '.json': 'JSON',

Â  Â  Â  Â  '.xml': 'XML',

Â  Â  Â  Â  '.yaml': 'YAML',

Â  Â  Â  Â  '.yml': 'YAML',

Â  Â  Â  Â  '.toml': 'TOML',

Â  Â  Â  Â  '.ini': 'Config',

Â  Â  Â  Â  '.conf': 'Config',

Â  Â  Â  Â  '.cfg': 'Config',

Â  Â  Â  Â  '.txt': 'Text',

Â  Â  Â  Â  '.md': 'Markdown',

Â  Â  Â  Â  '.rst': 'reStruct',

Â  Â  Â  Â  '.csv': 'CSV Data',

Â  Â  Â  Â  '.tsv': 'TSV Data',

Â  Â  Â  Â  '.xlsx': 'Excel',

Â  Â  Â  Â  '.xls': 'Excel',

Â  Â  Â  Â  '.pdf': 'PDF',

Â  Â  Â  Â  '.png': 'PNG Image',

Â  Â  Â  Â  '.jpg': 'JPEG Image',

Â  Â  Â  Â  '.jpeg': 'JPEG Image',

Â  Â  Â  Â  '.gif': 'GIF Image',

Â  Â  Â  Â  '.svg': 'SVG Image',

Â  Â  Â  Â  '.bmp': 'BMP Image',

Â  Â  Â  Â  '.webp': 'WebP Image',

Â  Â  Â  Â  '.mp4': 'Video',

Â  Â  Â  Â  '.avi': 'Video',

Â  Â  Â  Â  '.mov': 'Video',

Â  Â  Â  Â  '.mp3': 'Audio',

Â  Â  Â  Â  '.wav': 'Audio',

Â  Â  Â  Â  '.zip': 'Archive',

Â  Â  Â  Â  '.tar': 'Archive',

Â  Â  Â  Â  '.gz': 'Archive',

Â  Â  Â  Â  '.rar': 'Archive',

Â  Â  Â  Â  '.sql': 'SQL',

Â  Â  Â  Â  '.db': 'Database',

Â  Â  Â  Â  '.sqlite': 'SQLite',

Â  Â  Â  Â  '.log': 'Log',

Â  Â  Â  Â  '.sh': 'Shell',

Â  Â  Â  Â  '.bat': 'Batch',

Â  Â  Â  Â  '.ps1': 'PowerShell',

Â  Â  Â  Â  '.r': 'R Script',

Â  Â  Â  Â  '.ipynb': 'Jupyter NB',

Â  Â  }

Â  Â Â 

Â  Â  return type_map.get(extension, 'Unknown')





def _format_file_size(size_bytes: int) -> str:

Â  Â  """Format file size in human-readable format."""

Â  Â  if size_bytes == 0:

Â  Â  Â  Â  return "0 B"

Â  Â Â 

Â  Â  size_names = ["B", "KB", "MB", "GB", "TB"]

Â  Â  i = 0

Â  Â  size = float(size_bytes)

Â  Â Â 

Â  Â  while size >= 1024.0 and i < len(size_names) - 1:

Â  Â  Â  Â  size /= 1024.0

Â  Â  Â  Â  i += 1

Â  Â Â 

Â  Â  if i == 0:

Â  Â  Â  Â  return f"{int(size)} {size_names[i]}"

Â  Â  else:

Â  Â  Â  Â  return f"{size:.1f} {size_names[i]}"





# --------------------------------------------------------------------------- #

#Â  Entrypoint

# --------------------------------------------------------------------------- #



if __name__ == "__main__":

Â  Â  mcp.run(transport="stdio")







================================================

FILE: server/craw_page.py

================================================

"""

crawl_server.py â€“ FastMCP server exposing a crawlâ€‘andâ€‘clean tool

----------------------------------------------------------------

* crawl_page(url) â€“ Fetch a web page with **crawl4ai.AsyncWebCrawler**

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  and return the readable Markdown.



Dependencies

Â  Â  pip install crawl4ai mcp fastmcp

"""



# --------------------------------------------------------------------------- #

#Â  Imports

# --------------------------------------------------------------------------- #



from mcp.server.fastmcp import FastMCP

from crawl4ai import AsyncWebCrawler



# --------------------------------------------------------------------------- #

#Â  FastMCP server instance

# --------------------------------------------------------------------------- #



mcp = FastMCP("crawl")



# --------------------------------------------------------------------------- #

#Â  Tool

# --------------------------------------------------------------------------- #





@mcp.tool()

async def crawl_page(url: str) -> str:

Â  Â  """Deep crawl and extract key content from a web page (Markdown format).



Â  Â  This tool is designed to perform *deep analysis* on a specific link

Â  Â  retrieved from an earlier search step (e.g., via a web search tool).

Â  Â  Given a fully qualified HTTP(S) URL, it fetches the web page,

Â  Â  removes boilerplate content (menus, ads, nav bars, etc.), and

Â  Â  extracts the core readable content, returning it as a clean,

Â  Â  structured Markdown string.



Â  Â  This Markdown output is well-suited for downstream processing by

Â  Â  large language models (LLMs) for tasks such as:

Â  Â  - Answering user questions from a specific page

Â  Â  - Summarizing long articles or reports

Â  Â  - Extracting facts, definitions, lists, or instructions

Â  Â  - Contextual search over highâ€‘signal content



Â  Â  This is often used as a **follow-up** step after a general-purpose

Â  Â  search tool (e.g., via SearxNG), when the agent needs to "click through"

Â  Â  to an individual link and analyze its full content in a readable form.



Â  Â  Args:

Â  Â  Â  Â  url (str): A valid, fully-qualified URL (http:// or https://) that

Â  Â  Â  Â  Â  Â  points to a real and accessible web page (e.g. news article,

Â  Â  Â  Â  Â  Â  blog post, research page).



Â  Â  Returns:

Â  Â  Â  Â  str: Markdown-formatted main content of the page. If the crawl fails

Â  Â  Â  Â  Â  Â  (due to network errors, access restrictions, or page layout

Â  Â  Â  Â  Â  Â  issues), a plain-text error message is returned instead.

Â  Â  """

Â  Â  try:

Â  Â  Â  Â  async with AsyncWebCrawler() as crawler:

Â  Â  Â  Â  Â  Â  result = await crawler.arun(url=url)

Â  Â  Â  Â  Â  Â  return result.markdown

Â  Â  except Exception as exc:

Â  Â  Â  Â  return f"âš ï¸ Crawl error: {exc!s}"





# --------------------------------------------------------------------------- #

#Â  Entrypoint

# --------------------------------------------------------------------------- #



if __name__ == "__main__":

Â  Â  # Use stdio when embedding inside an agent, or HTTP during development.

Â  Â  mcp.run(transport="stdio")





================================================

FILE: server/documents_tool.py

================================================

"""

mcp_document_processing.py

FastMCP server exposing a Documentâ€‘processing tool

that works without the camel package.

"""



# --------------------------------------------------------------------------- #

#Â  Imports

# --------------------------------------------------------------------------- #

import asyncio, os, io, json, subprocess

from typing import Tuple, Optional, List, Literal



from loguru import logger

from retry import retry



from mcp.server.fastmcp import FastMCP

import anyio



# --- your own helper toolkits ------------------------------------------------ #

#Â  Â (provide these scripts in the Python path)

from image_tool import ask_question_about_image

from excel_tool import ExcelToolkit

from video_tool import ask_question_about_video

# --- thirdâ€‘party libs already used ------------------------------------------ #

import assemblyai as aai

from pptx.enum.shapes import MSO_SHAPE_TYPE

from pptx import Presentation

from PIL import Image

from docx2markdown._docx_to_markdown import docx_to_markdown

from chunkr_ai import Chunkr

import xmltodict

import nest_asyncio

nest_asyncio.apply()



from dotenv import load_dotenv

load_dotenv(".env")





# --------------------------------------------------------------------------- #

#Â  Toolkit implementation (no camel.BaseToolkit!)

# --------------------------------------------------------------------------- #

class DocumentProcessingToolkit:

Â  Â  """

Â  Â  This tool exposes a **generalâ€‘purpose documentâ€‘processing endpoint** that

Â  Â  converts almost any common file you point it to into **clean, readable

Â  Â  text or Markdown**.Â  It is useful whenever an agent needs to â€œlook insideâ€

Â  Â  an arbitrary file before reasoning over its contents.



Â  Â  Conceptually, you can think of it as an *allâ€‘inâ€‘one* â€œopen the file and

Â  Â  give me the textâ€ utility:



Â  Â  â€¢ **Images (.jpg / .jpeg / .png)** â€“ runs a vision model and returns a

Â  Â  detailed caption.Â Â 

Â  Â  â€¢ **Audio (.mp3 / .wav / .m4a)** â€“ performs automatic transcription.Â Â 

Â  Â  â€¢ **PowerPoint (.pptx)** â€“ pulls every textbox, captions each embedded

Â  Â  image, and preserves the slide order.Â Â 

Â  Â  â€¢ **Spreadsheets (.xls / .xlsx / .csv)** â€“ dumps cell values in a

Â  Â  readable, rowâ€‘wise layout.Â Â 

Â  Â  â€¢ **ZIP archives** â€“ unpacks the archive and lists the extracted files.Â Â 

Â  Â  â€¢ **Plain textâ€‘like formats (.py / .txt)** â€“ simply reads the file.Â Â 

Â  Â  â€¢ **JSON, JSONL, JSONâ€‘LD** â€“ returns the parsed JSON structure.Â Â 

Â  Â  â€¢ **XML** â€“ converts to a Python dict (falls back to raw XML on error).Â Â 

Â  Â  â€¢ **Word (.docx)** â€“ converts the entire document to Markdown.Â Â 

Â  Â  â€¢ **Video (.mov)** â€“ generates a descriptive narration of the clip.Â Â 

Â  Â  â€¢ **PDF or any other format** â€“ attempts Chunkr AI extraction first, and

Â  Â  then a plainâ€‘text PDF fallback if Chunkr fails.



Â  Â  Typical downstream tasks include:



Â  Â  - Letting an LLM **summarise** or **answer questions about** a

Â  Â  presentation, spreadsheet, contract, or research paper.Â Â 

Â  Â  - Quickly **indexing** large document batches for semantic search.Â Â 

Â  Â  - **Captioning media assets** (images & video) to improve accessibility.Â Â 

Â  Â  - Turning â€œopaqueâ€ binary files into humanâ€‘readable text for diffing or

Â  Â  version control.



Â  Â  ### Args

Â  Â  `document_path` *(str)* â€“ A fullyâ€‘qualified **local file path** that the

Â  Â  server can access (e.g. `/home/user/input/report.pdf`).Â  Network URLs are

Â  Â  *not* accepted.



Â  Â  ### Returns

Â  Â  `str` â€“ On success, a **plainâ€‘text or Markdown** representation of the

Â  Â  fileâ€™s meaningful content.Â Â 

Â  Â  If the file type is unsupported or an extraction error occurs, the tool

Â  Â  raises an exception containing a diagnostic message.

Â  Â  """



Â  Â  def __init__(self, cache_dir: Optional[str] = None):

Â  Â  Â  Â  self.excel_tool = ExcelToolkit()

Â  Â  Â  Â  self.cache_dir = cache_dir or "tmp/"



Â  Â  # --------------------------------------------------------------------- #

Â  Â  #Â  Public faÃ§ade

Â  Â  # --------------------------------------------------------------------- #

Â  Â  @retry(Exception,tries=5, delay=2, backoff=2)

Â  Â  def extract_document_content(self, document_path: str) -> Tuple[bool, str]:

Â  Â  Â  Â  logger.debug(f"[extract_document_content] {document_path=}")



Â  Â  Â  Â  # 1. Images ----------------------------------------------------------------

Â  Â  Â  Â  if document_path.lower().endswith((".jpg", ".jpeg", ".png")):

Â  Â  Â  Â  Â  Â  caption = asyncio.run(

Â  Â  Â  Â  Â  Â  Â  Â  ask_question_about_image(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  document_path,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Please make a detailed caption about the image."

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  return True, caption



Â  Â  Â  Â  # 2. Audio -----------------------------------------------------------------

Â  Â  Â  Â  if document_path.lower().endswith((".mp3", ".wav", ".m4a")):

Â  Â  Â  Â  Â  Â  aai.settings.api_key = os.getenv("ASSEMBLYAI_API_KEY")

Â  Â  Â  Â  Â  Â  config = aai.TranscriptionConfig(speech_model=aai.SpeechModel.best)

Â  Â  Â  Â  Â  Â  transcript = aai.Transcriber(config=config).transcribe(document_path)

Â  Â  Â  Â  Â  Â  logger.info(transcript.text)

Â  Â  Â  Â  Â  Â  if transcript.status == "error":

Â  Â  Â  Â  Â  Â  Â  Â  raise RuntimeError(f"Transcription failed: {transcript.error}")

Â  Â  Â  Â  Â  Â  return True, transcript.text



Â  Â  Â  Â  # 3. PPTX ------------------------------------------------------------------

Â  Â  Â  Â  if document_path.lower().endswith(".pptx"):

Â  Â  Â  Â  Â  Â  return True, asyncio.run(self._extract_pptx(document_path))



Â  Â  Â  Â  # 4. Spreadsheets -----------------------------------------------------------

Â  Â  Â  Â  if document_path.lower().endswith((".xls", ".xlsx", ".csv")):

Â  Â  Â  Â  Â  Â  return True, self.excel_tool.extract_excel_content(document_path)



Â  Â  Â  Â  # 5. Zip --------------------------------------------------------------------

Â  Â  Â  Â  if document_path.lower().endswith(".zip"):

Â  Â  Â  Â  Â  Â  return True, f"The extracted files are: {self._unzip_file(document_path)}"



Â  Â  Â  Â  # 6. Simple textâ€‘like formats ----------------------------------------------

Â  Â  Â  Â  simple_readers = {

Â  Â  Â  Â  Â  Â  ".py":Â  lambda p: open(p, encoding="utfâ€‘8").read(),

Â  Â  Â  Â  Â  Â  ".txt": lambda p: open(p, encoding="utfâ€‘8").read(),

Â  Â  Â  Â  }

Â  Â  Â  Â  if any(document_path.lower().endswith(ext) for ext in simple_readers):

Â  Â  Â  Â  Â  Â  reader = simple_readers[os.path.splitext(document_path)[1]]

Â  Â  Â  Â  Â  Â  return True, reader(document_path)



Â  Â  Â  Â  # 7. JSONÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â #

Â  Â  Â  Â  if document_path.lower().endswith((".json", ".jsonl", ".jsonld")):

Â  Â  Â  Â  Â  Â  return True, self._extract_json(document_path, encoding="utfâ€‘8")

Â  Â  Â  Â Â 



Â  Â  Â  Â  # 8. XMLÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  #

Â  Â  Â  Â  if document_path.lower().endswith(".xml"):

Â  Â  Â  Â  Â  Â  data = open(document_path, encoding="utfâ€‘8").read()

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  return True, xmltodict.parse(data)

Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  return True, data



Â  Â  Â  Â  # 9. DOCX â†’ markdown -------------------------------------------------------

Â  Â  Â  Â  if document_path.lower().endswith(".docx"):

Â  Â  Â  Â  Â  Â  md_path = f"{os.path.basename(document_path)}.md"

Â  Â  Â  Â  Â  Â  docx_to_markdown(document_path, md_path)

Â  Â  Â  Â  Â  Â  return True, open(md_path, encoding="utfâ€‘8").read()



Â  Â  Â  Â  # 10. MOV video ------------------------------------------------------------

Â  Â  Â  Â  if document_path.lower().endswith(".mov"):

Â  Â  Â  Â  Â  Â  description = ask_question_about_video(

Â  Â  Â  Â  Â  Â  Â  Â  document_path, "Please make a detailed description about the video."

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  return True, description



Â  Â  Â  Â  # 11. Fallback â€“ Chunkr / PDF text -----------------------------------------

Â  Â  Â  Â  return self._try_chunkr_then_fallback(document_path)



Â  Â  # ------------------------------------------------------------------------- #

Â  Â  #Â  helpers

Â  Â  # ------------------------------------------------------------------------- #

Â  Â  def _extract_json(self, json_path: str, encoding: str = "utfâ€‘8") -> str:

Â  Â  Â  Â  with open(json_path, 'r', encoding=encoding) as f:

Â  Â  Â  Â  Â  Â  if json_path.lower().endswith((".json",".jsonld")):

Â  Â  Â  Â  Â  Â  Â  Â  return json.load(f)Â Â 

Â  Â  Â  Â  Â  Â  elif json_path.lower().endswith(".jsonl"):

Â  Â  Â  Â  Â  Â  Â  Â  return [json.loads(line) for line in f]Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 



Â  Â  async def _extract_pptx(self, pptx_path: str) -> str:

Â  Â  Â  Â  prs = Presentation(pptx_path)

Â  Â  Â  Â  base = pptx_path.rsplit(".", 1)[0]

Â  Â  Â  Â  out = []



Â  Â  Â  Â  for slide_idx, slide in enumerate(prs.slides, 1):

Â  Â  Â  Â  Â  Â  txt = [f"Page {slide_idx}"]

Â  Â  Â  Â  Â  Â  captions = []

Â  Â  Â  Â  Â  Â  img_count = 0



Â  Â  Â  Â  Â  Â  for shape_idx, shape in enumerate(slide.shapes):

Â  Â  Â  Â  Â  Â  Â  Â  if getattr(shape, "text", "").strip():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  txt.append(shape.text.strip())



Â  Â  Â  Â  Â  Â  Â  Â  if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  img_count += 1

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  img = Image.open(io.BytesIO(shape.image.blob))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  img_path = f"{base}_slide_{slide_idx}_img_{shape_idx}.png"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  img.save(img_path)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  captions.append(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Image {img_count}: "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  + await ask_question_about_image(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  img_path, "Please make a detailed caption about the image."

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  Â  Â  out.append("\n".join(txt + captions))



Â  Â  Â  Â  return "\n\n".join(out)



Â  Â  def _try_chunkr_then_fallback(self, path: str) -> Tuple[bool, str]:

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  text = asyncio.run(

Â  Â  Â  Â  Â  Â  Â  Â  self._extract_with_chunkr(path, output_format="markdown")

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  return True, text

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.warning(f"Chunkr failed: {e}")

Â  Â  Â  Â  Â  Â  if path.lower().endswith(".pdf"):

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  from PyPDF2 import PdfReader

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  text = "".join(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p.extract_text() for p in PdfReader(open(path, "rb")).pages

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return True, text

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e2:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return False, f"PDF fallback failed: {e2}"

Â  Â  Â  Â  Â  Â  return False, f"Unsupported file type or processing error: {e}"



Â  Â  async def _extract_with_chunkr(

Â  Â  Â  Â  self, path: str, output_format: Literal["json", "markdown"] = "markdown"

Â  Â  ) -> str:

Â  Â  Â  Â  chunkr = Chunkr(api_key=os.getenv("CHUNKR_API_KEY"))

Â  Â  Â  Â  result = await chunkr.upload(path)



Â  Â  Â  Â  if result.status == "Failed":

Â  Â  Â  Â  Â  Â  raise RuntimeError(result.message)



Â  Â  Â  Â  out_path = f"{os.path.basename(path)}.{ 'json' if output_format=='json' else 'md' }"

Â  Â  Â  Â  (result.json if output_format == "json" else result.markdown)(out_path)

Â  Â  Â  Â  return open(out_path, encoding="utfâ€‘8").read()



Â  Â  def _unzip_file(self, zip_path: str) -> List[str]:

Â  Â  Â  Â  dest = os.path.join(self.cache_dir, os.path.splitext(os.path.basename(zip_path))[0])

Â  Â  Â  Â  os.makedirs(dest, exist_ok=True)

Â  Â  Â  Â  subprocess.run(["unzip", "-o", zip_path, "-d", dest], check=True)

Â  Â  Â  Â  return [os.path.join(r, f) for r, _, fs in os.walk(dest) for f in fs]





# --------------------------------------------------------------------------- #

#Â  FastMCP server

# --------------------------------------------------------------------------- #

mcp = FastMCP("document_processing")

toolkit = DocumentProcessingToolkit()





@mcp.tool()

async def process_document(document_path: str) -> str:

Â  Â  """

Â  Â  Process a document at the given *document_path*. The document can be multimedia

Â  Â  (image, audio, video), a presentation (PPTX), a spreadsheet, a ZIP archive,

Â  Â  a text file, JSON, XML, Word document, or PDF.

Â  Â Return the extracted text / markdown representation of *document_path*.

Â  Â supported formats include .png, .jpeg, .jpg, .mp3, .m4a, .PPTX, .xlsx, .csv, .txt, .json, .jsonl, .jsonld, .zip, .xml, .docx, .mov, and .pdf.

Â  Â  """

Â  Â  success, content = await anyio.to_thread.run_sync(

Â  Â  Â  Â  toolkit.extract_document_content, document_path

Â  Â  )

Â  Â  if not success:

Â  Â  Â  Â  raise ValueError(content)

Â  Â  return content





# --------------------------------------------------------------------------- #

#Â  Entrypoint

# --------------------------------------------------------------------------- #

if __name__ == "__main__":

Â  Â  mcp.run(transport="stdio")







================================================

FILE: server/excel_tool.py

================================================

"""

mcp_excel_toolkit.py

FastMCP server â€“ Excel/CSV extraction â†’ Markdown (no camel dependency)

"""



# --------------------------------------------------------------------------- #

#Â  Imports

# --------------------------------------------------------------------------- #

import os

from typing import Optional, Tuple



import anyio

import pandas as pd

from loguru import logger

from mcp.server.fastmcp import FastMCP





# --------------------------------------------------------------------------- #

#Â  Helper class

# --------------------------------------------------------------------------- #

class ExcelToolkit:

Â  Â  """

Â  Â  Extracts rich information from Excel (.xls/.xlsx) or CSV files:

Â  Â  Â  â€¢ Every sheet converted to Markdown using `tabulate`

Â  Â  Â  â€¢ List of all cell coordinates with value + font / fill RGB colours

Â  Â  """



Â  Â  def __init__(self, timeout: Optional[float] = None):

Â  Â  Â  Â  self.timeout = timeout



Â  Â  # ---------- public synchronous API ------------------------------------ #

Â  Â  def extract_excel_content(self, document_path: str) -> str:

Â  Â  Â  Â  if not document_path.lower().endswith((".xls", ".xlsx", ".csv")):

Â  Â  Â  Â  Â  Â  raise ValueError("Only .xls, .xlsx or .csv files are supported.")



Â  Â  Â  Â  logger.info(f"Processing Excel/CSV file: {document_path}")



Â  Â  Â  Â  if document_path.lower().endswith(".csv"):

Â  Â  Â  Â  Â  Â  return self._handle_csv(document_path)



Â  Â  Â  Â  # If it's .xls, convert to .xlsx first

Â  Â  Â  Â  if document_path.lower().endswith(".xls"):

Â  Â  Â  Â  Â  Â  from xls2xlsx import XLS2XLSX



Â  Â  Â  Â  Â  Â  out_path = document_path.rsplit(".", 1)[0] + ".xlsx"

Â  Â  Â  Â  Â  Â  XLS2XLSX(document_path).to_xlsx(out_path)

Â  Â  Â  Â  Â  Â  document_path = out_path

Â  Â  Â  Â  Â  Â  logger.debug(f"Converted .xls â†’ .xlsx : {out_path}")



Â  Â  Â  Â  return self._handle_xlsx(document_path)



Â  Â  # ---------- helpers ---------------------------------------------------- #

Â  Â  def _handle_csv(self, path: str) -> str:

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  df = pd.read_csv(path)

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"CSV read failed: {e}")

Â  Â  Â  Â  Â  Â  raise



Â  Â  Â  Â  return "CSV File Processed:\n" + self._df_to_md(df)



Â  Â  def _handle_xlsx(self, path: str) -> str:

Â  Â  Â  Â  from openpyxl import load_workbook



Â  Â  Â  Â  wb = load_workbook(path, data_only=True)

Â  Â  Â  Â  output_parts = []



Â  Â  Â  Â  for sheet_name in wb.sheetnames:

Â  Â  Â  Â  Â  Â  ws = wb[sheet_name]

Â  Â  Â  Â  Â  Â  cells_info = []



Â  Â  Â  Â  Â  Â  for row in ws.iter_rows():

Â  Â  Â  Â  Â  Â  Â  Â  for cell in row:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  coord = f"{cell.row}{cell.column_letter}"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  font_rgb = (

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cell.font.color.rgb

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if cell.font and cell.font.color and cell.font.color.rgb

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else None

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fill_rgb = (

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cell.fill.fgColor.rgb

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if cell.fill and cell.fill.fgColor and cell.fill.fgColor.rgb

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else None

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cells_info.append(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "index": coord,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "value": cell.value,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "font_color": font_rgb,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "fill_color": fill_rgb,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  Â  Â  # Reâ€‘load sheet via pandas for prettier Markdown

Â  Â  Â  Â  Â  Â  df = pd.read_excel(path, sheet_name=sheet_name, engine="openpyxl")



Â  Â  Â  Â  Â  Â  part = (

Â  Â  Â  Â  Â  Â  Â  Â  f"Sheet Name: {sheet_name}\n"

Â  Â  Â  Â  Â  Â  Â  Â  f"Cell information list:\n{cells_info}\n\n"

Â  Â  Â  Â  Â  Â  Â  Â  f"Markdown View of the content:\n{self._df_to_md(df)}\n"

Â  Â  Â  Â  Â  Â  Â  Â  + "-" * 40

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  output_parts.append(part)



Â  Â  Â  Â  return "\n".join(output_parts)



Â  Â  @staticmethod

Â  Â  def _df_to_md(df: pd.DataFrame) -> str:

Â  Â  Â  Â  from tabulate import tabulate



Â  Â  Â  Â  return tabulate(df, headers="keys", tablefmt="pipe")





# --------------------------------------------------------------------------- #

#Â  FastMCP server

# --------------------------------------------------------------------------- #

mcp = FastMCP("excel_toolkit")

toolkit = ExcelToolkit()





@mcp.tool()

async def extract_excel_content(document_path: str) -> str:

Â  Â  """

Â  Â  Return a Markdownâ€‘rich description of *document_path* (xls/xlsx/csv).

Â  Â  """

Â  Â  # Run the synchronous extractor in a worker thread so the event loop

Â  Â  # stays free.

Â  Â  try:

Â  Â  Â  Â  return await anyio.to_thread.run_sync(

Â  Â  Â  Â  Â  Â  toolkit.extract_excel_content, document_path

Â  Â  Â  Â  )

Â  Â  except Exception as e:

Â  Â  Â  Â  raise ValueError(str(e))





# --------------------------------------------------------------------------- #

#Â  Entrypoint

# --------------------------------------------------------------------------- #

if __name__ == "__main__":

Â  Â  mcp.run(transport="stdio")







================================================

FILE: server/image_tool.py

================================================

"""

mcp_image_analysis.py

FastMCP server â€“ vision tools (image â†’ caption / VQA)

"""



# --------------------------------------------------------------------------- #

#Â  Imports

# --------------------------------------------------------------------------- #

import base64

import io

import os

from typing import Optional



import anyio

import openai

import requests

from PIL import Image

from urllib.parse import urlparse

from openai import AsyncOpenAIÂ  Â  Â  Â  Â  Â  Â  # â† new



from mcp.server.fastmcp import FastMCP

from loguru import logger





from dotenv import load_dotenv

load_dotenv(".env")





# --------------------------------------------------------------------------- #

#Â  Helper class

# --------------------------------------------------------------------------- #

class ImageAnalysisToolkit:

Â  Â  """

Â  Â  Very small wrapper around OpenAI Visionâ€¯GPT models.

Â  Â  Provides two public coroutines:

Â  Â  Â  Â  â€¢ image_to_text

Â  Â  Â  Â  â€¢ ask_question_about_image

Â  Â  """



Â  Â  def __init__(self, timeout: float | None = None):

Â  Â  Â  Â  self.timeout = timeout or 15



Â  Â  # ---------------- public API ------------------------------------------ #

Â  Â  async def image_to_text(

Â  Â  Â  Â  self, image_path: str, sys_prompt: Optional[str] = None

Â  Â  ) -> str:

Â  Â  Â  Â  """

Â  Â  Â  Â  Return a detailed caption of *image_path*.

Â  Â  Â  Â  """

Â  Â  Â  Â  default_sys = (

Â  Â  Â  Â  Â  Â  "You are an expert image analyst. Provide a rich, concise "

Â  Â  Â  Â  Â  Â  "description of everything visible, including any text."

Â  Â  Â  Â  )

Â  Â  Â  Â  return await self._chat_with_image(

Â  Â  Â  Â  Â  Â  image_path,

Â  Â  Â  Â  Â  Â  user_prompt="Please describe the contents of this image.",

Â  Â  Â  Â  Â  Â  system_prompt=sys_prompt or default_sys,

Â  Â  Â  Â  )



Â  Â  async def ask_question_about_image(

Â  Â  Â  Â  self,

Â  Â  Â  Â  image_path: str,

Â  Â  Â  Â  question: str,

Â  Â  Â  Â  sys_prompt: Optional[str] = None,

Â  Â  ) -> str:

Â  Â  Â  Â  """

Â  Â  Â  Â  Answer *question* about *image_path*.

Â  Â  Â  Â  """

Â  Â  Â  Â  default_sys = (

Â  Â  Â  Â  Â  Â  "You answer questions about images by careful visual inspection, "

Â  Â  Â  Â  Â  Â  "reading any text, and reasoning from what you see. Please consider the reqirements of the question carefully"

Â  Â  Â  Â  )

Â  Â  Â  Â  return await self._chat_with_image(

Â  Â  Â  Â  Â  Â  image_path,

Â  Â  Â  Â  Â  Â  user_prompt=question,

Â  Â  Â  Â  Â  Â  system_prompt=sys_prompt or default_sys,

Â  Â  Â  Â  )



Â  Â  # ---------------- implementation -------------------------------------- #

Â  Â  async def _chat_with_image(

Â  Â  Â  Â  self, image_path: str, user_prompt: str, system_prompt: str

Â  Â  ) -> str:

Â  Â  Â  Â  """

Â  Â  Â  Â  Core routine: prepare image, run OpenAI vision chat, return content.

Â  Â  Â  Â  """

Â  Â  Â  Â  image_url = await self._prepare_image(image_path)



Â  Â  Â  Â  messages = [

Â  Â  Â  Â  Â  Â  {"role": "system", "content": system_prompt},

Â  Â  Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  Â  Â  "role": "user",

Â  Â  Â  Â  Â  Â  Â  Â  "content": [

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"type": "text", "text": user_prompt},

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"type": "image_url", "image_url": {"url": image_url}},

Â  Â  Â  Â  Â  Â  Â  Â  ],

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  ]

Â  Â  Â  Â  openai_client = AsyncOpenAI(

Â  Â  Â  Â  Â  Â  api_key=os.getenv("OPENAI_API_KEY"),

Â  Â  Â  Â  Â  Â  base_url=os.getenv("OPENAI_BASE_URL"),Â  # works with Azure etc.

Â  Â  Â  Â  )



Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  logger.info("Sending image to OpenAI ChatCompletion (vision)â€¦")

Â  Â  Â  Â  Â  Â  response = await openai_client.chat.completions.create(

Â  Â  Â  Â  Â  Â  Â  Â  model="gemini-2.5-pro-preview-05-06",

Â  Â  Â  Â  Â  Â  Â  Â  messages=messages,

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  return response.choices[0].message.content.strip()

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"OpenAI call failed: {e}")

Â  Â  Â  Â  Â  Â  raise



Â  Â  async def _prepare_image(self, path: str) -> str:

Â  Â  Â  Â  """

Â  Â  Â  Â  Turn *path* (local path or URL) into a URL or dataâ€‘URL acceptable to

Â  Â  Â  Â  the OpenAI Vision endpoint.

Â  Â  Â  Â  """

Â  Â  Â  Â  parsed = urlparse(path)



Â  Â  Â  Â  # Remote URL â€“ just return it (OpenAI fetches it directly)

Â  Â  Â  Â  if parsed.scheme in ("http", "https"):

Â  Â  Â  Â  Â  Â  logger.debug(f"Using remote image URL: {path}")

Â  Â  Â  Â  Â  Â  return path



Â  Â  Â  Â  # Local file â€“ read & encode

Â  Â  Â  Â  logger.debug(f"Encoding local image: {path}")

Â  Â  Â  Â  data = await anyio.to_thread.run_sync(lambda: open(path, "rb").read())

Â  Â  Â  Â  mime = Image.open(io.BytesIO(data)).get_format_mimetype()

Â  Â  Â  Â  b64 = base64.b64encode(data).decode()

Â  Â  Â  Â  return f"data:{mime};base64,{b64}"





# --------------------------------------------------------------------------- #

#Â  FastMCP server

# --------------------------------------------------------------------------- #

mcp = FastMCP("image_analysis")

toolkit = ImageAnalysisToolkit()





@mcp.tool()

async def image_to_text(image_path: str, sys_prompt: Optional[str] = None) -> str:

Â  Â  """

Â  Â  Generates a detailed and descriptive caption of the image located at *image_path*.



Â  Â  Parameters:

Â  Â  - image_path (str): The file path or URL of the image to analyze.

Â  Â  - sys_prompt (Optional[str]): An optional system prompt that can guide or influence the image captioning behavior, allowing for customization of the description style, detail level, or focus.



Â  Â  Returns:

Â  Â  - str: A detailed natural language description of the content, objects, scene, and relevant features in the image.

Â  Â  """

Â  Â  return await toolkit.image_to_text(image_path, sys_prompt)





@mcp.tool()

async def ask_question_about_image(

Â  Â  image_path: str, question: str, sys_prompt: Optional[str] = None

) -> str:

Â  Â  """

Â  Â  Answers a specific question related to the content of the image located at *image_path*.



Â  Â  Parameters:

Â  Â  - image_path (str): The file path or URL of the image to analyze.

Â  Â  - question (str): The question to be answered about the image content.

Â  Â  - sys_prompt (Optional[str]): An optional system prompt to guide the reasoning or answering style, providing context or desired behavior for the image analysis.



Â  Â  Returns:

Â  Â  - str: The answer to the question based on visual analysis and understanding of the image content.

Â  Â  """

Â  Â  return await toolkit.ask_question_about_image(image_path, question, sys_prompt)



# --------------------------------------------------------------------------- #

#Â  Entrypoint

# --------------------------------------------------------------------------- #

if __name__ == "__main__":

Â  Â  mcp.run(transport="stdio")





================================================

FILE: server/math_tool.py

================================================

from mcp.server.fastmcp import FastMCP



# --------------------------------------------------------------------------- #

#Â  FastMCP server instance

# --------------------------------------------------------------------------- #



mcp = FastMCP("math")



# --------------------------------------------------------------------------- #

#Â  Tools

# --------------------------------------------------------------------------- #



@mcp.tool()

async def add(a: float, b: float) -> float:

Â  Â  """Return a + b."""

Â  Â  return a + b





@mcp.tool()

async def sub(a: float, b: float) -> float:

Â  Â  """Return a - b."""

Â  Â  return a - b





@mcp.tool()

async def multiply(a: float, b: float, decimal_places: int = 2) -> float:

Â  Â  """Return a * b, rounded to *decimal_places* (defaultâ€¯2)."""

Â  Â  return round(a * b, decimal_places)





@mcp.tool()

async def divide(a: float, b: float, decimal_places: int = 2) -> float:

Â  Â  """Return a / b, rounded to *decimal_places* (defaultâ€¯2)."""

Â  Â  if b == 0:

Â  Â  Â  Â  raise ValueError("division by zero")

Â  Â  return round(a / b, decimal_places)





@mcp.tool()

async def round(a: float, decimal_places: int = 0) -> float:Â  Â # noqa: A001

Â  Â  """Round *a* to *decimal_places* (defaultâ€¯0)."""

Â  Â  return round(a, decimal_places)



# --------------------------------------------------------------------------- #

#Â  Entrypoint

# --------------------------------------------------------------------------- #



if __name__ == "__main__":

Â  Â  mcp.run(transport="stdio")





================================================

FILE: server/search_tool.py

================================================

"""

search_server.py â€“ FastMCP server exposing a SearxNG search tool

----------------------------------------------------------------

* search(query, num_results?, category?, language?, time_range?,

Â  Â  Â  Â  Â  safe_search?, host?)

Â  Â  Â  â€“ Privacyâ€‘respecting web search powered by any SearxNG instance.



Dependencies

Â  Â  pip install httpx mcp fastmcp

"""



# --------------------------------------------------------------------------- #

#Â  Imports

# --------------------------------------------------------------------------- #



from typing import Any, Dict, List, Optional



import httpx

from mcp.server.fastmcp import FastMCP



# --------------------------------------------------------------------------- #

#Â  FastMCP server instance

# --------------------------------------------------------------------------- #



mcp = FastMCP("search")



# --------------------------------------------------------------------------- #

#Â  Constants & simple validators

# --------------------------------------------------------------------------- #



DEFAULT_HOST = "http://127.0.0.1:8080"Â  # pick any public instance you like

DEFAULT_CATEGORY = "general"

SAFE_SEARCH_LEVELS = {0, 1, 2}

VALID_TIME_RANGES = {"day", "week", "month", "year"}





def _check_safe(level: int) -> None:

Â  Â  if level not in SAFE_SEARCH_LEVELS:

Â  Â  Â  Â  raise ValueError(f"safe_search must be 0,â€¯1 orâ€¯2 (got {level}).")





def _check_time_range(rng: Optional[str]) -> None:

Â  Â  if rng is not None and rng not in VALID_TIME_RANGES:

Â  Â  Â  Â  raise ValueError(

Â  Â  Â  Â  Â  Â  f"time_range must be one of {sorted(VALID_TIME_RANGES)} (got {rng})."

Â  Â  Â  Â  )





# --------------------------------------------------------------------------- #

#Â  Tool

# --------------------------------------------------------------------------- #





@mcp.tool()

async def search(

Â  Â  query: str,

Â  Â  num_results: int = 10,

Â  Â  category: str | None = None,

Â  Â  language: str = "en",

Â  Â  time_range: str | None = None,

Â  Â  safe_search: int = 1,

Â  Â  host: str = DEFAULT_HOST,

) -> List[Dict[str, str]]:

Â  Â  """Run a web search via any SearxNG instance (defaults to *nicfab.eu*).



Â  Â  Args:

Â  Â  Â  Â  query: The search string.

Â  Â  Â  Â  num_results: Max results to return (defaultâ€¯10 â€” max 20 is polite).

Â  Â  Â  Â  category: SearxNG category: *general*, *images*, *videos*, *news*,

Â  Â  Â  Â  Â  Â  *map*, *music*, *it*, *science*, *files*, *social media*.Â 

Â  Â  Â  Â  Â  Â  Categories not listed here can still be searched with the Search syntax.

Â  Â  Â  Â  Â  Â  Defaults to *general*.

Â  Â  Â  Â  language: Twoâ€‘letter language code (defaultâ€¯"en").

Â  Â  Â  Â  time_range: Optional freshness filter: "day" | "week" | "month" | "year".

Â  Â  Â  Â  safe_search: 0 = off â€¢ 1 = moderate â€¢ 2 = strict (defaultâ€¯1).

Â  Â  Â  Â  host: Full baseâ€‘URL of the SearxNG instance to query.



Â  Â  Returns:

Â  Â  Â  Â  A list of dicts with **title**, **link**, **snippet** keys.

Â  Â  """

Â  Â  _check_safe(safe_search)

Â  Â  _check_time_range(time_range)



Â  Â  params: Dict[str, Any] = {

Â  Â  Â  Â  "q": query,

Â  Â  Â  Â  "format": "json",

Â  Â  Â  Â  "language": language,

Â  Â  Â  Â  "categories": category or DEFAULT_CATEGORY,

Â  Â  Â  Â  "pageno": 1,

Â  Â  Â  Â  "safe": safe_search,

Â  Â  }

Â  Â  if time_range:

Â  Â  Â  Â  params["time_range"] = time_range



Â  Â  url = f"{host.rstrip('/')}/search"



Â  Â  async with httpx.AsyncClient(timeout=20.0, headers={"User-Agent": "fastmcp-search"}) as client:

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  r = await client.get(url, params=params)

Â  Â  Â  Â  Â  Â  r.raise_for_status()

Â  Â  Â  Â  Â  Â  results = r.json().get("results", [])[: num_results]

Â  Â  Â  Â  except Exception as exc:Â  # network / JSON / key errors

Â  Â  Â  Â  Â  Â  return [{"title": "Search error", "link": "", "snippet": str(exc)}]



Â  Â  return [

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  "title": it.get("title", ""),

Â  Â  Â  Â  Â  Â  "link": it.get("url", ""),

Â  Â  Â  Â  Â  Â  "snippet": it.get("content", ""),

Â  Â  Â  Â  }

Â  Â  Â  Â  for it in results

Â  Â  ]





# --------------------------------------------------------------------------- #

#Â  Entrypoint

# --------------------------------------------------------------------------- #



if __name__ == "__main__":

Â  Â  mcp.run(transport="stdio")Â  # or: mcp.run(host="0.0.0.0", port=5002)







================================================

FILE: server/serp_search.py

================================================

from serpapi import GoogleSearch

from mcp.server.fastmcp import FastMCP



# --------------------------------------------------------------------------- #

#Â  FastMCP server instance

# --------------------------------------------------------------------------- #



mcp = FastMCP("serpapi")



# --------------------------------------------------------------------------- #

#Â  Tools

# --------------------------------------------------------------------------- #



@mcp.tool()

async def google_search(query: str) -> list[dict]:

Â  Â  """

Â  Â  Run a Google search via SerpAPI and return the organic results.

Â  Â Â 

Â  Â  Parameters

Â  Â  ----------

Â  Â  query : str

Â  Â  Â  Â  The search query string (e.g., "Coffee")

Â  Â Â 

Â  Â  Returns

Â  Â  -------

Â  Â  list[dict]

Â  Â  Â  Â  The list of organic search results from Google.

Â  Â  """

Â  Â  params = {

Â  Â  Â  Â  "engine": "google",

Â  Â  Â  Â  "q": query,

Â  Â  Â  Â  "api_key": "" # please set your api key

Â  Â  }



Â  Â  search = GoogleSearch(params)

Â  Â  results = search.get_dict()

Â  Â  return results.get("organic_results", [])



# --------------------------------------------------------------------------- #

#Â  Entrypoint

# --------------------------------------------------------------------------- #



if __name__ == "__main__":

Â  Â  mcp.run(transport="stdio")







================================================

FILE: server/video_tool.py

================================================

#!/usr/bin/env python

"""

FastMCP videoâ€‘helper server (async OpenAI edition).



Dependencies

------------

pip install \

Â  yt_dlp ffmpeg-python pillow \

Â  opencv-python numpy scenedetect \

Â  openai>=1.14.0Â  # must include AsyncOpenAI

"""



from __future__ import annotations



import base64

import io

import os

import tempfile

from pathlib import Path

from typing import List



import ffmpeg

import yt_dlp

from mcp.server.fastmcp import FastMCP

from PIL import Image

import cv2

import numpy as np

from scenedetect import open_video, SceneManager

from scenedetect.detectors import ContentDetector

from openai import AsyncOpenAIÂ  Â  Â  Â  Â  Â  Â  # â† new

from dotenv import load_dotenv





load_dotenv()Â  # picks up OPENAI_* variables from .env if present



# --------------------------------------------------------------------------- #

#Â  OpenAI client (async)Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  #

# --------------------------------------------------------------------------- #



openai_client = AsyncOpenAI(

Â  Â  api_key=os.getenv("OPENAI_API_KEY"),

Â  Â  base_url=os.getenv("OPENAI_BASE_URL"),Â  # works with Azure etc.

)



# --------------------------------------------------------------------------- #

#Â  FastMCP instanceÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  #

# --------------------------------------------------------------------------- #



mcp = FastMCP("video_tools")



# --------------------------------------------------------------------------- #

#Â  Helper functions (unchanged except for async OpenAI calls)Â  Â  Â  Â  Â  Â  Â  Â  Â #

# --------------------------------------------------------------------------- #





def _capture_screenshot(video_file: str, timestamp: float, width: int = 320) -> Image.Image:

Â  Â  out, _ = (

Â  Â  Â  Â  ffmpeg.input(video_file, ss=timestamp)

Â  Â  Â  Â  .filter("scale", width, -1)

Â  Â  Â  Â  .output("pipe:", vframes=1, format="image2", vcodec="png")

Â  Â  Â  Â  .run(capture_stdout=True, capture_stderr=True)

Â  Â  )

Â  Â  return Image.open(io.BytesIO(out))





def _extract_audio(video_file: str, output_format: str = "mp3") -> str:

Â  Â  basename = os.path.splitext(video_file)[0]

Â  Â  out_path = f"{basename}.{output_format}"

Â  Â  (

Â  Â  Â  Â  ffmpeg.input(video_file)

Â  Â  Â  Â  .output(out_path, vn=None, acodec="libmp3lame")

Â  Â  Â  Â  .run(quiet=True)

Â  Â  )

Â  Â  return out_path





async def _transcribe_audio_async(audio_path: str) -> str:

Â  Â  """Whisper transcription via AsyncOpenAI; returns '' if disabled."""

Â  Â  if not openai_client.api_key:

Â  Â  Â  Â  return ""

Â  Â  rsp = await openai_client.audio.transcriptions.create(

Â  Â  Â  Â  model="whisper-1",

Â  Â  Â  Â  file=open(audio_path, "rb"),

Â  Â  )

Â  Â  return rsp.text.strip()





def _normalize(img: Image.Image, target_width: int = 512) -> Image.Image:

Â  Â  w, h = img.size

Â  Â  return img.resize((target_width, int(target_width * h / w)), Image.Resampling.LANCZOS).convert("RGB")





def _extract_keyframes(

Â  Â  video_path: str,

Â  Â  frame_interval: float = 4.0,

Â  Â  max_frames: int = 100,

Â  Â  target_width: int = 512,

) -> List[Image.Image]:

Â  Â  cap = cv2.VideoCapture(video_path)

Â  Â  total, fps = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), cap.get(cv2.CAP_PROP_FPS)

Â  Â  duration = total / fps if fps else 0

Â  Â  cap.release()



Â  Â  desired = min(max(int(duration / frame_interval) or 1, 1), max_frames)



Â  Â  video = open_video(video_path)

Â  Â  sm = SceneManager()

Â  Â  sm.add_detector(ContentDetector())

Â  Â  sm.detect_scenes(video)

Â  Â  scenes = sm.get_scene_list()



Â  Â  frames: List[Image.Image] = []

Â  Â  if scenes:

Â  Â  Â  Â  for i in np.linspace(0, len(scenes) - 1, min(len(scenes), desired), dtype=int):

Â  Â  Â  Â  Â  Â  frames.append(_capture_screenshot(video_path, scenes[i][0].get_seconds()))



Â  Â  while len(frames) < desired and duration:

Â  Â  Â  Â  t = len(frames) * frame_interval

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  frames.append(_capture_screenshot(video_path, t))

Â  Â  Â  Â  except ffmpeg.Error:Â  # short file edgeâ€‘case

Â  Â  Â  Â  Â  Â  break



Â  Â  return [_normalize(f, target_width) for f in frames]





def _images_to_base64(imgs: List[Image.Image]) -> List[str]:

Â  Â  out: List[str] = []

Â  Â  for im in imgs:

Â  Â  Â  Â  buf = io.BytesIO()

Â  Â  Â  Â  im.save(buf, format="JPEG", quality=90)

Â  Â  Â  Â  out.append(base64.b64encode(buf.getvalue()).decode())

Â  Â  return out





# --------------------------------------------------------------------------- #

#Â  ToolsÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  #

# --------------------------------------------------------------------------- #



@mcp.tool()

async def download_video(url: str, download_directory: str | None = None) -> str:

Â  Â  """

Â  Â  Downloads a video from the given URL using yt_dlp and returns the local file path.



Â  Â  Parameters:

Â  Â  - url (str): The URL of the video to download. Supported platforms include YouTube and others compatible with yt_dlp.

Â  Â  - download_directory (Optional[str]): Optional path to the directory where the video will be saved. If not specified, a temporary directory will be used.



Â  Â  Returns:

Â  Â  - str: The full file path of the downloaded video file.

Â  Â  """

Â  Â  download_directory = download_directory or tempfile.mkdtemp()

Â  Â  Path(download_directory).mkdir(parents=True, exist_ok=True)

Â  Â  template = str(Path(download_directory) / "%(title)s.%(ext)s")

Â  Â  opts = {"format": "bestvideo+bestaudio/best", "outtmpl": template}

Â  Â  with yt_dlp.YoutubeDL(opts) as ydl:

Â  Â  Â  Â  info = ydl.extract_info(url, download=True)

Â  Â  Â  Â  return ydl.prepare_filename(info)





@mcp.tool()

async def get_video_bytes(video_path: str) -> bytes:

Â  Â  """

Â  Â  Reads and returns the raw bytes of the video file located at *video_path*.



Â  Â  Parameters:

Â  Â  - video_path (str): The local path to the video file.



Â  Â  Returns:

Â  Â  - bytes: The binary content of the video file.

Â  Â  """

Â  Â  with open(video_path, "rb") as fh:

Â  Â  Â  Â  return fh.read()





@mcp.tool()

async def get_video_screenshots(video_path: str, amount: int = 3) -> List[str]:

Â  Â  """

Â  Â  Captures uniformly distributed screenshots from the video file and returns them as base64-encoded JPEG strings.



Â  Â  Parameters:

Â  Â  - video_path (str): The local path to the video file.

Â  Â  - amount (int): The number of screenshots to capture evenly spaced throughout the video. Default is 3.



Â  Â  Returns:

Â  Â  - List[str]: A list of base64-encoded JPEG images representing the captured frames.

Â  Â  """



Â  Â  probe = ffmpeg.probe(video_path)

Â  Â  dur = float(probe["format"]["duration"])

Â  Â  step = dur / (amount + 1)

Â  Â  imgs = [_capture_screenshot(video_path, (i + 1) * step) for i in range(amount)]

Â  Â  return _images_to_base64(imgs)





VIDEO_QA_PROMPT = """

Use the keyâ€‘frames and (optional) transcription to answer.



Transcription (may be empty):

{transcription}



Question:

{question}

""".strip()





@mcp.tool()

async def ask_question_about_video(

Â  Â  video_path: str,

Â  Â  question: str,

Â  Â  use_audio_transcription: bool = False,

) -> str:

Â  Â  """

Â  Â  Answers a specific question about the content of the video file by analyzing keyframes and optionally its audio transcription, using multimodal GPT-4o.



Â  Â  Parameters:

Â  Â  - video_path (str): The local path to the video file.

Â  Â  - question (str): The question to be answered based on the video content.

Â  Â  - use_audio_transcription (bool): Whether to include audio transcription via Whisper model (AsyncOpenAI) to assist in answering the question. Default is False.



Â  Â  Returns:

Â  Â  - str: The AI-generated answer to the question based on visual and (optional) audio analysis of the video.

Â  Â  """

Â  Â  if not openai_client.api_key:

Â  Â  Â  Â  return "OPENAI_API_KEY not set."



Â  Â  frames = _extract_keyframes(video_path)

Â  Â  images_b64 = _images_to_base64(frames)

Â  Â  transcription = ""

Â  Â  if use_audio_transcription:

Â  Â  Â  Â  transcription = await _transcribe_audio_async(_extract_audio(video_path))



Â  Â  user_message = [

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  "type": "text",

Â  Â  Â  Â  Â  Â  "text": VIDEO_QA_PROMPT.format(transcription=transcription, question=question),

Â  Â  Â  Â  },

Â  Â  Â  Â  *(

Â  Â  Â  Â  Â  Â  {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b}"}}

Â  Â  Â  Â  Â  Â  for b in images_b64

Â  Â  Â  Â  ),

Â  Â  ]



Â  Â  chat = await openai_client.chat.completions.create(

Â  Â  Â  Â  model="gemini-2.5-pro-preview-05-06",

Â  Â  Â  Â  messages=[{"role": "user", "content": user_message}],

Â  Â  Â  Â  max_tokens=512,

Â  Â  )

Â  Â  return chat.choices[0].message.content.strip()





# --------------------------------------------------------------------------- #

#Â  EntrypointÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â #

# --------------------------------------------------------------------------- #



if __name__ == "__main__":

Â  Â  mcp.run(transport="stdio")







================================================

FILE: server/interpreters/base.py

================================================

from abc import ABC, abstractmethod

from typing import Any, Dict, List





class BaseInterpreter(ABC):

Â  Â  r"""An abstract base class for code interpreters."""



Â  Â  @abstractmethod

Â  Â  def run(self, code: str, code_type: str) -> str:

Â  Â  Â  Â  r"""Executes the given code based on its type.



Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  code (str): The code to be executed.

Â  Â  Â  Â  Â  Â  code_type (str): The type of the code, which must be one of the

Â  Â  Â  Â  Â  Â  Â  Â  types returned by `supported_code_types()`.



Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  str: The result of the code execution. If the execution fails, this

Â  Â  Â  Â  Â  Â  Â  Â  should include sufficient information to diagnose and correct

Â  Â  Â  Â  Â  Â  Â  Â  the issue.



Â  Â  Â  Â  Raises:

Â  Â  Â  Â  Â  Â  InterpreterError: If the code execution encounters errors that

Â  Â  Â  Â  Â  Â  Â  Â  could be resolved by modifying or regenerating the code.

Â  Â  Â  Â  """

Â  Â  Â  Â  pass



Â  Â  @abstractmethod

Â  Â  def supported_code_types(self) -> List[str]:

Â  Â  Â  Â  r"""Provides supported code types by the interpreter."""

Â  Â  Â  Â  pass



Â  Â  @abstractmethod

Â  Â  def update_action_space(self, action_space: Dict[str, Any]) -> None:

Â  Â  Â  Â  r"""Updates action space for *python* interpreter"""

Â  Â  Â  Â  pass







================================================

FILE: server/interpreters/docker_interpreter.py

================================================

import io

import shlex

import tarfile

import uuid

from pathlib import Path

from typing import TYPE_CHECKING, Any, ClassVar, Dict, List, Optional

import subprocess

from colorama import Fore



from .base import BaseInterpreter

from .interpreter_error import InterpreterError

from .logger import get_logger





def is_docker_running() -> bool:

Â  Â  r"""Check if the Docker daemon is running.



Â  Â  Returns:

Â  Â  Â  Â  bool: True if the Docker daemon is running, False otherwise.

Â  Â  """

Â  Â  try:

Â  Â  Â  Â  result = subprocess.run(

Â  Â  Â  Â  Â  Â  ["docker", "info"],

Â  Â  Â  Â  Â  Â  check=True,

Â  Â  Â  Â  Â  Â  stdout=subprocess.PIPE,

Â  Â  Â  Â  Â  Â  stderr=subprocess.PIPE,

Â  Â  Â  Â  )

Â  Â  Â  Â  return result.returncode == 0

Â  Â  except (subprocess.CalledProcessError, FileNotFoundError):

Â  Â  Â  Â  return False

if TYPE_CHECKING:

Â  Â  from docker.models.containers import Container



logger = get_logger(__name__)





class DockerInterpreter(BaseInterpreter):

Â  Â  r"""A class for executing code files or code strings in a docker container.



Â  Â  This class handles the execution of code in different scripting languages

Â  Â  (currently Python and Bash) within a docker container, capturing their

Â  Â  stdout and stderr streams, and allowing user checking before executing code

Â  Â  strings.



Â  Â  Args:

Â  Â  Â  Â  require_confirm (bool, optional): If `True`, prompt user before

Â  Â  Â  Â  Â  Â  running code strings for security. Defaults to `True`.

Â  Â  Â  Â  print_stdout (bool, optional): If `True`, print the standard

Â  Â  Â  Â  Â  Â  output of the executed code. Defaults to `False`.

Â  Â  Â  Â  print_stderr (bool, optional): If `True`, print the standard error

Â  Â  Â  Â  Â  Â  of the executed code. Defaults to `True`.

Â  Â  """



Â  Â  _CODE_EXECUTE_CMD_MAPPING: ClassVar[Dict[str, str]] = {

Â  Â  Â  Â  "python": "python {file_name}",

Â  Â  Â  Â  "bash": "bash {file_name}",

Â  Â  Â  Â  "r": "Rscript {file_name}",

Â  Â  }



Â  Â  _CODE_EXTENSION_MAPPING: ClassVar[Dict[str, str]] = {

Â  Â  Â  Â  "python": "py",

Â  Â  Â  Â  "bash": "sh",

Â  Â  Â  Â  "r": "R",

Â  Â  }



Â  Â  _CODE_TYPE_MAPPING: ClassVar[Dict[str, str]] = {

Â  Â  Â  Â  "python": "python",

Â  Â  Â  Â  "py3": "python",

Â  Â  Â  Â  "python3": "python",

Â  Â  Â  Â  "py": "python",

Â  Â  Â  Â  "shell": "bash",

Â  Â  Â  Â  "bash": "bash",

Â  Â  Â  Â  "sh": "bash",

Â  Â  Â  Â  "r": "r",

Â  Â  Â  Â  "R": "r",

Â  Â  }



Â  Â  def __init__(

Â  Â  Â  Â  self,

Â  Â  Â  Â  require_confirm: bool = True,

Â  Â  Â  Â  print_stdout: bool = False,

Â  Â  Â  Â  print_stderr: bool = True,

Â  Â  ) -> None:

Â  Â  Â  Â  self.require_confirm = require_confirm

Â  Â  Â  Â  self.print_stdout = print_stdout

Â  Â  Â  Â  self.print_stderr = print_stderr



Â  Â  Â  Â  # lazy initialization of container

Â  Â  Â  Â  self._container: Optional[Container] = None



Â  Â  def __del__(self) -> None:

Â  Â  Â  Â  r"""Destructor for the DockerInterpreter class.



Â  Â  Â  Â  This method ensures that the Docker container is removed when the

Â  Â  Â  Â  interpreter is deleted.

Â  Â  Â  Â  """

Â  Â  Â  Â  if self._container is not None:

Â  Â  Â  Â  Â  Â  self._container.remove(force=True)



Â  Â  def _initialize_if_needed(self) -> None:

Â  Â  Â  Â  if self._container is not None:

Â  Â  Â  Â  Â  Â  return



Â  Â  Â  Â  if not is_docker_running():

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  "Docker daemon is not running. Please install/start docker "

Â  Â  Â  Â  Â  Â  Â  Â  "and try again."

Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  import docker



Â  Â  Â  Â  client = docker.from_env()



Â  Â  Â  Â  # Build custom image with Python and R

Â  Â  Â  Â  dockerfile_path = Path(__file__).parent / "docker"

Â  Â  Â  Â  image_tag = "camel-interpreter:latest"

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  client.images.get(image_tag)

Â  Â  Â  Â  except docker.errors.ImageNotFound:

Â  Â  Â  Â  Â  Â  logger.info("Building custom interpreter image...")

Â  Â  Â  Â  Â  Â  client.images.build(

Â  Â  Â  Â  Â  Â  Â  Â  path=str(dockerfile_path),

Â  Â  Â  Â  Â  Â  Â  Â  tag=image_tag,

Â  Â  Â  Â  Â  Â  Â  Â  rm=True,

Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  self._container = client.containers.run(

Â  Â  Â  Â  Â  Â  image_tag,

Â  Â  Â  Â  Â  Â  detach=True,

Â  Â  Â  Â  Â  Â  name=f"camel-interpreter-{uuid.uuid4()}",

Â  Â  Â  Â  Â  Â  command="tail -f /dev/null",

Â  Â  Â  Â  )



Â  Â  def _create_file_in_container(self, content: str) -> Path:

Â  Â  Â  Â  # get a random name for the file

Â  Â  Â  Â  filename = str(uuid.uuid4())

Â  Â  Â  Â  # create a tar in memory

Â  Â  Â  Â  tar_stream = io.BytesIO()

Â  Â  Â  Â  with tarfile.open(fileobj=tar_stream, mode='w') as tar:

Â  Â  Â  Â  Â  Â  tarinfo = tarfile.TarInfo(name=filename)

Â  Â  Â  Â  Â  Â  tarinfo.size = len(content)

Â  Â  Â  Â  Â  Â  tar.addfile(tarinfo, io.BytesIO(content.encode('utf-8')))

Â  Â  Â  Â  tar_stream.seek(0)



Â  Â  Â  Â  # copy the tar into the container

Â  Â  Â  Â  if self._container is None:

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  "Container is not initialized. Try running the code again."

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  self._container.put_archive("/tmp", tar_stream)

Â  Â  Â  Â  return Path(f"/tmp/{filename}")



Â  Â  def _run_file_in_container(

Â  Â  Â  Â  self,

Â  Â  Â  Â  file: Path,

Â  Â  Â  Â  code_type: str,

Â  Â  ) -> str:

Â  Â  Â  Â  code_type = self._check_code_type(code_type)

Â  Â  Â  Â  commands = shlex.split(

Â  Â  Â  Â  Â  Â  self._CODE_EXECUTE_CMD_MAPPING[code_type].format(

Â  Â  Â  Â  Â  Â  Â  Â  file_name=file.as_posix()

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  )

Â  Â  Â  Â  if self._container is None:

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  "Container is not initialized. Try running the code again."

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  stdout, stderr = self._container.exec_run(

Â  Â  Â  Â  Â  Â  commands,

Â  Â  Â  Â  Â  Â  demux=True,

Â  Â  Â  Â  ).output



Â  Â  Â  Â  if self.print_stdout and stdout:

Â  Â  Â  Â  Â  Â  print("======stdout======")

Â  Â  Â  Â  Â  Â  print(Fore.GREEN + stdout.decode() + Fore.RESET)

Â  Â  Â  Â  Â  Â  print("==================")

Â  Â  Â  Â  if self.print_stderr and stderr:

Â  Â  Â  Â  Â  Â  print("======stderr======")

Â  Â  Â  Â  Â  Â  print(Fore.RED + stderr.decode() + Fore.RESET)

Â  Â  Â  Â  Â  Â  print("==================")

Â  Â  Â  Â  exec_result = f"{stdout.decode()}" if stdout else ""

Â  Â  Â  Â  exec_result += f"(stderr: {stderr.decode()})" if stderr else ""

Â  Â  Â  Â  return exec_result



Â  Â  def run(

Â  Â  Â  Â  self,

Â  Â  Â  Â  code: str,

Â  Â  Â  Â  code_type: str,

Â  Â  ) -> str:

Â  Â  Â  Â  r"""Executes the given code in the container attached to the

Â  Â  Â  Â  interpreter, and captures the stdout and stderr streams.



Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  code (str): The code string to execute.

Â  Â  Â  Â  Â  Â  code_type (str): The type of code to execute (e.g., 'python',

Â  Â  Â  Â  Â  Â  Â  Â  'bash').



Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  str: A string containing the captured stdout and stderr of the

Â  Â  Â  Â  Â  Â  Â  Â  executed code.



Â  Â  Â  Â  Raises:

Â  Â  Â  Â  Â  Â  InterpreterError: If the user declines to run the code, or the

Â  Â  Â  Â  Â  Â  Â  Â  code type is unsupported, or there is an error in the docker

Â  Â  Â  Â  Â  Â  Â  Â  API/container

Â  Â  Â  Â  """

Â  Â  Â  Â  import docker.errors



Â  Â  Â  Â  code_type = self._check_code_type(code_type)



Â  Â  Â  Â  # Print code for security checking

Â  Â  Â  Â  if self.require_confirm:

Â  Â  Â  Â  Â  Â  logger.info(

Â  Â  Â  Â  Â  Â  Â  Â  f"The following {code_type} code will run on your "

Â  Â  Â  Â  Â  Â  Â  Â  f"computer: {code}"

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  while True:

Â  Â  Â  Â  Â  Â  Â  Â  choice = input("Running code? [Y/n]:").lower()

Â  Â  Â  Â  Â  Â  Â  Â  if choice in ["y", "yes", "ye", ""]:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  Â  Â  elif choice not in ["no", "n"]:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Execution halted: User opted not to run the code. "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "This choice stops the current operation and any "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "further code execution."

Â  Â  Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  self._initialize_if_needed()



Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  temp_file_path = self._create_file_in_container(code)

Â  Â  Â  Â  Â  Â  result = self._run_file_in_container(temp_file_path, code_type)

Â  Â  Â  Â  except docker.errors.APIError as e:

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  f"Execution halted due to docker API error: {e.explanation}. "

Â  Â  Â  Â  Â  Â  Â  Â  "This choice stops the current operation and any "

Â  Â  Â  Â  Â  Â  Â  Â  "further code execution."

Â  Â  Â  Â  Â  Â  ) from e

Â  Â  Â  Â  except docker.errors.DockerException as e:

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  f"Execution halted due to docker exceptoin: {e}. "

Â  Â  Â  Â  Â  Â  Â  Â  "This choice stops the current operation and any "

Â  Â  Â  Â  Â  Â  Â  Â  "further code execution."

Â  Â  Â  Â  Â  Â  ) from e

Â  Â  Â  Â  return result



Â  Â  def _check_code_type(self, code_type: str) -> str:

Â  Â  Â  Â  if code_type not in self._CODE_TYPE_MAPPING:

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  f"Unsupported code type {code_type}. Currently "

Â  Â  Â  Â  Â  Â  Â  Â  f"`{self.__class__.__name__}` only supports "

Â  Â  Â  Â  Â  Â  Â  Â  f"{', '.join(self._CODE_EXTENSION_MAPPING.keys())}."

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  return self._CODE_TYPE_MAPPING[code_type]



Â  Â  def supported_code_types(self) -> List[str]:

Â  Â  Â  Â  r"""Provides supported code types by the interpreter."""

Â  Â  Â  Â  return list(self._CODE_EXTENSION_MAPPING.keys())



Â  Â  def update_action_space(self, action_space: Dict[str, Any]) -> None:

Â  Â  Â  Â  r"""Updates action space for *python* interpreter"""

Â  Â  Â  Â  raise RuntimeError(

Â  Â  Â  Â  Â  Â  "SubprocessInterpreter doesn't support " "`action_space`."

Â  Â  Â  Â  )







================================================

FILE: server/interpreters/e2b_interpreter.py

================================================

import os

from typing import Any, ClassVar, Dict, List, Optional



from .base import BaseInterpreter

from .interpreter_error import InterpreterError

from .logger import get_logger





logger = get_logger(__name__)





class E2BInterpreter(BaseInterpreter):

Â  Â  r"""E2B Code Interpreter implementation.



Â  Â  Args:

Â  Â  Â  Â  require_confirm (bool, optional): If True, prompt user before running

Â  Â  Â  Â  Â  Â  code strings for security. (default: :obj:`True`)

Â  Â  """



Â  Â  _CODE_TYPE_MAPPING: ClassVar[Dict[str, Optional[str]]] = {

Â  Â  Â  Â  "python": None,

Â  Â  Â  Â  "py3": None,

Â  Â  Â  Â  "python3": None,

Â  Â  Â  Â  "py": None,

Â  Â  Â  Â  "shell": "bash",

Â  Â  Â  Â  "bash": "bash",

Â  Â  Â  Â  "sh": "bash",

Â  Â  Â  Â  "java": "java",

Â  Â  Â  Â  "javascript": "js",

Â  Â  Â  Â  "r": "r",

Â  Â  }



Â  Â Â 

Â  Â  def __init__(

Â  Â  Â  Â  self,

Â  Â  Â  Â  require_confirm: bool = True,

Â  Â  ) -> None:

Â  Â  Â  Â  from e2b_code_interpreter import Sandbox



Â  Â  Â  Â  self.require_confirm = require_confirm

Â  Â  Â  Â  self._sandbox = Sandbox(api_key=os.environ.get("E2B_API_KEY"))



Â  Â  def __del__(self) -> None:

Â  Â  Â  Â  r"""Destructor for the E2BInterpreter class.



Â  Â  Â  Â  This method ensures that the e2b sandbox is killed when the

Â  Â  Â  Â  interpreter is deleted.

Â  Â  Â  Â  """

Â  Â  Â  Â  if (

Â  Â  Â  Â  Â  Â  hasattr(self, '_sandbox')

Â  Â  Â  Â  Â  Â  and self._sandbox is not None

Â  Â  Â  Â  Â  Â  and self._sandbox.is_running()

Â  Â  Â  Â  ):

Â  Â  Â  Â  Â  Â  self._sandbox.kill()



Â  Â  def run(

Â  Â  Â  Â  self,

Â  Â  Â  Â  code: str,

Â  Â  Â  Â  code_type: str,

Â  Â  ) -> str:

Â  Â  Â  Â  r"""Executes the given code in the e2b sandbox.



Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  code (str): The code string to execute.

Â  Â  Â  Â  Â  Â  code_type (str): The type of code to execute (e.g., 'python',

Â  Â  Â  Â  Â  Â  Â  Â  'bash').



Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  str: The string representation of the output of the executed code.



Â  Â  Â  Â  Raises:

Â  Â  Â  Â  Â  Â  InterpreterError: If the `code_type` is not supported or if any

Â  Â  Â  Â  Â  Â  Â  Â  runtime error occurs during the execution of the code.

Â  Â  Â  Â  """

Â  Â  Â  Â  if code_type not in self._CODE_TYPE_MAPPING:

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  f"Unsupported code type {code_type}. "

Â  Â  Â  Â  Â  Â  Â  Â  f"`{self.__class__.__name__}` only supports "

Â  Â  Â  Â  Â  Â  Â  Â  f"{', '.join(list(self._CODE_TYPE_MAPPING.keys()))}."

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  # Print code for security checking

Â  Â  Â  Â  if self.require_confirm:

Â  Â  Â  Â  Â  Â  logger.info(

Â  Â  Â  Â  Â  Â  Â  Â  f"The following {code_type} code will run on your "

Â  Â  Â  Â  Â  Â  Â  Â  f"e2b sandbox: {code}"

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  while True:

Â  Â  Â  Â  Â  Â  Â  Â  choice = input("Running code? [Y/n]:").lower()

Â  Â  Â  Â  Â  Â  Â  Â  if choice in ["y", "yes", "ye"]:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  Â  Â  elif choice not in ["no", "n"]:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Execution halted: User opted not to run the code. "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "This choice stops the current operation and any "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "further code execution."

Â  Â  Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  if self._CODE_TYPE_MAPPING[code_type] is None:

Â  Â  Â  Â  Â  Â  execution = self._sandbox.run_code(code)

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  execution = self._sandbox.run_code(

Â  Â  Â  Â  Â  Â  Â  Â  code=code, language=self._CODE_TYPE_MAPPING[code_type]

Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  if execution.text and execution.text.lower() != "none":

Â  Â  Â  Â  Â  Â  return execution.text



Â  Â  Â  Â  if execution.logs:

Â  Â  Â  Â  Â  Â  if execution.logs.stdout:

Â  Â  Â  Â  Â  Â  Â  Â  return ",".join(execution.logs.stdout)

Â  Â  Â  Â  Â  Â  elif execution.logs.stderr:

Â  Â  Â  Â  Â  Â  Â  Â  return ",".join(execution.logs.stderr)



Â  Â  Â  Â  return str(execution.error)



Â  Â  def supported_code_types(self) -> List[str]:

Â  Â  Â  Â  r"""Provides supported code types by the interpreter."""

Â  Â  Â  Â  return list(self._CODE_TYPE_MAPPING.keys())



Â  Â  def update_action_space(self, action_space: Dict[str, Any]) -> None:

Â  Â  Â  Â  r"""Updates action space for *python* interpreter"""

Â  Â  Â  Â  raise RuntimeError("E2B doesn't support " "`action_space`.")







================================================

FILE: server/interpreters/internal_python_interpreter.py

================================================

import ast

import difflib

import importlib

import typing

from typing import Any, ClassVar, Dict, List, Optional



from .base import BaseInterpreter

from .interpreter_error import InterpreterError





class InternalPythonInterpreter(BaseInterpreter):

Â  Â  r"""A customized python interpreter to control the execution of

Â  Â  LLM-generated codes. The interpreter makes sure the code can only execute

Â  Â  functions given in action space and import white list. It also supports

Â  Â  fuzzy variable matching to retrieve uncertain input variable name.



Â  Â  .. highlight:: none



Â  Â  This class is adapted from the hugging face implementation

Â  Â  `python_interpreter.py <https://github.com/huggingface/transformers/blob/8f

Â  Â  093fb799246f7dd9104ff44728da0c53a9f67a/src/transformers/tools/python_interp

Â  Â  reter.py>`_. The original license applies::



Â  Â  Â  Â  Copyright 2023 The HuggingFace Inc. team. All rights reserved.



Â  Â  Â  Â  Licensed under the Apache License, Version 2.0 (the "License");

Â  Â  Â  Â  you may not use this file except in compliance with the License.

Â  Â  Â  Â  You may obtain a copy of the License at



Â  Â  Â  Â  Â  Â  http://www.apache.org/licenses/LICENSE-2.0



Â  Â  Â  Â  Unless required by applicable law or agreed to in writing, software

Â  Â  Â  Â  distributed under the License is distributed on an "AS IS" BASIS,

Â  Â  Â  Â  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or

Â  Â  Â  Â  implied. See the License for the specific language governing

Â  Â  Â  Â  permissions and limitations under the License.



Â  Â  We have modified the original code to suit our requirements. We have

Â  Â  encapsulated the original functions within a class and saved the

Â  Â  interpreter state after execution. We have added support for "import"

Â  Â  statements, "for" statements, and several binary and unary operators. We

Â  Â  have added import white list to keep `import` statement safe. Additionally,

Â  Â  we have modified the variable matching logic and introduced the

Â  Â  :obj:`fuzz_state` for fuzzy matching.



Â  Â  Modifications copyright (C) 2023 CAMEL-AI.org



Â  Â  Args:

Â  Â  Â  Â  action_space (Dict[str, Any], optional): A dictionary that maps action

Â  Â  Â  Â  Â  Â  names to their corresponding functions or objects. The interpreter

Â  Â  Â  Â  Â  Â  can only execute functions that are either directly listed in this

Â  Â  Â  Â  Â  Â  dictionary or are member functions of objects listed in this

Â  Â  Â  Â  Â  Â  dictionary. The concept of :obj:`action_space` is derived from

Â  Â  Â  Â  Â  Â  EmbodiedAgent, representing the actions that an agent is capable of

Â  Â  Â  Â  Â  Â  performing. If `None`, set to empty dict. (default: :obj:`None`)

Â  Â  Â  Â  import_white_list (List[str], optional): A list that stores

Â  Â  Â  Â  Â  Â  the Python modules or functions that can be imported in the code.

Â  Â  Â  Â  Â  Â  All submodules and functions of the modules listed in this list are

Â  Â  Â  Â  Â  Â  importable. Any other import statements will be rejected. The

Â  Â  Â  Â  Â  Â  module and its submodule or function name are separated by a period

Â  Â  Â  Â  Â  Â  (:obj:`.`). (default: :obj:`None`)

Â  Â  Â  Â  unsafe_mode (bool, optional): If `True`, the interpreter runs the code

Â  Â  Â  Â  Â  Â  by `eval()` or `exec()` without any security check.

Â  Â  Â  Â  Â  Â  (default: :obj:`False`)

Â  Â  Â  Â  raise_error (bool, optional): Raise error if the interpreter fails.

Â  Â  Â  Â  Â  Â  (default: :obj:`False`)

Â  Â  """



Â  Â  _CODE_TYPES: ClassVar[List[str]] = ["python", "py", "python3", "python2"]





Â  Â  _DEFAULT_BUILTINS = {

Â  Â  Â  Â  "range": range,

Â  Â  Â  Â  "len": len,

Â  Â  Â  Â  "print": print,

Â  Â  }



Â  Â  # âž‹â€ƒé»˜è®¤å…è®¸ import çš„æ¨¡å—ï¼ˆæŒ‰éœ€å¢žåˆ ï¼‰

Â  Â  _DEFAULT_IMPORTS = [

Â  Â  Â  Â  "math",

Â  Â  Â  Â  "sympy",Â  Â  Â  Â  Â  Â  # æ•´ä¸ª sympy

Â  Â  Â  Â  "astor",

Â  Â  Â  Â  "sympy.core",Â  Â  Â  Â # â€”â€”å¦‚æžœæƒ³æ›´ç»†ç²’åº¦å¯æ‹†åˆ†

Â  Â  Â  Â  "sympy.symbols",

Â  Â  ]



Â  Â  def __init__(

Â  Â  Â  Â  self,

Â  Â  Â  Â  action_space: Optional[Dict[str, Any]] = None,

Â  Â  Â  Â  import_white_list: Optional[List[str]] = None,

Â  Â  Â  Â  unsafe_mode: bool = False,

Â  Â  Â  Â  raise_error: bool = False,

Â  Â  ) -> None:

Â  Â  Â  Â  # self.action_space = action_space or dict()

Â  Â  Â  Â  self.action_space = {**self._DEFAULT_BUILTINS, **(action_space or {})}

Â  Â  Â  Â  self.state = self.action_space.copy()

Â  Â  Â  Â  self.fuzz_state: Dict[str, Any] = dict()

Â  Â  Â  Â  # self.import_white_list = import_white_list or list()

Â  Â  Â  Â  self.import_white_list = list(

Â  Â  Â  Â  Â  Â  {*(import_white_list or []), *self._DEFAULT_IMPORTS}

Â  Â  Â  Â  )

Â  Â  Â  Â  self.raise_error = raise_error

Â  Â  Â  Â  self.unsafe_mode = unsafe_mode



Â  Â  def run(self, code: str, code_type: str) -> str:

Â  Â  Â  Â  r"""Executes the given code with specified code type in the

Â  Â  Â  Â  interpreter.



Â  Â  Â  Â  This method takes a string of code and its type, checks if the code

Â  Â  Â  Â  type is supported, and then executes the code. If `unsafe_mode` is

Â  Â  Â  Â  set to `False`, the code is executed in a controlled environment using

Â  Â  Â  Â  the `execute` method. If `unsafe_mode` is `True`, the code is executed

Â  Â  Â  Â  using `eval()` or `exec()` with the action space as the global context.

Â  Â  Â  Â  An `InterpreterError` is raised if the code type is unsupported or if

Â  Â  Â  Â  any runtime error occurs during execution.



Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  code (str): The python code to be executed.

Â  Â  Â  Â  Â  Â  code_type (str): The type of the code, which should be one of the

Â  Â  Â  Â  Â  Â  supported code types (`python`, `py`, `python3`, `python2`).





Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  str: The string representation of the output of the executed code.



Â  Â  Â  Â  Raises:

Â  Â  Â  Â  Â  Â  InterpreterError: If the `code_type` is not supported or if any

Â  Â  Â  Â  Â  Â  Â  Â  runtime error occurs during the execution of the code.

Â  Â  Â  Â  """

Â  Â  Â  Â  if code_type not in self._CODE_TYPES:

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  f"Unsupported code type {code_type}. "

Â  Â  Â  Â  Â  Â  Â  Â  f"`{self.__class__.__name__}` only supports "

Â  Â  Â  Â  Â  Â  Â  Â  f"{', '.join(self._CODE_TYPES)}."

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  if self.unsafe_mode:

Â  Â  Â  Â  Â  Â  import contextlib

Â  Â  Â  Â  Â  Â  import io



Â  Â  Â  Â  Â  Â  # Try to execute first and capture stdout

Â  Â  Â  Â  Â  Â  output_buffer = io.StringIO()

Â  Â  Â  Â  Â  Â  with contextlib.redirect_stdout(output_buffer):

Â  Â  Â  Â  Â  Â  Â  Â  exec(code, self.action_space)

Â  Â  Â  Â  Â  Â  result = output_buffer.getvalue()



Â  Â  Â  Â  Â  Â  # If no output was captured, try to evaluate the code

Â  Â  Â  Â  Â  Â  if not result:

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = str(eval(code, self.action_space))

Â  Â  Â  Â  Â  Â  Â  Â  except (SyntaxError, NameError):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = ""Â  # If eval fails, return empty string



Â  Â  Â  Â  Â  Â  return result

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  return str(self.execute(code))



Â  Â  def update_action_space(self, action_space: Dict[str, Any]) -> None:

Â  Â  Â  Â  r"""Updates action space for *python* interpreter."""

Â  Â  Â  Â  self.action_space.update(action_space)



Â  Â  def supported_code_types(self) -> List[str]:

Â  Â  Â  Â  r"""Provides supported code types by the interpreter."""

Â  Â  Â  Â  return self._CODE_TYPES



Â  Â  def execute(

Â  Â  Â  Â  self,

Â  Â  Â  Â  code: str,

Â  Â  Â  Â  state: Optional[Dict[str, Any]] = None,

Â  Â  Â  Â  fuzz_state: Optional[Dict[str, Any]] = None,

Â  Â  Â  Â  keep_state: bool = True,

Â  Â  ) -> Any:

Â  Â  Â  Â  r"""Execute the input python codes in a security environment.



Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  code (str): Generated python code to be executed.

Â  Â  Â  Â  Â  Â  state (Optional[Dict[str, Any]], optional): External variables that

Â  Â  Â  Â  Â  Â  Â  Â  may be used in the generated code. (default: :obj:`None`)

Â  Â  Â  Â  Â  Â  fuzz_state (Optional[Dict[str, Any]], optional): External variables

Â  Â  Â  Â  Â  Â  Â  Â  that do not have certain variable names. The interpreter will

Â  Â  Â  Â  Â  Â  Â  Â  use fuzzy matching to access these variables. For example, if

Â  Â  Â  Â  Â  Â  Â  Â  :obj:`fuzz_state` has a variable :obj:`image`, the generated

Â  Â  Â  Â  Â  Â  Â  Â  code can use :obj:`input_image` to access it. (default:

Â  Â  Â  Â  Â  Â  Â  Â  :obj:`None`)

Â  Â  Â  Â  Â  Â  keep_state (bool, optional):Â  If :obj:`True`, :obj:`state` and

Â  Â  Â  Â  Â  Â  Â  Â  :obj:`fuzz_state` will be kept for later execution. Otherwise,

Â  Â  Â  Â  Â  Â  Â  Â  they will be cleared. (default: :obj:`True`)



Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  Any: The value of the last statement (excluding "import") in the

Â  Â  Â  Â  Â  Â  Â  Â  code. For this interpreter, the value of an expression is its

Â  Â  Â  Â  Â  Â  Â  Â  value, the value of an "assign" statement is the assigned

Â  Â  Â  Â  Â  Â  Â  Â  value, and the value of an "if" and "for" block statement is

Â  Â  Â  Â  Â  Â  Â  Â  the value of the last statement in the block.

Â  Â  Â  Â  """



Â  Â  Â  Â  if state is not None:

Â  Â  Â  Â  Â  Â  self.state.update(state)

Â  Â  Â  Â  if fuzz_state is not None:

Â  Â  Â  Â  Â  Â  self.fuzz_state.update(fuzz_state)



Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  expression = ast.parse(code)

Â  Â  Â  Â  except SyntaxError as e:

Â  Â  Â  Â  Â  Â  if self.raise_error:

Â  Â  Â  Â  Â  Â  Â  Â  raise InterpreterError(f"Syntax error in code: {e}")

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  import traceback



Â  Â  Â  Â  Â  Â  Â  Â  return traceback.format_exc()



Â  Â  Â  Â  result = None

Â  Â  Â  Â  for idx, node in enumerate(expression.body):

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  line_result = self._execute_ast(node)

Â  Â  Â  Â  Â  Â  except InterpreterError as e:

Â  Â  Â  Â  Â  Â  Â  Â  if not keep_state:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.clear_state()

Â  Â  Â  Â  Â  Â  Â  Â  msg = (

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Evaluation of the code stopped at node {idx}. "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"See:\n{e}"

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  # More information can be provided by `ast.unparse()`,

Â  Â  Â  Â  Â  Â  Â  Â  # which is new in python 3.9.

Â  Â  Â  Â  Â  Â  Â  Â  if self.raise_error:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise InterpreterError(msg)

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  import traceback



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return traceback.format_exc()

Â  Â  Â  Â  Â  Â  if line_result is not None:

Â  Â  Â  Â  Â  Â  Â  Â  result = line_result



Â  Â  Â  Â  if not keep_state:

Â  Â  Â  Â  Â  Â  self.clear_state()



Â  Â  Â  Â  return result



Â  Â  def clear_state(self) -> None:

Â  Â  Â  Â  r"""Initialize :obj:`state` and :obj:`fuzz_state`."""

Â  Â  Â  Â  self.state = self.action_space.copy()

Â  Â  Â  Â  self.fuzz_state = {}



Â  Â  # ast.Index is deprecated after python 3.9, which cannot pass type check,

Â  Â  # but is still necessary for older versions.

Â  Â  @typing.no_type_check

Â  Â  def _execute_ast(self, expression: ast.AST) -> Any:

Â  Â  Â  Â  if isinstance(expression, ast.Assign):

Â  Â  Â  Â  Â  Â  # Assignment -> evaluate the assignment which should

Â  Â  Â  Â  Â  Â  # update the state. We return the variable assigned as it may

Â  Â  Â  Â  Â  Â  # be used to determine the final result.

Â  Â  Â  Â  Â  Â  return self._execute_assign(expression)

Â  Â  Â  Â  elif isinstance(expression, ast.Attribute):

Â  Â  Â  Â  Â  Â  value = self._execute_ast(expression.value)

Â  Â  Â  Â  Â  Â  return getattr(value, expression.attr)

Â  Â  Â  Â  elif isinstance(expression, ast.BinOp):

Â  Â  Â  Â  Â  Â  # Binary Operator -> return the result value

Â  Â  Â  Â  Â  Â  return self._execute_binop(expression)

Â  Â  Â  Â  elif isinstance(expression, ast.Call):

Â  Â  Â  Â  Â  Â  # Function call -> return the value of the function call

Â  Â  Â  Â  Â  Â  return self._execute_call(expression)

Â  Â  Â  Â  elif isinstance(expression, ast.Compare):

Â  Â  Â  Â  Â  Â  # Compare -> return True or False

Â  Â  Â  Â  Â  Â  return self._execute_condition(expression)

Â  Â  Â  Â  elif isinstance(expression, ast.Constant):

Â  Â  Â  Â  Â  Â  # Constant -> just return the value

Â  Â  Â  Â  Â  Â  return expression.value

Â  Â  Â  Â  elif isinstance(expression, ast.Dict):

Â  Â  Â  Â  Â  Â  # Dict -> evaluate all keys and values

Â  Â  Â  Â  Â  Â  result: Dict = {}

Â  Â  Â  Â  Â  Â  for k, v in zip(expression.keys, expression.values):

Â  Â  Â  Â  Â  Â  Â  Â  if k is not None:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result[self._execute_ast(k)] = self._execute_ast(v)

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result.update(self._execute_ast(v))

Â  Â  Â  Â  Â  Â  return result

Â  Â  Â  Â  elif isinstance(expression, ast.Expr):

Â  Â  Â  Â  Â  Â  # Expression -> evaluate the content

Â  Â  Â  Â  Â  Â  return self._execute_ast(expression.value)

Â  Â  Â  Â  elif isinstance(expression, ast.For):

Â  Â  Â  Â  Â  Â  return self._execute_for(expression)

Â  Â  Â  Â  elif isinstance(expression, ast.FormattedValue):

Â  Â  Â  Â  Â  Â  # Formatted value (part of f-string) -> evaluate the content

Â  Â  Â  Â  Â  Â  # and return

Â  Â  Â  Â  Â  Â  return self._execute_ast(expression.value)

Â  Â  Â  Â  elif isinstance(expression, ast.If):

Â  Â  Â  Â  Â  Â  # If -> execute the right branch

Â  Â  Â  Â  Â  Â  return self._execute_if(expression)

Â  Â  Â  Â  elif isinstance(expression, ast.Import):

Â  Â  Â  Â  Â  Â  # Import -> add imported names in self.state and return None.

Â  Â  Â  Â  Â  Â  self._execute_import(expression)

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  elif isinstance(expression, ast.ImportFrom):

Â  Â  Â  Â  Â  Â  self._execute_import_from(expression)

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  elif hasattr(ast, "Index") and isinstance(expression, ast.Index):

Â  Â  Â  Â  Â  Â  # cannot pass type check

Â  Â  Â  Â  Â  Â  return self._execute_ast(expression.value)

Â  Â  Â  Â  elif isinstance(expression, ast.JoinedStr):

Â  Â  Â  Â  Â  Â  return "".join(

Â  Â  Â  Â  Â  Â  Â  Â  [str(self._execute_ast(v)) for v in expression.values]

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  elif isinstance(expression, ast.List):

Â  Â  Â  Â  Â  Â  # List -> evaluate all elements

Â  Â  Â  Â  Â  Â  return [self._execute_ast(elt) for elt in expression.elts]

Â  Â  Â  Â  elif isinstance(expression, ast.Name):

Â  Â  Â  Â  Â  Â  # Name -> pick up the value in the state

Â  Â  Â  Â  Â  Â  return self._execute_name(expression)

Â  Â  Â  Â  elif isinstance(expression, ast.Subscript):

Â  Â  Â  Â  Â  Â  # Subscript -> return the value of the indexing

Â  Â  Â  Â  Â  Â  return self._execute_subscript(expression)

Â  Â  Â  Â  elif isinstance(expression, ast.Tuple):

Â  Â  Â  Â  Â  Â  return tuple([self._execute_ast(elt) for elt in expression.elts])

Â  Â  Â  Â  elif isinstance(expression, ast.UnaryOp):

Â  Â  Â  Â  Â  Â  # Binary Operator -> return the result value

Â  Â  Â  Â  Â  Â  return self._execute_unaryop(expression)

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  # For now we refuse anything else. Let's add things as we need

Â  Â  Â  Â  Â  Â  # them.

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  f"{expression.__class__.__name__} is not supported."

Â  Â  Â  Â  Â  Â  )



Â  Â  def _execute_assign(self, assign: ast.Assign) -> Any:

Â  Â  Â  Â  targets = assign.targets

Â  Â  Â  Â  result = self._execute_ast(assign.value)



Â  Â  Â  Â  for target in targets:

Â  Â  Â  Â  Â  Â  self._assign(target, result)

Â  Â  Â  Â  return result



Â  Â  def _assign(self, target: ast.expr, value: Any):

Â  Â  Â  Â  if isinstance(target, ast.Name):

Â  Â  Â  Â  Â  Â  self.state[target.id] = value

Â  Â  Â  Â  elif isinstance(target, ast.Tuple):

Â  Â  Â  Â  Â  Â  if not isinstance(value, tuple):

Â  Â  Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Expected type tuple, but got"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{value.__class__.__name__} instead."

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  if len(target.elts) != len(value):

Â  Â  Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Expected {len(target.elts)} values but got"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f" {len(value)}."

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  for t, v in zip(target.elts, value):

Â  Â  Â  Â  Â  Â  Â  Â  self.state[self._execute_ast(t)] = v

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  f"Unsupported variable type. Expected "

Â  Â  Â  Â  Â  Â  Â  Â  f"ast.Name or ast.Tuple, got "

Â  Â  Â  Â  Â  Â  Â  Â  f"{target.__class__.__name__} instead."

Â  Â  Â  Â  Â  Â  )



Â  Â  def _execute_call(self, call: ast.Call) -> Any:

Â  Â  Â  Â  callable_func = self._execute_ast(call.func)



Â  Â  Â  Â  # Todo deal with args

Â  Â  Â  Â  args = [self._execute_ast(arg) for arg in call.args]

Â  Â  Â  Â  kwargs = {

Â  Â  Â  Â  Â  Â  keyword.arg: self._execute_ast(keyword.value)

Â  Â  Â  Â  Â  Â  for keyword in call.keywords

Â  Â  Â  Â  }

Â  Â  Â  Â  return callable_func(*args, **kwargs)



Â  Â  def _execute_subscript(self, subscript: ast.Subscript):

Â  Â  Â  Â  index = self._execute_ast(subscript.slice)

Â  Â  Â  Â  value = self._execute_ast(subscript.value)

Â  Â  Â  Â  if not isinstance(subscript.ctx, ast.Load):

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  f"{subscript.ctx.__class__.__name__} is not supported for "

Â  Â  Â  Â  Â  Â  Â  Â  "subscript."

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  if isinstance(value, (list, tuple)):

Â  Â  Â  Â  Â  Â  return value[int(index)]

Â  Â  Â  Â  if index in value:

Â  Â  Â  Â  Â  Â  return value[index]

Â  Â  Â  Â  if isinstance(index, str) and isinstance(value, dict):

Â  Â  Â  Â  Â  Â  close_matches = difflib.get_close_matches(

Â  Â  Â  Â  Â  Â  Â  Â  index,

Â  Â  Â  Â  Â  Â  Â  Â  [key for key in list(value.keys()) if isinstance(key, str)],

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  if len(close_matches) > 0:

Â  Â  Â  Â  Â  Â  Â  Â  return value[close_matches[0]]



Â  Â  Â  Â  raise InterpreterError(f"Could not index {value} with '{index}'.")



Â  Â  def _execute_name(self, name: ast.Name):

Â  Â  Â  Â  if isinstance(name.ctx, ast.Store):

Â  Â  Â  Â  Â  Â  return name.id

Â  Â  Â  Â  elif isinstance(name.ctx, ast.Load):

Â  Â  Â  Â  Â  Â  return self._get_value_from_state(name.id)

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  raise InterpreterError(f"{name.ctx} is not supported.")



Â  Â  def _execute_condition(self, condition: ast.Compare):

Â  Â  Â  Â  if len(condition.ops) > 1:

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  "Cannot evaluate conditions with multiple operators"

Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  left = self._execute_ast(condition.left)

Â  Â  Â  Â  comparator = condition.ops[0]

Â  Â  Â  Â  right = self._execute_ast(condition.comparators[0])



Â  Â  Â  Â  if isinstance(comparator, ast.Eq):

Â  Â  Â  Â  Â  Â  return left == right

Â  Â  Â  Â  elif isinstance(comparator, ast.NotEq):

Â  Â  Â  Â  Â  Â  return left != right

Â  Â  Â  Â  elif isinstance(comparator, ast.Lt):

Â  Â  Â  Â  Â  Â  return left < right

Â  Â  Â  Â  elif isinstance(comparator, ast.LtE):

Â  Â  Â  Â  Â  Â  return left <= right

Â  Â  Â  Â  elif isinstance(comparator, ast.Gt):

Â  Â  Â  Â  Â  Â  return left > right

Â  Â  Â  Â  elif isinstance(comparator, ast.GtE):

Â  Â  Â  Â  Â  Â  return left >= right

Â  Â  Â  Â  elif isinstance(comparator, ast.Is):

Â  Â  Â  Â  Â  Â  return left is right

Â  Â  Â  Â  elif isinstance(comparator, ast.IsNot):

Â  Â  Â  Â  Â  Â  return left is not right

Â  Â  Â  Â  elif isinstance(comparator, ast.In):

Â  Â  Â  Â  Â  Â  return left in right

Â  Â  Â  Â  elif isinstance(comparator, ast.NotIn):

Â  Â  Â  Â  Â  Â  return left not in right

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  raise InterpreterError(f"Unsupported operator: {comparator}")



Â  Â  def _execute_if(self, if_statement: ast.If):

Â  Â  Â  Â  result = None

Â  Â  Â  Â  if not isinstance(if_statement.test, ast.Compare):

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  "Only Compare expr supported in if statement, get"

Â  Â  Â  Â  Â  Â  Â  Â  f" {if_statement.test.__class__.__name__}"

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  if self._execute_condition(if_statement.test):

Â  Â  Â  Â  Â  Â  for line in if_statement.body:

Â  Â  Â  Â  Â  Â  Â  Â  line_result = self._execute_ast(line)

Â  Â  Â  Â  Â  Â  Â  Â  if line_result is not None:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = line_result

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  for line in if_statement.orelse:

Â  Â  Â  Â  Â  Â  Â  Â  line_result = self._execute_ast(line)

Â  Â  Â  Â  Â  Â  Â  Â  if line_result is not None:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = line_result

Â  Â  Â  Â  return result



Â  Â  def _execute_for(self, for_statement: ast.For):

Â  Â  Â  Â  result = None

Â  Â  Â  Â  for value in self._execute_ast(for_statement.iter):

Â  Â  Â  Â  Â  Â  self._assign(for_statement.target, value)

Â  Â  Â  Â  Â  Â  for line in for_statement.body:

Â  Â  Â  Â  Â  Â  Â  Â  line_result = self._execute_ast(line)

Â  Â  Â  Â  Â  Â  Â  Â  if line_result is not None:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = line_result



Â  Â  Â  Â  return result



Â  Â  def _execute_import(self, import_module: ast.Import) -> None:

Â  Â  Â  Â  for module in import_module.names:

Â  Â  Â  Â  Â  Â  self._validate_import(module.name)

Â  Â  Â  Â  Â  Â  alias = module.asname or module.name

Â  Â  Â  Â  Â  Â  self.state[alias] = importlib.import_module(module.name)



Â  Â  def _execute_import_from(self, import_from: ast.ImportFrom):

Â  Â  Â  Â  if import_from.module is None:

Â  Â  Â  Â  Â  Â  raise InterpreterError("\"from . import\" is not supported.")

Â  Â  Â  Â  for import_name in import_from.names:

Â  Â  Â  Â  Â  Â  full_name = import_from.module + f".{import_name.name}"

Â  Â  Â  Â  Â  Â  self._validate_import(full_name)

Â  Â  Â  Â  Â  Â  imported_module = importlib.import_module(import_from.module)

Â  Â  Â  Â  Â  Â  alias = import_name.asname or import_name.name

Â  Â  Â  Â  Â  Â  self.state[alias] = getattr(imported_module, import_name.name)



Â  Â  def _validate_import(self, full_name: str):

Â  Â  Â  Â  tmp_name = ""

Â  Â  Â  Â  found_name = False

Â  Â  Â  Â  for name in full_name.split("."):

Â  Â  Â  Â  Â  Â  tmp_name += name if tmp_name == "" else f".{name}"

Â  Â  Â  Â  Â  Â  if tmp_name in self.import_white_list:

Â  Â  Â  Â  Â  Â  Â  Â  found_name = True

Â  Â  Â  Â  Â  Â  Â  Â  return



Â  Â  Â  Â  if not found_name:

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  f"It is not permitted to import modules "

Â  Â  Â  Â  Â  Â  Â  Â  f"than module white list (try to import "

Â  Â  Â  Â  Â  Â  Â  Â  f"{full_name})."

Â  Â  Â  Â  Â  Â  )



Â  Â  def _execute_binop(self, binop: ast.BinOp):

Â  Â  Â  Â  left = self._execute_ast(binop.left)

Â  Â  Â  Â  operator = binop.op

Â  Â  Â  Â  right = self._execute_ast(binop.right)



Â  Â  Â  Â  if isinstance(operator, ast.Add):

Â  Â  Â  Â  Â  Â  return left + right

Â  Â  Â  Â  elif isinstance(operator, ast.Sub):

Â  Â  Â  Â  Â  Â  return left - right

Â  Â  Â  Â  elif isinstance(operator, ast.Mult):

Â  Â  Â  Â  Â  Â  return left * right

Â  Â  Â  Â  elif isinstance(operator, ast.Div):

Â  Â  Â  Â  Â  Â  return left / right

Â  Â  Â  Â  elif isinstance(operator, ast.FloorDiv):

Â  Â  Â  Â  Â  Â  return left // right

Â  Â  Â  Â  elif isinstance(operator, ast.Mod):

Â  Â  Â  Â  Â  Â  return left % right

Â  Â  Â  Â  elif isinstance(operator, ast.Pow):

Â  Â  Â  Â  Â  Â  return left**right

Â  Â  Â  Â  elif isinstance(operator, ast.LShift):

Â  Â  Â  Â  Â  Â  return left << right

Â  Â  Â  Â  elif isinstance(operator, ast.RShift):

Â  Â  Â  Â  Â  Â  return left >> right

Â  Â  Â  Â  elif isinstance(operator, ast.MatMult):

Â  Â  Â  Â  Â  Â  return left @ right

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  raise InterpreterError(f"Operator not supported: {operator}")



Â  Â  def _execute_unaryop(self, unaryop: ast.UnaryOp):

Â  Â  Â  Â  operand = self._execute_ast(unaryop.operand)

Â  Â  Â  Â  operator = unaryop.op



Â  Â  Â  Â  if isinstance(operator, ast.UAdd):

Â  Â  Â  Â  Â  Â  return +operand

Â  Â  Â  Â  elif isinstance(operator, ast.USub):

Â  Â  Â  Â  Â  Â  return -operand

Â  Â  Â  Â  elif isinstance(operator, ast.Not):

Â  Â  Â  Â  Â  Â  return not operand

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  raise InterpreterError(f"Operator not supported: {operator}")



Â  Â  def _get_value_from_state(self, key: str) -> Any:

Â  Â  Â  Â  if key in self.state:

Â  Â  Â  Â  Â  Â  return self.state[key]

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  close_matches = difflib.get_close_matches(

Â  Â  Â  Â  Â  Â  Â  Â  key, list(self.fuzz_state.keys()), n=1

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  if close_matches:

Â  Â  Â  Â  Â  Â  Â  Â  return self.fuzz_state[close_matches[0]]

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  raise InterpreterError(f"The variable `{key}` is not defined.")







================================================

FILE: server/interpreters/interpreter_error.py

================================================

class InterpreterError(Exception):

Â  Â  r"""Exception raised for errors that can be solved by regenerating code"""



Â  Â  pass







================================================

FILE: server/interpreters/interpreters.py

================================================

from .base import BaseInterpreter

from .docker_interpreter import DockerInterpreter

from .e2b_interpreter import E2BInterpreter

from .internal_python_interpreter import InternalPythonInterpreter

from .interpreter_error import InterpreterError

from .ipython_interpreter import JupyterKernelInterpreter

from .subprocess_interpreter import SubprocessInterpreter



__all__ = [

Â  Â  'BaseInterpreter',

Â  Â  'InterpreterError',

Â  Â  'InternalPythonInterpreter',

Â  Â  'SubprocessInterpreter',

Â  Â  'DockerInterpreter',

Â  Â  'JupyterKernelInterpreter',

Â  Â  'E2BInterpreter',

]







================================================

FILE: server/interpreters/ipython_interpreter.py

================================================

import queue

import re

from typing import TYPE_CHECKING, Any, Dict, List, Optional



from .base import BaseInterpreter

from .interpreter_error import InterpreterError



if TYPE_CHECKING:

Â  Â  from jupyter_client import BlockingKernelClient, KernelManager



TIMEOUT = 30





class JupyterKernelInterpreter(BaseInterpreter):

Â  Â  r"""A class for executing code strings in a Jupyter Kernel.



Â  Â  Args:

Â  Â  Â  Â  require_confirm (bool, optional): If `True`, prompt user before

Â  Â  Â  Â  Â  Â  running code strings for security. Defaults to `True`.

Â  Â  Â  Â  print_stdout (bool, optional): If `True`, print the standard

Â  Â  Â  Â  Â  Â  output of the executed code. Defaults to `False`.

Â  Â  Â  Â  print_stderr (bool, optional): If `True`, print the standard error

Â  Â  Â  Â  Â  Â  of the executed code. Defaults to `True`.

Â  Â  """



Â  Â  def __init__(

Â  Â  Â  Â  self,

Â  Â  Â  Â  require_confirm: bool = True,

Â  Â  Â  Â  print_stdout: bool = False,

Â  Â  Â  Â  print_stderr: bool = True,

Â  Â  ) -> None:

Â  Â  Â  Â  self.require_confirm = require_confirm

Â  Â  Â  Â  self.print_stdout = print_stdout

Â  Â  Â  Â  self.print_stderr = print_stderr



Â  Â  Â  Â  self.kernel_manager: Optional[KernelManager] = None

Â  Â  Â  Â  self.client: Optional[BlockingKernelClient] = None



Â  Â  def __del__(self) -> None:

Â  Â  Â  Â  r"""Clean up the kernel and client."""



Â  Â  Â  Â  if self.kernel_manager:

Â  Â  Â  Â  Â  Â  self.kernel_manager.shutdown_kernel()

Â  Â  Â  Â  if self.client:

Â  Â  Â  Â  Â  Â  self.client.stop_channels()



Â  Â  def _initialize_if_needed(self) -> None:

Â  Â  Â  Â  r"""Initialize the kernel manager and client if they are not already

Â  Â  Â  Â  initialized.

Â  Â  Â  Â  """



Â  Â  Â  Â  if self.kernel_manager is not None:

Â  Â  Â  Â  Â  Â  return



Â  Â  Â  Â  from jupyter_client.manager import start_new_kernel



Â  Â  Â  Â  self.kernel_manager, self.client = start_new_kernel()



Â  Â  @staticmethod

Â  Â  def _clean_ipython_output(output: str) -> str:

Â  Â  Â  Â  r"""Remove ANSI escape sequences from the output."""



Â  Â  Â  Â  ansi_escape = re.compile(r'\x1B[@-_][0-?]*[ -/]*[@-~]')

Â  Â  Â  Â  return ansi_escape.sub('', output)



Â  Â  def _execute(self, code: str, timeout: float) -> str:

Â  Â  Â  Â  r"""Execute the code in the Jupyter kernel and return the result."""



Â  Â  Â  Â  if not self.kernel_manager or not self.client:

Â  Â  Â  Â  Â  Â  raise InterpreterError("Jupyter client is not initialized.")



Â  Â  Â  Â  self.client.execute(code)

Â  Â  Â  Â  outputs = []

Â  Â  Â  Â  while True:

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  msg = self.client.get_iopub_msg(timeout=timeout)

Â  Â  Â  Â  Â  Â  Â  Â  msg_content = msg["content"]

Â  Â  Â  Â  Â  Â  Â  Â  msg_type = msg.get("msg_type", None)



Â  Â  Â  Â  Â  Â  Â  Â  if msg_content.get("execution_state", None) == "idle":

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break



Â  Â  Â  Â  Â  Â  Â  Â  if msg_type == "error":

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(msg_content.keys())

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(msg_content)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  traceback = "\n".join(msg_content["traceback"])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  outputs.append(traceback)

Â  Â  Â  Â  Â  Â  Â  Â  elif msg_type == "stream":

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  outputs.append(msg_content["text"])

Â  Â  Â  Â  Â  Â  Â  Â  elif msg_type in ["execute_result", "display_data"]:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  outputs.append(msg_content["data"]["text/plain"])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if "image/png" in msg_content["data"]:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  outputs.append(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"\n![image](data:image/png;base64,"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{msg_content['data']['image/png']})\n"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  except queue.Empty:

Â  Â  Â  Â  Â  Â  Â  Â  outputs.append("Time out")

Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  outputs.append(f"Exception occurred: {e!s}")

Â  Â  Â  Â  Â  Â  Â  Â  break



Â  Â  Â  Â  exec_result = "\n".join(outputs)

Â  Â  Â  Â  return self._clean_ipython_output(exec_result)



Â  Â  def run(self, code: str, code_type: str) -> str:

Â  Â  Â  Â  r"""Executes the given code in the Jupyter kernel.



Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  code (str): The code string to execute.

Â  Â  Â  Â  Â  Â  code_type (str): The type of code to execute (e.g., 'python',

Â  Â  Â  Â  Â  Â  Â  Â  'bash').



Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  str: A string containing the captured result of the

Â  Â  Â  Â  Â  Â  Â  Â  executed code.



Â  Â  Â  Â  Raises:

Â  Â  Â  Â  Â  Â  InterpreterError: If there is an error when doing code execution.

Â  Â  Â  Â  """

Â  Â  Â  Â  self._initialize_if_needed()



Â  Â  Â  Â  if code_type == "bash":

Â  Â  Â  Â  Â  Â  code = f"%%bash\n({code})"

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  result = self._execute(code, timeout=TIMEOUT)

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  raise InterpreterError(f"Execution failed: {e!s}")



Â  Â  Â  Â  return result



Â  Â  def supported_code_types(self) -> List[str]:

Â  Â  Â  Â  r"""Provides supported code types by the interpreter.



Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  List[str]: Supported code types.

Â  Â  Â  Â  """

Â  Â  Â  Â  return ["python", "bash"]



Â  Â  def update_action_space(self, action_space: Dict[str, Any]) -> None:

Â  Â  Â  Â  r"""Updates the action space for the interpreter.



Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  action_space (Dict[str, Any]): A dictionary representing the

Â  Â  Â  Â  Â  Â  Â  Â  new or updated action space.



Â  Â  Â  Â  Raises:

Â  Â  Â  Â  Â  Â  RuntimeError: Always raised because `JupyterKernelInterpreter`

Â  Â  Â  Â  Â  Â  Â  Â  does not support updating the action space.

Â  Â  Â  Â  """

Â  Â  Â  Â  raise RuntimeError(

Â  Â  Â  Â  Â  Â  "SubprocessInterpreter doesn't support " "`action_space`."

Â  Â  Â  Â  )







================================================

FILE: server/interpreters/logger.py

================================================

import logging



def get_logger(name: str = __name__, level=logging.INFO):

Â  Â  logger = logging.getLogger(name)

Â  Â  if not logger.handlers:Â Â 

Â  Â  Â  Â  logger.setLevel(level)

Â  Â  Â  Â  ch = logging.StreamHandler()

Â  Â  Â  Â  ch.setLevel(level)

Â  Â  Â  Â  formatter = logging.Formatter(

Â  Â  Â  Â  Â  Â  '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

Â  Â  Â  Â  )

Â  Â  Â  Â  ch.setFormatter(formatter)

Â  Â  Â  Â  logger.addHandler(ch)

Â  Â  return logger





================================================

FILE: server/interpreters/subprocess_interpreter.py

================================================

import os

import subprocess

import sys

import tempfile

from pathlib import Path

from typing import Any, ClassVar, Dict, List



from colorama import Fore



from .base import BaseInterpreter

from .interpreter_error import InterpreterError

from .logger import get_logger



logger = get_logger(__name__)





class SubprocessInterpreter(BaseInterpreter):

Â  Â  r"""SubprocessInterpreter is a class for executing code files or code

Â  Â  strings in a subprocess.



Â  Â  This class handles the execution of code in different scripting languages

Â  Â  (currently Python and Bash) within a subprocess, capturing their

Â  Â  stdout and stderr streams, and allowing user checking before executing code

Â  Â  strings.



Â  Â  Args:

Â  Â  Â  Â  require_confirm (bool, optional): If True, prompt user before running

Â  Â  Â  Â  Â  Â  code strings for security. (default: :obj:`True`)

Â  Â  Â  Â  print_stdout (bool, optional): If True, print the standard output of

Â  Â  Â  Â  Â  Â  the executed code. (default: :obj:`False`)

Â  Â  Â  Â  print_stderr (bool, optional): If True, print the standard error of the

Â  Â  Â  Â  Â  Â  executed code. (default: :obj:`True`)

Â  Â  Â  Â  execution_timeout (int, optional): Maximum time in seconds to wait for

Â  Â  Â  Â  Â  Â  code execution to complete. (default: :obj:`60`)

Â  Â  """



Â  Â  _CODE_EXECUTE_CMD_MAPPING: ClassVar[Dict[str, Dict[str, str]]] = {

Â  Â  Â  Â  "python": {"posix": "python {file_name}", "nt": "python {file_name}"},

Â  Â  Â  Â  "bash": {"posix": "bash {file_name}", "nt": "bash {file_name}"},

Â  Â  Â  Â  "r": {"posix": "Rscript {file_name}", "nt": "Rscript {file_name}"},

Â  Â  }



Â  Â  _CODE_EXTENSION_MAPPING: ClassVar[Dict[str, str]] = {

Â  Â  Â  Â  "python": "py",

Â  Â  Â  Â  "bash": "sh",

Â  Â  Â  Â  "r": "R",

Â  Â  }



Â  Â  _CODE_TYPE_MAPPING: ClassVar[Dict[str, str]] = {

Â  Â  Â  Â  "python": "python",

Â  Â  Â  Â  "py3": "python",

Â  Â  Â  Â  "python3": "python",

Â  Â  Â  Â  "py": "python",

Â  Â  Â  Â  "shell": "bash",

Â  Â  Â  Â  "bash": "bash",

Â  Â  Â  Â  "sh": "bash",

Â  Â  Â  Â  "r": "r",

Â  Â  Â  Â  "R": "r",

Â  Â  }



Â  Â  def __init__(

Â  Â  Â  Â  self,

Â  Â  Â  Â  require_confirm: bool = True,

Â  Â  Â  Â  print_stdout: bool = False,

Â  Â  Â  Â  print_stderr: bool = True,

Â  Â  Â  Â  execution_timeout: int = 60,

Â  Â  ) -> None:

Â  Â  Â  Â  self.require_confirm = require_confirm

Â  Â  Â  Â  self.print_stdout = print_stdout

Â  Â  Â  Â  self.print_stderr = print_stderr

Â  Â  Â  Â  self.execution_timeout = execution_timeout



Â  Â  def run_file(

Â  Â  Â  Â  self,

Â  Â  Â  Â  file: Path,

Â  Â  Â  Â  code_type: str,

Â  Â  ) -> str:

Â  Â  Â  Â  r"""Executes a code file in a subprocess and captures its output.



Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  file (Path): The path object of the file to run.

Â  Â  Â  Â  Â  Â  code_type (str): The type of code to execute (e.g., 'python',

Â  Â  Â  Â  Â  Â  Â  Â  'bash').



Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  str: A string containing the captured stdout and stderr of the

Â  Â  Â  Â  Â  Â  Â  Â  executed code.

Â  Â  Â  Â  """

Â  Â  Â  Â  if not file.is_file():

Â  Â  Â  Â  Â  Â  return f"{file} is not a file."

Â  Â  Â  Â  code_type = self._check_code_type(code_type)

Â  Â  Â  Â  if self._CODE_TYPE_MAPPING[code_type] == "python":

Â  Â  Â  Â  Â  Â  # For Python code, use ast to analyze and modify the code

Â  Â  Â  Â  Â  Â  import ast



Â  Â  Â  Â  Â  Â  import astor



Â  Â  Â  Â  Â  Â  with open(file, 'r', encoding='utf-8') as f:

Â  Â  Â  Â  Â  Â  Â  Â  source = f.read()



Â  Â  Â  Â  Â  Â  # Parse the source code

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  tree = ast.parse(source)

Â  Â  Â  Â  Â  Â  Â  Â  # Get the last node

Â  Â  Â  Â  Â  Â  Â  Â  if tree.body:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  last_node = tree.body[-1]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Handle expressions that would normally not produce output

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # For example: In a REPL, typing '1 + 2' should show '3'



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if isinstance(last_node, ast.Expr):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Only wrap in print(repr()) if it's not already a

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # print call

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not (

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  isinstance(last_node.value, ast.Call)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  and isinstance(last_node.value.func, ast.Name)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  and last_node.value.func.id == 'print'

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Transform the AST to wrap the expression in print

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # (repr())

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Example transformation:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  #Â  Â Before: x + y

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  #Â  Â After:Â  print(repr(x + y))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tree.body[-1] = ast.Expr(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value=ast.Call(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Create print() function call

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  func=ast.Name(id='print', ctx=ast.Load()),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  args=[

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ast.Call(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Create repr() function call

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  func=ast.Name(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  id='repr', ctx=ast.Load()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Pass the original expression as

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # argument to repr()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  args=[last_node.value],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keywords=[],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keywords=[],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Fix missing source locations

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ast.fix_missing_locations(tree)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Convert back to source

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  modified_source = astor.to_source(tree)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Create a temporary file with the modified source

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temp_file = self._create_temp_file(modified_source, "py")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cmd = ["python", str(temp_file)]

Â  Â  Â  Â  Â  Â  except (SyntaxError, TypeError, ValueError) as e:

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"Failed to parse Python code with AST: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  platform_type = 'posix' if os.name != 'nt' else 'nt'

Â  Â  Â  Â  Â  Â  Â  Â  cmd_template = self._CODE_EXECUTE_CMD_MAPPING[code_type][

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  platform_type

Â  Â  Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â  Â  Â  base_cmd = cmd_template.split()[0]



Â  Â  Â  Â  Â  Â  Â  Â  # Check if command is available

Â  Â  Â  Â  Â  Â  Â  Â  if not self._is_command_available(base_cmd):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Command '{base_cmd}' not found. Please ensure it "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"is installed and available in your PATH."

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  Â  Â  Â  Â  cmd = [base_cmd, str(file)]

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  # For non-Python code, use standard execution

Â  Â  Â  Â  Â  Â  platform_type = 'posix' if os.name != 'nt' else 'nt'

Â  Â  Â  Â  Â  Â  cmd_template = self._CODE_EXECUTE_CMD_MAPPING[code_type][

Â  Â  Â  Â  Â  Â  Â  Â  platform_type

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â  base_cmd = cmd_template.split()[0]Â  # Get 'python', 'bash', etc.



Â  Â  Â  Â  Â  Â  # Check if command is available

Â  Â  Â  Â  Â  Â  if not self._is_command_available(base_cmd):

Â  Â  Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Command '{base_cmd}' not found. Please ensure it "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"is installed and available in your PATH."

Â  Â  Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  Â  Â  cmd = [base_cmd, str(file)]



Â  Â  Â  Â  # Get current Python executable's environment

Â  Â  Â  Â  env = os.environ.copy()



Â  Â  Â  Â  # On Windows, ensure we use the correct Python executable path

Â  Â  Â  Â  if os.name == 'nt':

Â  Â  Â  Â  Â  Â  python_path = os.path.dirname(sys.executable)

Â  Â  Â  Â  Â  Â  if 'PATH' in env:

Â  Â  Â  Â  Â  Â  Â  Â  env['PATH'] = python_path + os.pathsep + env['PATH']

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  env['PATH'] = python_path



Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  proc = subprocess.Popen(

Â  Â  Â  Â  Â  Â  Â  Â  cmd,

Â  Â  Â  Â  Â  Â  Â  Â  stdout=subprocess.PIPE,

Â  Â  Â  Â  Â  Â  Â  Â  stderr=subprocess.PIPE,

Â  Â  Â  Â  Â  Â  Â  Â  text=True,

Â  Â  Â  Â  Â  Â  Â  Â  env=env,

Â  Â  Â  Â  Â  Â  Â  Â  shell=False,Â  # Never use shell=True for security

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  # Add timeout to prevent hanging processes

Â  Â  Â  Â  Â  Â  stdout, stderr = proc.communicate(timeout=self.execution_timeout)

Â  Â  Â  Â  Â  Â  return_code = proc.returncode

Â  Â  Â  Â  except subprocess.TimeoutExpired:

Â  Â  Â  Â  Â  Â  proc.kill()

Â  Â  Â  Â  Â  Â  stdout, stderr = proc.communicate()

Â  Â  Â  Â  Â  Â  return_code = proc.returncode

Â  Â  Â  Â  Â  Â  timeout_msg = (

Â  Â  Â  Â  Â  Â  Â  Â  f"Process timed out after {self.execution_timeout} seconds "

Â  Â  Â  Â  Â  Â  Â  Â  f"and was terminated."

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  stderr = f"{stderr}\n{timeout_msg}"



Â  Â  Â  Â  # Clean up temporary file if it was created

Â  Â  Â  Â  temp_file_to_clean = locals().get('temp_file')

Â  Â  Â  Â  if temp_file_to_clean is not None:

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  if temp_file_to_clean.exists():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temp_file_to_clean.unlink()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except PermissionError:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # On Windows, files might be locked

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Could not delete temp file "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{temp_file_to_clean} (may be locked)"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"Failed to cleanup temporary file: {e}")



Â  Â  Â  Â  if self.print_stdout and stdout:

Â  Â  Â  Â  Â  Â  print("======stdout======")

Â  Â  Â  Â  Â  Â  print(Fore.GREEN + stdout + Fore.RESET)

Â  Â  Â  Â  Â  Â  print("==================")

Â  Â  Â  Â  if self.print_stderr and stderr:

Â  Â  Â  Â  Â  Â  print("======stderr======")

Â  Â  Â  Â  Â  Â  print(Fore.RED + stderr + Fore.RESET)

Â  Â  Â  Â  Â  Â  print("==================")



Â  Â  Â  Â  # Build the execution result

Â  Â  Â  Â  exec_result = ""

Â  Â  Â  Â  if stdout:

Â  Â  Â  Â  Â  Â  exec_result += stdout

Â  Â  Â  Â  if stderr:

Â  Â  Â  Â  Â  Â  exec_result += f"(stderr: {stderr})"

Â  Â  Â  Â  if return_code != 0:

Â  Â  Â  Â  Â  Â  error_msg = f"(Execution failed with return code {return_code})"

Â  Â  Â  Â  Â  Â  if not stderr:

Â  Â  Â  Â  Â  Â  Â  Â  exec_result += error_msg

Â  Â  Â  Â  Â  Â  elif error_msg not in stderr:

Â  Â  Â  Â  Â  Â  Â  Â  exec_result += error_msg

Â  Â  Â  Â  return exec_result



Â  Â  def run(

Â  Â  Â  Â  self,

Â  Â  Â  Â  code: str,

Â  Â  Â  Â  code_type: str,

Â  Â  ) -> str:

Â  Â  Â  Â  r"""Generates a temporary file with the given code, executes it, and

Â  Â  Â  Â  Â  Â  deletes the file afterward.



Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  code (str): The code string to execute.

Â  Â  Â  Â  Â  Â  code_type (str): The type of code to execute (e.g., 'python',

Â  Â  Â  Â  Â  Â  Â  Â  'bash').



Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  str: A string containing the captured stdout and stderr of the

Â  Â  Â  Â  Â  Â  Â  Â  executed code.



Â  Â  Â  Â  Raises:

Â  Â  Â  Â  Â  Â  InterpreterError: If the user declines to run the code or if the

Â  Â  Â  Â  Â  Â  Â  Â  code type is unsupported.

Â  Â  Â  Â  """

Â  Â  Â  Â  code_type = self._check_code_type(code_type)



Â  Â  Â  Â  # Print code for security checking

Â  Â  Â  Â  if self.require_confirm:

Â  Â  Â  Â  Â  Â  logger.info(

Â  Â  Â  Â  Â  Â  Â  Â  f"The following {code_type} code will run on your "

Â  Â  Â  Â  Â  Â  Â  Â  f"computer: {code}"

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  while True:

Â  Â  Â  Â  Â  Â  Â  Â  choice = input("Running code? [Y/n]:").lower().strip()

Â  Â  Â  Â  Â  Â  Â  Â  if choice in ["y", "yes", "ye", ""]:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  Â  Â  elif choice in ["no", "n"]:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Execution halted: User opted not to run the code. "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "This choice stops the current operation and any "

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "further code execution."

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print("Please enter 'y' or 'n'.")



Â  Â  Â  Â  temp_file_path = None

Â  Â  Â  Â  temp_dir = None

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  temp_file_path = self._create_temp_file(

Â  Â  Â  Â  Â  Â  Â  Â  code=code, extension=self._CODE_EXTENSION_MAPPING[code_type]

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  temp_dir = temp_file_path.parent

Â  Â  Â  Â  Â  Â  return self.run_file(temp_file_path, code_type)

Â  Â  Â  Â  finally:

Â  Â  Â  Â  Â  Â  # Clean up temp file and directory

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  if temp_file_path and temp_file_path.exists():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temp_file_path.unlink()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except PermissionError:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # On Windows, files might be locked

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Could not delete temp file {temp_file_path}"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  Â  Â  Â  Â  if temp_dir and temp_dir.exists():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  import shutil



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shutil.rmtree(temp_dir, ignore_errors=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"Could not delete temp directory: {e}")

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"Error during cleanup: {e}")



Â  Â  def _create_temp_file(self, code: str, extension: str) -> Path:

Â  Â  Â  Â  r"""Creates a temporary file with the given code and extension.



Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  code (str): The code to write to the temporary file.

Â  Â  Â  Â  Â  Â  extension (str): The file extension to use.



Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  Path: The path to the created temporary file.

Â  Â  Â  Â  """

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Create a temporary directory first to ensure we have write

Â  Â  Â  Â  Â  Â  # permissions

Â  Â  Â  Â  Â  Â  temp_dir = tempfile.mkdtemp()

Â  Â  Â  Â  Â  Â  # Create file path with appropriate extension

Â  Â  Â  Â  Â  Â  file_path = Path(temp_dir) / f"temp_code.{extension}"



Â  Â  Â  Â  Â  Â  # Write code to file with appropriate encoding

Â  Â  Â  Â  Â  Â  with open(file_path, 'w', encoding='utf-8') as f:

Â  Â  Â  Â  Â  Â  Â  Â  f.write(code)



Â  Â  Â  Â  Â  Â  return file_path

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  # Clean up temp directory if creation failed

Â  Â  Â  Â  Â  Â  if 'temp_dir' in locals():

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  import shutil



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shutil.rmtree(temp_dir, ignore_errors=True)

Â  Â  Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pass

Â  Â  Â  Â  Â  Â  logger.error(f"Failed to create temporary file: {e}")

Â  Â  Â  Â  Â  Â  raise



Â  Â  def _check_code_type(self, code_type: str) -> str:

Â  Â  Â  Â  if code_type not in self._CODE_TYPE_MAPPING:

Â  Â  Â  Â  Â  Â  raise InterpreterError(

Â  Â  Â  Â  Â  Â  Â  Â  f"Unsupported code type {code_type}. Currently "

Â  Â  Â  Â  Â  Â  Â  Â  f"`{self.__class__.__name__}` only supports "

Â  Â  Â  Â  Â  Â  Â  Â  f"{', '.join(self._CODE_EXTENSION_MAPPING.keys())}."

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  return self._CODE_TYPE_MAPPING[code_type]



Â  Â  def supported_code_types(self) -> List[str]:

Â  Â  Â  Â  r"""Provides supported code types by the interpreter."""

Â  Â  Â  Â  return list(self._CODE_EXTENSION_MAPPING.keys())



Â  Â  def update_action_space(self, action_space: Dict[str, Any]) -> None:

Â  Â  Â  Â  r"""Updates action space for *python* interpreter"""

Â  Â  Â  Â  raise RuntimeError(

Â  Â  Â  Â  Â  Â  "SubprocessInterpreter doesn't support " "`action_space`."

Â  Â  Â  Â  )



Â  Â  def _is_command_available(self, command: str) -> bool:

Â  Â  Â  Â  r"""Check if a command is available in the system PATH.



Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  command (str): The command to check.



Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  bool: True if the command is available, False otherwise.

Â  Â  Â  Â  """

Â  Â  Â  Â  if os.name == 'nt':Â  # Windows

Â  Â  Â  Â  Â  Â  # On Windows, use where.exe to find the command

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  with open(os.devnull, 'w') as devnull:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  subprocess.check_call(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ['where', command],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stdout=devnull,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stderr=devnull,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shell=False,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â  except subprocess.CalledProcessError:

Â  Â  Â  Â  Â  Â  Â  Â  return False

Â  Â  Â  Â  else:Â  # Unix-like systems

Â  Â  Â  Â  Â  Â  # On Unix-like systems, use which to find the command

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  with open(os.devnull, 'w') as devnull:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  subprocess.check_call(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ['which', command],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stdout=devnull,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stderr=devnull,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shell=False,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â  except subprocess.CalledProcessError:

Â  Â  Â  Â  Â  Â  Â  Â  return False



"""




Show thinking
This codebase is well-structured for agentic systems but is not currently equipped to use confidence-based methods like DeepConf out-of-the-box. However, its modular planner-executor architecture makes it highly adaptable for integrating such techniques.

The core challenge is that the system currently learns from the final outcomes of entire task executions (a macro-level signal), whereas DeepConf operates on token-level probabilities during generation (a micro-level signal). Integrating the two would require modifying the LLM API calls to request logprobs and then using those scores to guide the agent's decision-making loops.

Current Architecture: Learning from Experience
The "Memento" framework is a classic planner-executor agent. Its learning mechanism is based on Case-Based Reasoning (CBR), not on the internal confidence of the LLM during generation.

Planner-Executor Loop: The HierarchicalClient in client/agent.py orchestrates the workflow. A "Meta-Planner" (e.g., GPT-4.1) receives a high-level query and breaks it into a JSON plan of sequential tasks. An "Executor" (e.g., o3) then takes each task one by one, using a suite of tools (search, code execution, etc.) to complete it.

Learning Mechanism: The README.md explains that Memento's core concept is to learn by logging successful and failed task trajectories into a "Case Bank". It improves over time by retrieving relevant past cases to inform its planning for new, similar tasks. This is a form of reinforcement learning based on the final reward r_T of a completed trajectory.

LLM Interaction: The key interaction with the LLM happens in the OpenAIBackend.chat method within client/agent.py. This function constructs and sends the API request to an OpenAI-compatible endpoint. Currently, it only processes the content and tool_calls from the response, with no mechanism for handling logprobs.

Feasibility of Integrating Confidence-Based Methods
Integrating DeepConf-like methods is highly feasible and could significantly enhance the agent's performance by providing a real-time quality signal for its generated thoughts, plans, and actions. This would complement the existing long-term learning from the Case Bank.

Hereâ€™s how confidence scores could be applied at different stages of the agent's workflow:

At the Planner Stage (Trace Selection): The Meta-Planner generates a sequence of steps. Instead of generating just one plan, the agent could generate multiple potential plans (traces). By calculating a confidence score for each plan, it could select the highest-confidence plan to execute, increasing the likelihood of success from the start. This is an "offline" application of confidence scoring.

At the Executor Stage (Action Gating & Early Stopping): The Executor generates intermediate thoughts and tool calls.

Gating Tool Calls: If the Executor generates a tool call with very low confidence, it could indicate a hallucinated or poorly formed request. The agent could be programmed to pause and re-plan rather than execute a potentially erroneous and costly tool call.

Stopping Faulty Reasoning: If the Executor's thought process (the text leading up to a tool call) shows declining confidence, it could be "stuck" in a loop or generating nonsensical text. An "online" early-stopping mechanism could terminate this faulty reasoning path and prompt the Planner for a new approach.

Code-Level Implementation Strategy
Adapting the Memento codebase would involve modifications primarily within client/agent.py.

Step 1: Modify the LLM Backend to Request and Process logprobs
The OpenAIBackend.chat method is the central point for this change.

Update the API Call Payload: Modify the payload dictionary to include the parameters needed for confidence scoring.

Python

# In client/agent.py, inside OpenAIBackend.chat method

payload: Dict[str, Any] = {
    "model": self.model,
    "messages": messages,
    "max_tokens": max_tokens,
    "logprobs": True,       # Request log probabilities
    "top_logprobs": 5,      # Request the top 5 logprobs for each token
}
Parse the Response: The API response will now contain a logprobs object. This needs to be parsed and returned along with the message content.

Python

# In client/agent.py, inside OpenAIBackend.chat method, after the API call

msg = resp.choices[0].message
log_probs_content = resp.choices[0].logprobs.content if resp.choices[0].logprobs else None

# ... (existing tool_calls parsing) ...

return {
    "content": msg.content,
    "tool_calls": tool_calls,
    "logprobs": log_probs_content  # Return the parsed logprobs
}
Step 2: Implement Confidence Calculation Utilities
Create a new helper function to calculate the confidence score from the logprobs data, based on the metric you choose (e.g., Lowest Group Confidence). This function would process the list of token logprobs returned by the API.

Python

# This would be a new function, potentially in a separate utils.py file
# or directly within client/agent.py

def calculate_lowest_group_confidence(logprobs_content: List[Any], window_size: int = 2048) -> float:
    """Calculates the lowest average confidence over a sliding window."""
    if not logprobs_content or not logprobs_content[0].top_logprobs:
        return 0.0

    confidences = []
    for token_logprob in logprobs_content:
        # DeepConf defines confidence as the negative mean of *alternative* token logprobs
        top_logprobs = token_logprob.top_logprobs
        if len(top_logprobs) > 1:
            # Exclude the chosen token's logprob for the calculation
            alternative_logprobs = [lp.logprob for lp in top_logprobs[1:]]
            new_conf = -sum(alternative_logprobs) / len(alternative_logprobs)
            confidences.append(new_conf)
        else:
            confidences.append(0.0) # Or another default for no alternatives

    if not confidences:
        return 0.0

    if len(confidences) < window_size:
        return sum(confidences) / len(confidences) # Return mean if window is not full

    # Calculate sliding window means and find the minimum
    window_means = []
    for i in range(len(confidences) - window_size + 1):
        window = confidences[i:i + window_size]
        window_means.append(sum(window) / len(window))

    return min(window_means) if window_means else 0.0
Step 3: Integrate Confidence Scoring into the Agent Loop
Modify the main HierarchicalClient.process_query loop to use the new confidence scores.

For Planner Trace Selection:
You would wrap the call to self.meta_llm.chat to generate several plans, calculate the confidence for each, and proceed with the best one.

Python

# In client/agent.py, inside HierarchicalClient.process_query

# --- Example of generating N plans and selecting the best ---
candidate_plans = []
for _ in range(3): # Generate 3 candidate plans
    meta_reply = await self.meta_llm.chat(planner_msgs)
    confidence = calculate_lowest_group_confidence(meta_reply["logprobs"])
    candidate_plans.append({"reply": meta_reply, "confidence": confidence})

# Select the plan with the highest confidence
best_plan = max(candidate_plans, key=lambda x: x['confidence'])
meta_reply = best_plan['reply']
meta_content = meta_reply["content"] or ""
# --- Proceed with the best plan ---
Summary and Recommendation
Synergy: Memento's Case-Based Reasoning and DeepConf's confidence scoring are not mutually exclusive; they are highly complementary. DeepConf can act as a real-time filter to ensure that only high-quality, confident reasoning traces are executed and subsequently stored in the Case Bank, making Memento's long-term learning more robust.

Path Forward:

Start with the Planner: The most straightforward first step is to implement confidence scoring for the Meta-Planner. Use it to generate multiple plans and select the most confident one. This improves the odds of success before any complex tool use begins.

Advance to the Executor: Subsequently, apply confidence scoring to the Executor's outputs. Use it as a guardrail to prevent the agent from executing low-confidence tool calls, prompting a re-planning step instead. This adds a layer of real-time self-correction to the agent's behavior.







Think very hard on novel methods to use an agentic framework like agno-agi to combine and optimize the concepts and new ideas from both code bases



Show thinking
Excellent question. By synthesizing the real-time, micro-level feedback of DeepConf with the long-term, macro-level learning of Memento, an advanced agentic framework can be created that is both more efficient and more intelligent. The core innovation is to treat confidence as a rich, storable feature of experience, not just as a transient signal for early stopping.

Here are four novel methods to combine and optimize these concepts within an agentic framework.

1. Confidence-Gated Case Memory
Problem: Memento's Case Bank learns from task success or failure, treating all successful plans as equally valuable. However, a plan might succeed through luck despite a flawed, low-confidence reasoning process. Storing and retrieving such cases could teach the agent bad habits.

Novel Method: Augment Memento's memory to create a Confidence-Gated Case Bank. Instead of just storing the final outcome (reward), each case will also store a detailed Confidence Profile of the entire reasoning trace.

Implementation & Optimization:

Richer Memory Tuples: When a task is completed, the agent stores an expanded tuple: (state, plan, outcome, confidence_profile).

The Confidence Profile: This isn't a single number but a structured object capturing key metrics from the execution trace:

planner_confidence: The confidence score of the initial plan generation.

min_executor_confidence: The lowest confidence score recorded during any of the executor's "thought" steps. This acts as a "weakest link" indicator.

avg_tool_confidence: The average confidence across all generated tool calls, indicating how certain the agent was about its actions.

Smarter Retrieval: During Case-Based Reasoning, the Planner's retrieval mechanism can now use this profile. For a critical, high-stakes task, it can be configured to retrieve not just successful past cases, but cases that were both successful AND executed with high confidence. Conversely, analyzing cases that were confidently wrong (high confidence, failed outcome) becomes a priority for identifying and correcting flawed reasoning patterns.

2. Dynamic, Memory-Driven Confidence Thresholding

Problem: DeepConf's online mode relies on a "warmup" phase to set a static confidence threshold for early stopping. An agent tackles a wide variety of tasks (e.g., coding, creative writing, data analysis), each with a different "normal" level of confidence. A single static threshold is suboptimal.

Novel Method: Use the Confidence-Gated Case Bank to dynamically calibrate the early-stopping threshold based on the specific task at hand. The agent learns what an acceptable level of confidence looks like for different problem domains.

Implementation & Optimization:

Task-Type Tagging: When storing cases, add metadata about the task (e.g., task_type: 'code-generation', tool_used: 'execute_terminal_command').

Pre-Execution Calibration: When the Planner receives a new task, it first identifies its type. It then queries its memory for all successful cases of that type.

Adaptive Threshold Calculation: The agent analyzes the historical confidence_profile distribution for these successful cases and calculates a statistically-backed threshold for the current task. For example, it could set the early-stopping threshold to the 10th percentile of the min_executor_confidence scores from past successes. This ensures the threshold is strict for tasks that are historically executed with high certainty (like math) and more lenient for creative or exploratory tasks.

3. Hierarchical Confidence Pruning
Problem: In a multi-step agentic plan, a momentary dip in confidence might not be fatal if the agent can self-correct. Terminating the entire planâ€”as a simple early-stop mechanism would doâ€”is inefficient.

Novel Method: Implement a tiered early-stopping system that prunes faulty reasoning at the lowest possible level, preserving the valid parts of the plan.

Implementation & Optimization:

Step-Level Pruning (Meso): Instead of stopping the whole process, first apply confidence checks to each individual step generated by the Executor. If a single thought-action step is generated with low confidence, the agent discards only that step and prompts the Executor to "try that step again," potentially with a different approach. This is far cheaper than restarting the entire plan.

Plan-Level Pruning (Macro): If the Executor fails multiple retries on the same step, or if a pattern of low confidence emerges across several consecutive steps, the framework then escalates the failure. It terminates the current plan and sends a failure signal back to the Planner. The Planner, now aware that the entire strategy is flawed, can use Memento's CBR to retrieve a completely different case and formulate a new plan from scratch. This creates a resilient system that self-corrects at the cheapest level first.

4. Confidence as an Intrinsic Reward Signal
Problem: An agent's reward is typically sparse; it only knows if the final answer was correct. This makes it hard to learn which intermediate steps were good, especially in long reasoning chains.

Novel Method: Use the confidence of a generated action as a form of intrinsic reward. This provides dense, step-by-step feedback that encourages the agent to not just choose the right tool, but to formulate its reasoning and tool calls clearly and unambiguously.

Implementation & Optimization:

Action Confidence Score: When the Executor generates a tool call (e.g., execute_terminal_command(...)), calculate the confidence score of that specific generation.

Shaping the Reward: This confidence score is fed back into the agent's learning algorithm (which informs the "value" used for Memento's CBR). The agent learns to prefer reasoning paths where its actions are generated with high confidence.

Improved Behavior: This incentivizes the agent to develop more robust and less ambiguous reasoning patterns. Over time, it will learn to avoid convoluted "thought" processes that lead to low-confidence, hesitant tool calls, resulting in a more efficient and reliable agent. When Memento retrieves cases, it will favor those with a history of high intrinsic rewards, promoting the adoption of these clear reasoning styles.

Summary of Synergies
By combining these frameworks, you create a powerful symbiotic loop:

DeepConf acts as the agent's real-time "nervous system," providing immediate feedback on the quality and certainty of its thought process.

Memento acts as the agent's "long-term memory," learning from the outcomes that this nervous system produced.

This integrated "agno-agi" framework would learn faster by populating its memory with higher-quality experiences, operate more efficiently by pruning faulty reasoning in real-time, and adapt its behavior more intelligently by learning the appropriate level of "certainty" required for different tasks.



